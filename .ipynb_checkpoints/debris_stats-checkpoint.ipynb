{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate a 3-panel plot for input arrays\n",
    "def plot_array(dem, clim=None, titles=None, cmap='inferno', label=None, overlay=None, fn=None, close_fig=True):\n",
    "    fig, ax = plt.subplots(1,1, sharex=True, sharey=True, figsize=(10,5))\n",
    "    alpha = 1.0\n",
    "    #Gray background\n",
    "    ax.set_facecolor('0.5')\n",
    "    #Force aspect ratio to match images\n",
    "    ax.set(aspect='equal')\n",
    "    #Turn off axes labels/ticks\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if titles is not None:\n",
    "        ax.set_title(titles[0])\n",
    "    #Plot background shaded relief map\n",
    "    if overlay is not None:\n",
    "        alpha = 0.7\n",
    "        ax.imshow(overlay, cmap='gray', clim=(1,255))\n",
    "    #Plot each array\n",
    "    im_list = [ax.imshow(dem, clim=clim, cmap=cmap, alpha=alpha)]\n",
    "    fig.tight_layout()\n",
    "    fig.colorbar(im_list[0], label=label, extend='both', shrink=0.5)\n",
    "    if fn is not None:\n",
    "        fig.savefig(fn, bbox_inches='tight', pad_inches=0, dpi=150)\n",
    "    if close_fig:\n",
    "        plt.close(fig)\n",
    "\n",
    "def nearest_nonzero_idx(a,x,y):\n",
    "    r,c = np.nonzero(a)\n",
    "    min_idx = ((r - x)**2 + (c - y)**2).argmin()\n",
    "    return r[min_idx], c[min_idx]\n",
    "\n",
    "\n",
    "def maskedarray_gt(data, value, set_value=None):\n",
    "    \"\"\" Greater than operation on masked array to avoid warning errors \"\"\"\n",
    "    if set_value is None:\n",
    "        set_value = value\n",
    "    data = np.nan_to_num(data,0)\n",
    "    data[data > value] = set_value\n",
    "    return data\n",
    "\n",
    "\n",
    "def maskedarray_lt(data, value, set_value=None):\n",
    "    \"\"\" Less than operation on masked array to avoid warning errors \"\"\"\n",
    "    if set_value is None:\n",
    "        set_value = value\n",
    "    data = np.nan_to_num(data,0)\n",
    "    data[data < value] = value\n",
    "    return data\n",
    "\n",
    "\n",
    "def emergence_pixels(gf, vel_x_raw, vel_y_raw, icethickness_raw, xres, yres, \n",
    "                     vel_min=0, max_velocity=600, vel_depth_avg_factor=0.8, option_border=1,\n",
    "                     positive_is_east=True, positive_is_north=True, constant_icethickness=False, debug=True):\n",
    "    \"\"\" Compute the emergence velocity using an ice flux approach\n",
    "    \"\"\"\n",
    "    # Glacier mask\n",
    "    glac_mask = np.zeros(vel_x_raw.shape) + 1\n",
    "    glac_mask[gf.z1.mask] = 0\n",
    "    \n",
    "    # Modify vel_y by multiplying velocity by -1 such that matrix operations agree with flow direction\n",
    "    #    Specifically, a negative y velocity means the pixel is flowing south.\n",
    "    #    However, if you were to subtract that value from the rows, it would head north in the matrix.\n",
    "    #    This is due to the fact that the number of rows start at 0 at the top.\n",
    "    #    Therefore, multipylying by -1 aligns the matrix operations with the flow direction\n",
    "    if positive_is_north:\n",
    "        vel_y = -1*vel_y_raw * vel_depth_avg_factor\n",
    "    else:\n",
    "        vel_y = vel_y_raw * vel_depth_avg_factor\n",
    "    if positive_is_east:\n",
    "        vel_x = vel_x_raw * vel_depth_avg_factor\n",
    "    else:\n",
    "        vel_x = -1*vel_x_raw * vel_depth_avg_factor\n",
    "    vel_total = (vel_y**2 + vel_x**2)**0.5\n",
    "    # Ice thickness\n",
    "    icethickness = icethickness_raw.copy()\n",
    "    if constant_icethickness:\n",
    "        icethickness[:,:] = 1\n",
    "        icethickness = icethickness * glac_mask\n",
    "#     print('mean ice thickness:', np.round(icethickness.mean(),0), 'm')\n",
    "    # Compute the initial volume\n",
    "    volume_initial = icethickness * (xres * yres)\n",
    "    pix_maxres = xres\n",
    "    if yres > pix_maxres:\n",
    "        pix_maxres = yres\n",
    "    # Quality control options:\n",
    "    # Apply a border based on the max specified velocity to prevent errors associated with pixels going out of bounds\n",
    "    if option_border == 1:\n",
    "        border = int(max_velocity / pix_maxres) + 1\n",
    "        for r in range(vel_x.shape[0]):\n",
    "            for c in range(vel_x.shape[1]):\n",
    "                if (r < border) | (r >= vel_x.shape[0] - border) | (c < border) | (c >= vel_x.shape[1] - border):\n",
    "                    vel_x[r,c] = 0\n",
    "                    vel_y[r,c] = 0\n",
    "    # Minimum/maximum velocity bounds\n",
    "    vel_x[vel_total < vel_min] = 0\n",
    "    vel_y[vel_total < vel_min] = 0\n",
    "    vel_x[vel_total > max_velocity] = 0\n",
    "    vel_y[vel_total > max_velocity] = 0\n",
    "#     # Remove clusters of high velocity on stagnant portions of glaciers due to feature tracking of ice cliffs and ponds\n",
    "#     if option_stagnantbands == 1:\n",
    "#         vel_x[bands <= stagnant_band] = 0\n",
    "#         vel_y[bands <= stagnant_band] = 0        \n",
    "    # Compute displacement in units of pixels\n",
    "    vel_x_pix = vel_x / xres\n",
    "    vel_y_pix = vel_y / yres\n",
    "    # Compute the displacement and fraction of pixels moved for all columns (x-axis)\n",
    "    # col_x1 is the number of columns to the closest pixel receiving ice [ex. 2.6 returns 2, -2.6 returns -2]\n",
    "    #    int() automatically rounds towards zero\n",
    "    col_x1 = vel_x_pix.astype(int)\n",
    "    # col_x2 is the number of columns to the further pixel receiving ice [ex. 2.6 returns 3, -2.6 returns -3]\n",
    "    #    np.sign() returns a value of 1 or -1, so it's adding 1 pixel away from zero\n",
    "    col_x2 = (vel_x_pix + np.sign(vel_x_pix)).astype(int)\n",
    "    # rem_x2 is the fraction of the pixel that remains in the further pixel (col_x2) [ex. 2.6 returns 0.6, -2.6 returns 0.6]\n",
    "    #    np.sign() returns a value of 1 or -1, so multiplying by that ensures you have a positive value\n",
    "    #    then when you take the remainder using \"% 1\", you obtain the desired fraction\n",
    "    rem_x2 = np.multiply(np.sign(vel_x_pix), vel_x_pix) % 1\n",
    "    # rem_x1 is the fraction of the pixel that remains in the closer pixel (col_x1) [ex. 2.6 returns 0.4, -2.6 returns 0.4]\n",
    "    rem_x1 = 1 - rem_x2\n",
    "    # Repeat the displacement and fraction computations for all rows (y-axis)\n",
    "    row_y1 = vel_y_pix.astype(int)\n",
    "    row_y2 = (vel_y_pix + np.sign(vel_y_pix)).astype(int)\n",
    "    rem_y2 = np.multiply(np.sign(vel_y_pix), vel_y_pix) % 1\n",
    "    rem_y1 = 1 - rem_y2\n",
    "          \n",
    "    # Compute the mass flux for each pixel\n",
    "    volume_final = np.zeros(volume_initial.shape)\n",
    "    for r in range(vel_x.shape[0]):\n",
    "        for c in range(vel_x.shape[1]):\n",
    "            volume_final[r+row_y1[r,c], c+col_x1[r,c]] = (\n",
    "                volume_final[r+row_y1[r,c], c+col_x1[r,c]] + rem_y1[r,c]*rem_x1[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y2[r,c], c+col_x1[r,c]] = (\n",
    "                volume_final[r+row_y2[r,c], c+col_x1[r,c]] + rem_y2[r,c]*rem_x1[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y1[r,c], c+col_x2[r,c]] = (\n",
    "                volume_final[r+row_y1[r,c], c+col_x2[r,c]] + rem_y1[r,c]*rem_x2[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y2[r,c], c+col_x2[r,c]] = (\n",
    "                volume_final[r+row_y2[r,c], c+col_x2[r,c]] + rem_y2[r,c]*rem_x2[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "         \n",
    "    # Redistribute off-glacier volume back onto the nearest pixel on the glacier\n",
    "    offglac_row, offglac_col = np.where((glac_mask == 0) & (volume_final > 0))\n",
    "    for nidx in range(0,len(offglac_row)):\n",
    "        nrow = offglac_row[nidx]\n",
    "        ncol = offglac_col[nidx]\n",
    "        ridx, cidx = nearest_nonzero_idx(glac_mask, nrow, ncol)\n",
    "        # Add off-glacier volume back onto nearest pixel on glacier\n",
    "        volume_final[ridx,cidx] += volume_final[nrow,ncol]\n",
    "        volume_final[nrow,ncol] = 0\n",
    "            \n",
    "    # Check that mass is conserved (threshold = 0.1 m x pixel_size**2)\n",
    "    if debug:\n",
    "        print('Mass is conserved?', np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() < 0.01)\n",
    "        print(np.round(np.absolute(volume_final.sum() - volume_initial.sum()),1), \n",
    "              np.round(np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() * 100,2), '%')\n",
    "        \n",
    "    if np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() > 0.01:\n",
    "        print('MASS NOT CONSERVED FOR EMERGENCE VELOCITY')\n",
    "    # Final ice thickness\n",
    "    icethickness_final = volume_final / (xres * yres)\n",
    "    # Emergence velocity\n",
    "    emergence_velocity = icethickness_final - icethickness\n",
    "    return emergence_velocity\n",
    "\n",
    "\n",
    "\n",
    "class GlacFeat:\n",
    "    def __init__(self, feat, glacname_fieldname, glacnum_fieldname):\n",
    "\n",
    "        self.glacname = feat.GetField(glacname_fieldname)\n",
    "        if self.glacname is None:\n",
    "            self.glacname = \"\"\n",
    "        else:\n",
    "            #RGI has some nonstandard characters\n",
    "            #self.glacname = self.glacname.decode('unicode_escape').encode('ascii','ignore')\n",
    "            #glacname = re.sub(r'[^\\x00-\\x7f]',r'', glacname)\n",
    "            self.glacname = re.sub(r'\\W+', '', self.glacname)\n",
    "            self.glacname = self.glacname.replace(\" \", \"\")\n",
    "            self.glacname = self.glacname.replace(\"_\", \"\")\n",
    "            self.glacname = self.glacname.replace(\"/\", \"\")\n",
    "\n",
    "        self.glacnum = feat.GetField(glacnum_fieldname)\n",
    "        fn = feat.GetDefnRef().GetName()\n",
    "        #RGIId (String) = RGI50-01.00004\n",
    "        self.glacnum = '%0.5f' % float(self.glacnum.split('-')[-1])\n",
    "\n",
    "        if self.glacname:\n",
    "            self.feat_fn = \"%s_%s\" % (self.glacnum, self.glacname)\n",
    "        else:\n",
    "            self.feat_fn = str(self.glacnum)\n",
    "\n",
    "        self.glac_geom_orig = geolib.geom_dup(feat.GetGeometryRef())\n",
    "        self.glac_geom = geolib.geom_dup(self.glac_geom_orig)\n",
    "        #Hack to deal with fact that this is not preserved in geom when loaded from pickle on disk\n",
    "        self.glac_geom_srs_wkt = self.glac_geom.GetSpatialReference().ExportToWkt()\n",
    "\n",
    "        #Attributes written by mb_calc\n",
    "        self.z1 = None\n",
    "        self.z1_hs = None\n",
    "        self.z1_stats = None\n",
    "        self.z1_ela = None\n",
    "        self.z2 = None\n",
    "        self.z2_hs = None\n",
    "        self.z2_stats = None\n",
    "        self.z2_ela = None\n",
    "        self.z2_aspect = None\n",
    "        self.z2_aspect_stats = None\n",
    "        self.z2_slope = None\n",
    "        self.z2_slope_stats = None\n",
    "        self.res = None\n",
    "        self.dhdt = None\n",
    "        self.mb = None\n",
    "        self.mb_mean = None\n",
    "        self.t1 = None\n",
    "        self.t2 = None\n",
    "        self.dt = None\n",
    "        self.t1_mean = None\n",
    "        self.t2_mean = None\n",
    "        self.dt_mean = None\n",
    "\n",
    "        self.H = None\n",
    "        self.H_mean = np.nan\n",
    "        self.vx = None\n",
    "        self.vy = None\n",
    "        self.vm = None\n",
    "        self.vm_mean = np.nan\n",
    "        self.divQ = None\n",
    "        self.emvel = None\n",
    "        self.debris_class = None\n",
    "        self.debris_thick = None\n",
    "        self.debris_thick_mean = np.nan\n",
    "        self.perc_clean = np.nan\n",
    "        self.perc_debris = np.nan\n",
    "        self.perc_pond = np.nan\n",
    "        self.dc_area = None\n",
    "\n",
    "    def geom_srs_update(self, srs=None):\n",
    "        if self.glac_geom.GetSpatialReference() is None:\n",
    "            if srs is None:\n",
    "                srs = osr.SpatialReference()\n",
    "                srs.ImportFromWkt(self.glac_geom_srs_wkt)\n",
    "            self.glac_geom.AssignSpatialReference(srs)\n",
    "\n",
    "    def geom_attributes(self, srs=None):\n",
    "        self.geom_srs_update()\n",
    "        if srs is not None:\n",
    "            #Should reproject here to equal area, before geom_attributes\n",
    "            #self.glac_geom.AssignSpatialReference(glac_shp_srs)\n",
    "            #self.glac_geom_local = geolib.geom2localortho(self.glac_geom)\n",
    "            geolib.geom_transform(self.glac_geom, srs)\n",
    "\n",
    "        self.glac_geom_extent = geolib.geom_extent(self.glac_geom)\n",
    "        self.glac_area = self.glac_geom.GetArea()\n",
    "        self.glac_area_km2 = self.glac_area / 1E6\n",
    "        self.cx, self.cy = self.glac_geom.Centroid().GetPoint_2D()\n",
    "        \n",
    "        \n",
    "#RGI uses 50 m bins\n",
    "def hist_plot(gf, bin_width=50.0, dz_clim=(-2.0, 2.0), exportcsv=True, csv_ending='', mb_df=None):\n",
    "    #print(\"Generating histograms\")\n",
    "    #Create bins for full range of input data and specified bin width\n",
    "\n",
    "    #NOTE: these counts/areas are for valid pixels only\n",
    "    #Not necessarily a true representation of actual glacier hypsometry\n",
    "    #Need a void-filled DEM for this\n",
    "    if mb_df is not None:\n",
    "        # Align bins with mass balance data\n",
    "        bin_center_min = mb_df.loc[0,'# bin_center_elev_m']\n",
    "        while bin_center_min > gf.z1.min() + bin_width/2:\n",
    "            bin_center_min -= mb_bin_size\n",
    "        bin_center_max = mb_df['# bin_center_elev_m'].values[-1]\n",
    "        while bin_center_max < gf.z1.max():\n",
    "            bin_center_max += mb_bin_size    \n",
    "        z_bin_centers = np.arange(bin_center_min, bin_center_max + mb_bin_size/2, mb_bin_size)\n",
    "        z_bin_edges = np.arange(bin_center_min - mb_bin_size / 2, bin_center_max + mb_bin_size, mb_bin_size)\n",
    "    else:\n",
    "        z_bin_edges, z_bin_centers = malib.get_bins(gf.z1, bin_width)\n",
    "        \n",
    "    #Need to compress here, otherwise histogram uses masked values!\n",
    "    z1_bin_counts, z1_bin_edges = np.histogram(gf.z1.compressed(), bins=z_bin_edges)\n",
    "    z1_bin_areas = z1_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "    #RGI standard is integer thousandths of glaciers total area\n",
    "    #Should check to make sure sum of bin areas equals total area\n",
    "    #z1_bin_areas_perc = 100. * z1_bin_areas / np.sum(z1_bin_areas)\n",
    "    z1_bin_areas_perc = 100. * (z1_bin_areas / gf.glac_area_km2)\n",
    "\n",
    "    #If we only have one elevation grid with dhdt\n",
    "    if gf.z2 is not None:\n",
    "        z2_bin_counts, z2_bin_edges = np.histogram(gf.z2.compressed(), bins=z_bin_edges)\n",
    "        z2_bin_areas = z2_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "        #z2_bin_areas_perc = 100. * z2_bin_areas / np.sum(z2_bin_areas)\n",
    "        z2_bin_areas_perc = 100. * (z1_bin_areas / gf.glac_area_km2)\n",
    "    else:\n",
    "        z2_bin_counts = z1_bin_counts\n",
    "        z2_bin_edges = z1_bin_edges\n",
    "        z2_bin_areas = z1_bin_areas\n",
    "        z2_bin_areas_perc = z1_bin_areas_perc\n",
    "        \n",
    "    if gf.dc_area is not None:\n",
    "#         z_bin_edges, z_bin_centers = malib.get_bins(gf.z1, bin_width)\n",
    "#         #Need to compress here, otherwise histogram uses masked values!\n",
    "#         z1_bin_counts, z1_bin_edges = np.histogram(gf.z1.compressed(), bins=z_bin_edges)\n",
    "#         z1_bin_areas = z1_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "#         #RGI standard is integer thousandths of glaciers total area\n",
    "#         #Should check to make sure sum of bin areas equals total area\n",
    "#         #z1_bin_areas_perc = 100. * z1_bin_areas / np.sum(z1_bin_areas)\n",
    "#         z1_bin_areas_perc = 100. * (z1_bin_areas / gf.glac_area_km2)\n",
    "        \n",
    "# #         dc_bin_edges, dc_bin_centers = malib.get_bins(gf.dc_area, bin_width)\n",
    "        dc_bin_counts, dc_bin_edges = np.histogram(gf.dc_area.compressed(), bins=z_bin_edges)\n",
    "        dc_bin_areas = dc_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "        #RGI standard is integer thousandths of glaciers total area\n",
    "        dc_bin_areas_perc = 100. * (dc_bin_areas / gf.glac_area_km2)\n",
    "\n",
    "    #Create arrays to store output\n",
    "    slope_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "    slope_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    aspect_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "    aspect_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.dhdt is not None:\n",
    "        mb_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        np.ma.set_fill_value(mb_bin_med, np.nan)\n",
    "        mb_bin_mad = np.ma.masked_all_like(mb_bin_med)\n",
    "        mb_bin_mean = np.ma.masked_all_like(mb_bin_med)\n",
    "        mb_bin_std = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_med = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_mad = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_mean = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_std = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_count = np.ma.masked_all_like(mb_bin_med)\n",
    "    if gf.vm is not None:\n",
    "        vm_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        vm_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.H is not None:\n",
    "        H_bin_mean = np.ma.masked_all_like(z1_bin_areas)\n",
    "        H_bin_std = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.emvel is not None:\n",
    "        emvel_bin_mean = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_std = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.debris_class is not None:\n",
    "#         perc_clean = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         perc_debris = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         perc_pond = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_clean_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_debris_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_pond_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "\n",
    "#         gf.dhdt_clean = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 1).data))\n",
    "#         gf.dhdt_debris = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 2).data))\n",
    "#         gf.dhdt_pond = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 3).data))\n",
    "\n",
    "    if gf.debris_thick_ts is not None:\n",
    "        debris_thick_ts_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_ts_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.meltfactor_ts is not None:\n",
    "        meltfactor_ts_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        meltfactor_ts_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "\n",
    "    #Bin sample count must be greater than this value\n",
    "    min_bin_samp_count = 9\n",
    "\n",
    "    #Loop through each bin and extract stats\n",
    "    idx = np.digitize(gf.z1, z_bin_edges)\n",
    "    for bin_n in range(z_bin_centers.size):\n",
    "        if gf.dhdt is not None:\n",
    "            mb_bin_samp = gf.mb_map[(idx == bin_n+1)]\n",
    "            if mb_bin_samp.count() > min_bin_samp_count:\n",
    "                mb_bin_med[bin_n] = malib.fast_median(mb_bin_samp)\n",
    "                mb_bin_mad[bin_n] = malib.mad(mb_bin_samp)\n",
    "                mb_bin_mean[bin_n] = mb_bin_samp.mean()\n",
    "                mb_bin_std[bin_n] = mb_bin_samp.std()\n",
    "            dhdt_bin_samp = gf.dhdt[(idx == bin_n+1)]\n",
    "            if dhdt_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_bin_med[bin_n] = malib.fast_median(dhdt_bin_samp)\n",
    "                dhdt_bin_mad[bin_n] = malib.mad(dhdt_bin_samp)\n",
    "                dhdt_bin_mean[bin_n] = dhdt_bin_samp.mean()\n",
    "                dhdt_bin_std[bin_n] = dhdt_bin_samp.std()\n",
    "                dhdt_bin_count[bin_n] = dhdt_bin_samp.count()\n",
    "        if gf.debris_thick is not None:\n",
    "            debris_thick_bin_samp = gf.debris_thick[(idx == bin_n+1)]\n",
    "            if debris_thick_bin_samp.size > min_bin_samp_count:\n",
    "                debris_thick_med[bin_n] = malib.fast_median(debris_thick_bin_samp)\n",
    "                debris_thick_mad[bin_n] = malib.mad(debris_thick_bin_samp)\n",
    "        \n",
    "        if gf.debris_thick_ts is not None:\n",
    "            debris_thick_ts_bin_samp = gf.debris_thick_ts[(idx == bin_n+1)]\n",
    "            if debris_thick_ts_bin_samp.size > min_bin_samp_count:\n",
    "                debris_thick_ts_med[bin_n] = malib.fast_median(debris_thick_ts_bin_samp)\n",
    "                debris_thick_ts_mad[bin_n] = malib.mad(debris_thick_ts_bin_samp)\n",
    "        if gf.meltfactor_ts is not None:\n",
    "            meltfactor_ts_bin_samp = gf.meltfactor_ts[(idx == bin_n+1)]\n",
    "            if meltfactor_ts_bin_samp.size > min_bin_samp_count:\n",
    "                meltfactor_ts_med[bin_n] = malib.fast_median(meltfactor_ts_bin_samp)\n",
    "                meltfactor_ts_mad[bin_n] = malib.mad(meltfactor_ts_bin_samp)\n",
    "        \n",
    "        if gf.debris_class is not None:\n",
    "            debris_class_bin_samp = gf.debris_class[(idx == bin_n+1)]\n",
    "            dhdt_clean_bin_samp = gf.dhdt_clean[(idx == bin_n+1)]\n",
    "            dhdt_debris_bin_samp = gf.dhdt_debris[(idx == bin_n+1)]\n",
    "            dhdt_pond_bin_samp = gf.dhdt_pond[(idx == bin_n+1)]\n",
    "            if debris_class_bin_samp.count() > min_bin_samp_count:\n",
    "                perc_clean[bin_n] = 100. * (debris_class_bin_samp == 1).sum()/debris_class_bin_samp.count()\n",
    "                perc_debris[bin_n] = 100. * (debris_class_bin_samp == 2).sum()/debris_class_bin_samp.count()\n",
    "                perc_pond[bin_n] = 100. * (debris_class_bin_samp == 3).sum()/debris_class_bin_samp.count()\n",
    "            if dhdt_clean_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_clean_bin_med[bin_n] = malib.fast_median(dhdt_clean_bin_samp)\n",
    "            if dhdt_debris_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_debris_bin_med[bin_n] = malib.fast_median(dhdt_debris_bin_samp)\n",
    "            if dhdt_pond_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_pond_bin_med[bin_n] = malib.fast_median(dhdt_pond_bin_samp)\n",
    "        if gf.vm is not None:\n",
    "            vm_bin_samp = gf.vm[(idx == bin_n+1)]\n",
    "            if vm_bin_samp.size > min_bin_samp_count:\n",
    "                vm_bin_med[bin_n] = malib.fast_median(vm_bin_samp)\n",
    "                vm_bin_mad[bin_n] = malib.mad(vm_bin_samp)\n",
    "        if gf.H is not None:\n",
    "            H_bin_samp = gf.H[(idx == bin_n+1)]\n",
    "            if H_bin_samp.size > min_bin_samp_count:\n",
    "                H_bin_mean[bin_n] = H_bin_samp.mean()\n",
    "                H_bin_std[bin_n] = H_bin_samp.std()\n",
    "        if gf.emvel is not None:\n",
    "            emvel_bin_samp = gf.emvel[(idx == bin_n+1)]\n",
    "            if emvel_bin_samp.size > min_bin_samp_count:\n",
    "                emvel_bin_mean[bin_n] = emvel_bin_samp.mean()\n",
    "                emvel_bin_std[bin_n] = emvel_bin_samp.std()\n",
    "                emvel_bin_med[bin_n] = malib.fast_median(emvel_bin_samp)\n",
    "                emvel_bin_mad[bin_n] = malib.mad(emvel_bin_samp)\n",
    "        slope_bin_samp = gf.z1_slope[(idx == bin_n+1)]\n",
    "        if slope_bin_samp.size > min_bin_samp_count:\n",
    "            slope_bin_med[bin_n] = malib.fast_median(slope_bin_samp)\n",
    "            slope_bin_mad[bin_n] = malib.mad(slope_bin_samp)\n",
    "        aspect_bin_samp = gf.z1_aspect[(idx == bin_n+1)]\n",
    "        if aspect_bin_samp.size > min_bin_samp_count:\n",
    "            aspect_bin_med[bin_n] = malib.fast_median(aspect_bin_samp)\n",
    "            aspect_bin_mad[bin_n] = malib.mad(aspect_bin_samp)\n",
    "\n",
    "    if gf.dhdt is not None:\n",
    "        dhdt_bin_areas = dhdt_bin_count * gf.res[0] * gf.res[1] / 1E6\n",
    "        #dhdt_bin_areas_perc = 100. * dhdt_bin_areas / np.sum(dhdt_bin_areas)\n",
    "        dhdt_bin_areas_perc = 100. * (dhdt_bin_areas / gf.glac_area_km2)\n",
    "\n",
    "    outbins_header = 'bin_center_elev_m, z1_bin_count_valid, z1_bin_area_valid_km2, z1_bin_area_perc, z2_bin_count_valid, z2_bin_area_valid_km2, z2_bin_area_perc, slope_bin_med, aspect_bin_med'\n",
    "    fmt = '%0.1f, %0.0f, %0.3f, %0.2f, %0.0f, %0.3f, %0.2f, %0.2f, %0.2f'\n",
    "    outbins = [z_bin_centers, z1_bin_counts, z1_bin_areas, z1_bin_areas_perc, z2_bin_counts, z2_bin_areas, z2_bin_areas_perc, slope_bin_med, aspect_bin_med]\n",
    "    if gf.dhdt is not None:\n",
    "        outbins_header += ', dhdt_bin_count, dhdt_bin_area_valid_km2, dhdt_bin_area_perc, dhdt_bin_med_ma, dhdt_bin_mad_ma, dhdt_bin_mean_ma, dhdt_bin_std_ma, mb_bin_med_mwea, mb_bin_mad_mwea, mb_bin_mean_mwea, mb_bin_std_mwea'\n",
    "        fmt += ', %0.0f, %0.3f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "        outbins.extend([dhdt_bin_count, dhdt_bin_areas, dhdt_bin_areas_perc, dhdt_bin_med, dhdt_bin_mad, dhdt_bin_mean, dhdt_bin_std, \\\n",
    "                        mb_bin_med, mb_bin_mad, mb_bin_mean, mb_bin_std])\n",
    "    if gf.dc_area is not None:\n",
    "        outbins_header += ', dc_bin_count_valid, dc_bin_area_valid_km2, dc_bin_area_perc'\n",
    "        fmt += ', %0.0f, %0.3f, %0.2f'\n",
    "        outbins.extend([dc_bin_counts, dc_bin_areas, dc_bin_areas_perc])\n",
    "#         outbins.extend([z1_bin_counts, z1_bin_areas, z1_bin_areas_perc])\n",
    "        \n",
    "        \n",
    "    if gf.debris_thick is not None:\n",
    "        outbins_header += ', debris_thick_med_m, debris_thick_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        debris_thick_med[debris_thick_med == -(np.inf)] = 0.00\n",
    "        debris_thick_mad[debris_thick_mad == -(np.inf)] = 0.00\n",
    "        outbins.extend([debris_thick_med, debris_thick_mad])\n",
    "    \n",
    "    if gf.debris_thick_ts is not None:\n",
    "        outbins_header += ',debris_thick_ts_med_m,debris_thick_ts_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        debris_thick_ts_med[debris_thick_ts_med == -(np.inf)] = 0.00\n",
    "        debris_thick_ts_mad[debris_thick_ts_mad == -(np.inf)] = 0.00\n",
    "        outbins.extend([debris_thick_ts_med, debris_thick_ts_mad])\n",
    "    if gf.meltfactor_ts is not None:\n",
    "        outbins_header += ',meltfactor_ts_med_m,meltfactor_ts_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        meltfactor_ts_med[meltfactor_ts_med == -(np.inf)] = 1\n",
    "        meltfactor_ts_med[meltfactor_ts_med > 1] = 1\n",
    "        meltfactor_ts_med[meltfactor_ts_med <= 0] = 1\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad == -(np.inf)] = 0\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad > 1] = 0\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad <= 0] = 0\n",
    "        outbins.extend([meltfactor_ts_med, meltfactor_ts_mad])\n",
    "    \n",
    "    if gf.debris_class is not None:\n",
    "        outbins_header += ', perc_debris, perc_pond, perc_clean, dhdt_debris_med, dhdt_pond_med, dhdt_clean_med'\n",
    "        fmt += ', %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "        outbins.extend([perc_debris, perc_pond, perc_clean, dhdt_debris_bin_med, dhdt_pond_bin_med, dhdt_clean_bin_med])\n",
    "    if gf.vm is not None:\n",
    "        outbins_header += ', vm_med, vm_mad'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        outbins.extend([vm_bin_med, vm_bin_mad])\n",
    "    if gf.H is not None:\n",
    "        outbins_header += ', H_mean, H_std'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        outbins.extend([H_bin_mean, H_bin_std])\n",
    "#         outbins_header += ', H_mean, H_std, emvel_mean, emvel_std'\n",
    "#         fmt += ', %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "#         outbins.extend([H_bin_mean, H_bin_std, emvel_bin_mean, emvel_bin_std])\n",
    "\n",
    "    if gf.emvel is not None:\n",
    "        outbins_header += ', emvel_mean, emvel_std, emvel_med, emvel_mad'\n",
    "        fmt += ', %0.3f, %0.3f, %0.3f, %0.3f'\n",
    "        outbins.extend([emvel_bin_mean, emvel_bin_std, emvel_bin_med, emvel_bin_mad])\n",
    "    \n",
    "    outbins = np.ma.array(outbins).T.astype('float32')\n",
    "    np.ma.set_fill_value(outbins, np.nan)\n",
    "    outbins = outbins.filled(np.nan)\n",
    "    \n",
    "    outbins_df = pd.DataFrame(outbins, columns=outbins_header.split(','))\n",
    "    outbins_df['debris_perc'] = outbins_df[' dc_bin_count_valid'] / outbins_df[' z1_bin_count_valid'] * 100\n",
    "    \n",
    "    if mb_df is not None:\n",
    "        # ADD MASS BALANCE DATA\n",
    "        mb_df = mb_df[np.isfinite(mb_df['# bin_center_elev_m'])]\n",
    "        mb_df.reset_index(inplace=True, drop=True)\n",
    "        # start index for merge\n",
    "        if mb_df.loc[0,'# bin_center_elev_m'] >= outbins_df.loc[0,'bin_center_elev_m']:\n",
    "            mb_df_idx1 = 0\n",
    "            outbins_idx1 = np.where(outbins_df['bin_center_elev_m'] == mb_df.loc[0,'# bin_center_elev_m'])[0][0]\n",
    "        else:\n",
    "            outbins_idx1 = 0\n",
    "            mb_df_idx1 = np.where(outbins_df.loc[0,'bin_center_elev_m'] == mb_df['# bin_center_elev_m'])[0][0]\n",
    "    #     print('idx1:', \n",
    "    #           '\\noutbins:', outbins_idx1, outbins_df.loc[outbins_idx1,'bin_center_elev_m'],\n",
    "    #           '\\ndfbins:', mb_df_idx1, mb_df.loc[mb_df_idx1,'# bin_center_elev_m'])\n",
    "        # end index for merge\n",
    "        if outbins_df.loc[outbins_df.shape[0]-1,'bin_center_elev_m'] >= mb_df.loc[mb_df.shape[0]-1,'# bin_center_elev_m']:\n",
    "            outbins_idx2 = np.where(outbins_df['bin_center_elev_m'] == mb_df.loc[mb_df.shape[0]-1,'# bin_center_elev_m'])[0][0]\n",
    "            mb_df_idx2 = mb_df.shape[0]-1\n",
    "        else:\n",
    "            outbins_idx2 = outbins_df.shape[0]-1\n",
    "            mb_df_idx2 = np.where(outbins_df.loc[outbins_df.shape[0]-1,'bin_center_elev_m'] == mb_df['# bin_center_elev_m'])[0][0]\n",
    "    #     print('idx2:', \n",
    "    #           '\\noutbins:', outbins_idx2, outbins_df.loc[outbins_idx2,'bin_center_elev_m'],\n",
    "    #           '\\ndfbins:', mb_df_idx2, mb_df.loc[mb_df_idx2,'# bin_center_elev_m'])\n",
    "        outbins_df[' mb_bin_mean_mwea'] = np.nan\n",
    "        outbins_df[' mb_bin_std_mwea'] = np.nan\n",
    "        outbins_df[' mb_bin_area_valid_km2'] = np.nan\n",
    "        outbins_df.loc[outbins_idx1:outbins_idx2+1,' mb_bin_mean_mwea'] = mb_df.loc[mb_df_idx1:mb_df_idx2+1,' mb_bin_mean_mwea']\n",
    "        outbins_df.loc[outbins_idx1:outbins_idx2+1,' mb_bin_std_mwea'] = mb_df.loc[mb_df_idx1:mb_df_idx2+1,' mb_bin_std_mwea']\n",
    "        outbins_df.loc[outbins_idx1:outbins_idx2+1,' mb_bin_area_valid_km2'] = mb_df.loc[mb_df_idx1:mb_df_idx2+1,' z1_bin_area_valid_km2']\n",
    "        try:\n",
    "            outbins_df['startyear'] = mb_df.loc[mb_df_idx1,'startyear']\n",
    "            outbins_df['endyear'] = mb_df.loc[mb_df_idx1,'endyear']\n",
    "        except:\n",
    "            outbins_df['startyear'] = 2000\n",
    "            outbins_df['endyear'] = 2012\n",
    "    \n",
    "    if exportcsv:\n",
    "        if int(gf.feat_fn.split('.')[0]) < 10:\n",
    "            outbins_fullfn = os.path.join(outdir_csv, gf.feat_fn[0:7] + csv_ending)\n",
    "        else:\n",
    "            outbins_fullfn = os.path.join(outdir_csv, gf.feat_fn[0:8] + csv_ending)\n",
    "        outbins_df.to_csv(outbins_fullfn, index=False)\n",
    "#         np.savetxt(outbins_fn, outbins, fmt=fmt, delimiter=',', header=outbins_header)\n",
    "    \n",
    "    outbins_df = pd.DataFrame(outbins, columns=outbins_header.split(','))\n",
    "    \n",
    "    return outbins_df, z_bin_edges\n",
    "#     return z_bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\"\"\"\n",
    "Compute debris thickness through sub-debris and temperature inversion methods\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from scipy import ndimage\n",
    "import xarray as xr\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "from pygeotools.lib import malib, warplib, geolib, iolib, timelib\n",
    "# from imview.lib import pltlib\n",
    "\n",
    "\n",
    "import globaldebris_input as input\n",
    "\n",
    "#INPUT\n",
    "# topdir='/Users/davidrounce/Documents/Dave_Rounce/HiMAT/DEMs/'\n",
    "#Output directory\n",
    "# outdir = '/Users/davidrounce/Documents/Dave_Rounce/HiMAT/DEMs/Shean_2019_0213/mb_combined_20190213_nmad_bins/'\n",
    "\n",
    "outdir = input.output_fp + 'mb_bins/'\n",
    "outdir_fig = outdir + '/figures/'\n",
    "outdir_csv = outdir + '/csv/'\n",
    "\n",
    "if os.path.exists(outdir) == False:\n",
    "    os.makedirs(outdir)\n",
    "if os.path.exists(input.glac_shp_proj_fp) == False:\n",
    "    os.makedirs(input.glac_shp_proj_fp)\n",
    "if os.path.exists(outdir_csv) == False:\n",
    "    os.makedirs(outdir_csv)\n",
    "if os.path.exists(outdir_fig) == False:\n",
    "    os.makedirs(outdir_fig)\n",
    "\n",
    "\n",
    "csv_ending = '_mb_bins_wdc_emvel_offset.csv'\n",
    "verbose = False\n",
    "close_fig = True\n",
    "extra_layers = True\n",
    "\n",
    "# #RGI inventory\n",
    "# glac_str = '15.03473' # Ngozumpa\n",
    "\n",
    "# met_sample_fullfn = ('/Users/davidrounce/Documents/Dave_Rounce/DebrisGlaciers_WG/Melt_Intercomparison/' + \n",
    "#                      'rounce_model/hma_data/' + input.roi + '_ERA5-metdata_2000_2018-z.nc')\n",
    "# debris_elevstats_fullfn = ('/Users/davidrounce/Documents/Dave_Rounce/DebrisGlaciers_WG/Melt_Intercomparison/' +\n",
    "#                            'rounce_model/hma_data/' + input.roi + '_debris_elevstats.nc')\n",
    "\n",
    "# glac_shp_fn_dict = {'13':topdir + '../RGI/rgi60/13_rgi60_CentralAsia/13_rgi60_CentralAsia.shp',\n",
    "#                     '14':topdir + '../RGI/rgi60/14_rgi60_SouthAsiaWest/14_rgi60_SouthAsiaWest.shp',\n",
    "#                     '15':topdir + '../RGI/rgi60/15_rgi60_SouthAsiaEast/15_rgi60_SouthAsiaEast.shp'}\n",
    "\n",
    "# glac_shp_fn = glac_shp_fn_dict[region]\n",
    "# glacfeat_fn = outdir + 'glacfeat_list.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RGIId</th>\n",
       "      <th>GLIMSId</th>\n",
       "      <th>BgnDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>CenLon</th>\n",
       "      <th>CenLat</th>\n",
       "      <th>O1Region</th>\n",
       "      <th>O2Region</th>\n",
       "      <th>Area</th>\n",
       "      <th>Zmin</th>\n",
       "      <th>...</th>\n",
       "      <th>DC_EndDate</th>\n",
       "      <th>DC_CTSmean</th>\n",
       "      <th>DC_Area_%</th>\n",
       "      <th>layer</th>\n",
       "      <th>path</th>\n",
       "      <th>area_singl</th>\n",
       "      <th>DC_Area_v2</th>\n",
       "      <th>DC_Area__1</th>\n",
       "      <th>geometry</th>\n",
       "      <th>CenLon_360</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGI60-13.00604</td>\n",
       "      <td>G077997E35568N</td>\n",
       "      <td>20020802</td>\n",
       "      <td>-9999999</td>\n",
       "      <td>77.994225</td>\n",
       "      <td>35.576353</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>9.855</td>\n",
       "      <td>5401</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>20.314421</td>\n",
       "      <td>6.274</td>\n",
       "      <td>Fixed geometries_13</td>\n",
       "      <td>MultiPolygon?crs=EPSG:4326&amp;field=RGIId:string(...</td>\n",
       "      <td>178920</td>\n",
       "      <td>498099</td>\n",
       "      <td>5.054</td>\n",
       "      <td>MULTIPOLYGON (((77.97991 35.56762, 77.98057 35...</td>\n",
       "      <td>77.994225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RGI60-13.00611</td>\n",
       "      <td>G094298E30361N</td>\n",
       "      <td>19990721</td>\n",
       "      <td>-9999999</td>\n",
       "      <td>94.297566</td>\n",
       "      <td>30.362306</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>2.035</td>\n",
       "      <td>4133</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>14.447194</td>\n",
       "      <td>16.629</td>\n",
       "      <td>Fixed geometries_13</td>\n",
       "      <td>MultiPolygon?crs=EPSG:4326&amp;field=RGIId:string(...</td>\n",
       "      <td>339441</td>\n",
       "      <td>339441</td>\n",
       "      <td>16.680</td>\n",
       "      <td>POLYGON ((94.29031 30.36096, 94.29062 30.36096...</td>\n",
       "      <td>94.297566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RGI60-13.00643</td>\n",
       "      <td>G094928E30607N</td>\n",
       "      <td>19990923</td>\n",
       "      <td>-9999999</td>\n",
       "      <td>94.924077</td>\n",
       "      <td>30.606840</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>28.533</td>\n",
       "      <td>4345</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>15.933888</td>\n",
       "      <td>11.582</td>\n",
       "      <td>Fixed geometries_13</td>\n",
       "      <td>MultiPolygon?crs=EPSG:4326&amp;field=RGIId:string(...</td>\n",
       "      <td>2810534</td>\n",
       "      <td>3118320</td>\n",
       "      <td>10.929</td>\n",
       "      <td>MULTIPOLYGON (((94.90493 30.61977, 94.90524 30...</td>\n",
       "      <td>94.924077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RGI60-13.00713</td>\n",
       "      <td>G094777E30796N</td>\n",
       "      <td>19990923</td>\n",
       "      <td>-9999999</td>\n",
       "      <td>94.776375</td>\n",
       "      <td>30.796349</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>2.996</td>\n",
       "      <td>5085</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>35.635464</td>\n",
       "      <td>6.459</td>\n",
       "      <td>Fixed geometries_13</td>\n",
       "      <td>MultiPolygon?crs=EPSG:4326&amp;field=RGIId:string(...</td>\n",
       "      <td>26102</td>\n",
       "      <td>168314</td>\n",
       "      <td>5.618</td>\n",
       "      <td>MULTIPOLYGON (((94.78556 30.79927, 94.78619 30...</td>\n",
       "      <td>94.776375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RGI60-13.00757</td>\n",
       "      <td>G094632E30674N</td>\n",
       "      <td>19990923</td>\n",
       "      <td>-9999999</td>\n",
       "      <td>94.638690</td>\n",
       "      <td>30.669223</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>2.887</td>\n",
       "      <td>4619</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>18.331045</td>\n",
       "      <td>8.324</td>\n",
       "      <td>Fixed geometries_13</td>\n",
       "      <td>MultiPolygon?crs=EPSG:4326&amp;field=RGIId:string(...</td>\n",
       "      <td>111620</td>\n",
       "      <td>197136</td>\n",
       "      <td>6.828</td>\n",
       "      <td>MULTIPOLYGON (((94.64190 30.66558, 94.64252 30...</td>\n",
       "      <td>94.638690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2953</th>\n",
       "      <td>RGI60-15.13061</td>\n",
       "      <td>G097506E28969N</td>\n",
       "      <td>20091014</td>\n",
       "      <td>-9999999</td>\n",
       "      <td>97.510296</td>\n",
       "      <td>28.965740</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4.332</td>\n",
       "      <td>4608</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>6.183309</td>\n",
       "      <td>23.622</td>\n",
       "      <td>Fixed geometries_15</td>\n",
       "      <td>MultiPolygon?crs=EPSG:4326&amp;field=RGIId:string(...</td>\n",
       "      <td>45913</td>\n",
       "      <td>940764</td>\n",
       "      <td>21.717</td>\n",
       "      <td>MULTIPOLYGON (((97.51190 28.95634, 97.51344 28...</td>\n",
       "      <td>97.510296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>RGI60-15.13065</td>\n",
       "      <td>G097526E28985N</td>\n",
       "      <td>20091014</td>\n",
       "      <td>-9999999</td>\n",
       "      <td>97.530043</td>\n",
       "      <td>28.986540</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>6.516</td>\n",
       "      <td>4141</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7.996689</td>\n",
       "      <td>6.464</td>\n",
       "      <td>Fixed geometries_15</td>\n",
       "      <td>MultiPolygon?crs=EPSG:4326&amp;field=RGIId:string(...</td>\n",
       "      <td>38712</td>\n",
       "      <td>385319</td>\n",
       "      <td>5.913</td>\n",
       "      <td>MULTIPOLYGON (((97.53329 28.98960, 97.53360 28...</td>\n",
       "      <td>97.530043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>RGI60-15.13067</td>\n",
       "      <td>G097536E29002N</td>\n",
       "      <td>20091014</td>\n",
       "      <td>-9999999</td>\n",
       "      <td>97.536678</td>\n",
       "      <td>29.002900</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2.788</td>\n",
       "      <td>4445</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7.494829</td>\n",
       "      <td>8.651</td>\n",
       "      <td>Fixed geometries_15</td>\n",
       "      <td>MultiPolygon?crs=EPSG:4326&amp;field=RGIId:string(...</td>\n",
       "      <td>81025</td>\n",
       "      <td>171952</td>\n",
       "      <td>6.168</td>\n",
       "      <td>MULTIPOLYGON (((97.54210 28.99809, 97.54241 28...</td>\n",
       "      <td>97.536678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>RGI60-15.13092</td>\n",
       "      <td>G097617E28935N</td>\n",
       "      <td>20091014</td>\n",
       "      <td>-9999999</td>\n",
       "      <td>97.619880</td>\n",
       "      <td>28.934007</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4.619</td>\n",
       "      <td>4583</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>8.035735</td>\n",
       "      <td>15.257</td>\n",
       "      <td>Fixed geometries_15</td>\n",
       "      <td>MultiPolygon?crs=EPSG:4326&amp;field=RGIId:string(...</td>\n",
       "      <td>550995</td>\n",
       "      <td>642827</td>\n",
       "      <td>13.917</td>\n",
       "      <td>MULTIPOLYGON (((97.62523 28.92939, 97.62554 28...</td>\n",
       "      <td>97.619880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>RGI60-15.13104</td>\n",
       "      <td>G079456E31052N</td>\n",
       "      <td>20090730</td>\n",
       "      <td>-9999999</td>\n",
       "      <td>79.453200</td>\n",
       "      <td>31.048246</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>22.704</td>\n",
       "      <td>5235</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>66.676066</td>\n",
       "      <td>6.477</td>\n",
       "      <td>Fixed geometries_15</td>\n",
       "      <td>MultiPolygon?crs=EPSG:4326&amp;field=RGIId:string(...</td>\n",
       "      <td>1333229</td>\n",
       "      <td>1333229</td>\n",
       "      <td>5.872</td>\n",
       "      <td>POLYGON ((79.42237 31.07560, 79.42236 31.07614...</td>\n",
       "      <td>79.453200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2958 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               RGIId         GLIMSId   BgnDate   EndDate     CenLon  \\\n",
       "0     RGI60-13.00604  G077997E35568N  20020802  -9999999  77.994225   \n",
       "1     RGI60-13.00611  G094298E30361N  19990721  -9999999  94.297566   \n",
       "2     RGI60-13.00643  G094928E30607N  19990923  -9999999  94.924077   \n",
       "3     RGI60-13.00713  G094777E30796N  19990923  -9999999  94.776375   \n",
       "4     RGI60-13.00757  G094632E30674N  19990923  -9999999  94.638690   \n",
       "...              ...             ...       ...       ...        ...   \n",
       "2953  RGI60-15.13061  G097506E28969N  20091014  -9999999  97.510296   \n",
       "2954  RGI60-15.13065  G097526E28985N  20091014  -9999999  97.530043   \n",
       "2955  RGI60-15.13067  G097536E29002N  20091014  -9999999  97.536678   \n",
       "2956  RGI60-15.13092  G097617E28935N  20091014  -9999999  97.619880   \n",
       "2957  RGI60-15.13104  G079456E31052N  20090730  -9999999  79.453200   \n",
       "\n",
       "         CenLat O1Region O2Region    Area  Zmin  ...  DC_EndDate  DC_CTSmean  \\\n",
       "0     35.576353       13        5   9.855  5401  ...        2017   20.314421   \n",
       "1     30.362306       13        9   2.035  4133  ...        2017   14.447194   \n",
       "2     30.606840       13        9  28.533  4345  ...        2017   15.933888   \n",
       "3     30.796349       13        9   2.996  5085  ...        2017   35.635464   \n",
       "4     30.669223       13        9   2.887  4619  ...        2017   18.331045   \n",
       "...         ...      ...      ...     ...   ...  ...         ...         ...   \n",
       "2953  28.965740       15        3   4.332  4608  ...        2017    6.183309   \n",
       "2954  28.986540       15        3   6.516  4141  ...        2017    7.996689   \n",
       "2955  29.002900       15        3   2.788  4445  ...        2017    7.494829   \n",
       "2956  28.934007       15        3   4.619  4583  ...        2017    8.035735   \n",
       "2957  31.048246       15        1  22.704  5235  ...        2017   66.676066   \n",
       "\n",
       "      DC_Area_%                layer  \\\n",
       "0         6.274  Fixed geometries_13   \n",
       "1        16.629  Fixed geometries_13   \n",
       "2        11.582  Fixed geometries_13   \n",
       "3         6.459  Fixed geometries_13   \n",
       "4         8.324  Fixed geometries_13   \n",
       "...         ...                  ...   \n",
       "2953     23.622  Fixed geometries_15   \n",
       "2954      6.464  Fixed geometries_15   \n",
       "2955      8.651  Fixed geometries_15   \n",
       "2956     15.257  Fixed geometries_15   \n",
       "2957      6.477  Fixed geometries_15   \n",
       "\n",
       "                                                   path  area_singl  \\\n",
       "0     MultiPolygon?crs=EPSG:4326&field=RGIId:string(...      178920   \n",
       "1     MultiPolygon?crs=EPSG:4326&field=RGIId:string(...      339441   \n",
       "2     MultiPolygon?crs=EPSG:4326&field=RGIId:string(...     2810534   \n",
       "3     MultiPolygon?crs=EPSG:4326&field=RGIId:string(...       26102   \n",
       "4     MultiPolygon?crs=EPSG:4326&field=RGIId:string(...      111620   \n",
       "...                                                 ...         ...   \n",
       "2953  MultiPolygon?crs=EPSG:4326&field=RGIId:string(...       45913   \n",
       "2954  MultiPolygon?crs=EPSG:4326&field=RGIId:string(...       38712   \n",
       "2955  MultiPolygon?crs=EPSG:4326&field=RGIId:string(...       81025   \n",
       "2956  MultiPolygon?crs=EPSG:4326&field=RGIId:string(...      550995   \n",
       "2957  MultiPolygon?crs=EPSG:4326&field=RGIId:string(...     1333229   \n",
       "\n",
       "      DC_Area_v2  DC_Area__1  \\\n",
       "0         498099       5.054   \n",
       "1         339441      16.680   \n",
       "2        3118320      10.929   \n",
       "3         168314       5.618   \n",
       "4         197136       6.828   \n",
       "...          ...         ...   \n",
       "2953      940764      21.717   \n",
       "2954      385319       5.913   \n",
       "2955      171952       6.168   \n",
       "2956      642827      13.917   \n",
       "2957     1333229       5.872   \n",
       "\n",
       "                                               geometry  CenLon_360  \n",
       "0     MULTIPOLYGON (((77.97991 35.56762, 77.98057 35...   77.994225  \n",
       "1     POLYGON ((94.29031 30.36096, 94.29062 30.36096...   94.297566  \n",
       "2     MULTIPOLYGON (((94.90493 30.61977, 94.90524 30...   94.924077  \n",
       "3     MULTIPOLYGON (((94.78556 30.79927, 94.78619 30...   94.776375  \n",
       "4     MULTIPOLYGON (((94.64190 30.66558, 94.64252 30...   94.638690  \n",
       "...                                                 ...         ...  \n",
       "2953  MULTIPOLYGON (((97.51190 28.95634, 97.51344 28...   97.510296  \n",
       "2954  MULTIPOLYGON (((97.53329 28.98960, 97.53360 28...   97.530043  \n",
       "2955  MULTIPOLYGON (((97.54210 28.99809, 97.54241 28...   97.536678  \n",
       "2956  MULTIPOLYGON (((97.62523 28.92939, 97.62554 28...   97.619880  \n",
       "2957  POLYGON ((79.42237 31.07560, 79.42236 31.07614...   79.453200  \n",
       "\n",
       "[2958 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Debris cover extent shapefile with statistics\n",
    "dc_shp = gpd.read_file(input.debriscover_fp + input.debriscover_fn_dict[input.roi])\n",
    "dc_shp = dc_shp.sort_values(by=['RGIId'])\n",
    "\n",
    "# Subset by percent debris-covered or debris-covered area\n",
    "dc_shp_subset = dc_shp[((dc_shp['DC_Area__1'] > input.dc_percarea_threshold) | \n",
    "                        (dc_shp['DC_Area_v2'] / 1e6 > input.dc_area_threshold))\n",
    "                        & (dc_shp['Area'] > input.min_glac_area)].copy()\n",
    "dc_shp_subset.reset_index(inplace=True, drop=True)\n",
    "dc_shp_subset['CenLon_360'] = dc_shp_subset['CenLon']\n",
    "dc_shp_subset.loc[dc_shp_subset['CenLon_360'] < 0, 'CenLon_360'] = 360 + dc_shp_subset.loc[dc_shp_subset['CenLon_360'] < 0, 'CenLon_360']\n",
    "dc_shp_subset\n",
    "\n",
    "# # # ===== LOAD RGI DATA (works for multiple regions together) =====\n",
    "# # rgi_fns = []\n",
    "# # for i in os.listdir(input.rgi_fp):\n",
    "# # #     print(i)\n",
    "# #     for reg_no in input.roi_rgidict[input.roi]:\n",
    "# #         reg_str = str(reg_no).zfill(2)\n",
    "# #         if i.startswith(reg_str) and i.endswith('.csv'):\n",
    "# #             rgi_fns.append(i)\n",
    "# # rgi_fns = sorted(rgi_fns)\n",
    "\n",
    "# # # Load RGI data\n",
    "# # rgi_data = None\n",
    "# # for i in rgi_fns:\n",
    "# #     rgi_reg = pd.read_csv(input.rgi_fp + i)\n",
    "    \n",
    "# #     if rgi_data is None:\n",
    "# #         rgi_data = rgi_reg\n",
    "# #     else:\n",
    "# #         rgi_data = pd.concat((rgi_data, rgi_reg), axis=0)\n",
    "# # rgi_data.reset_index(inplace=True, drop=True)\n",
    "# # rgi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LOAD GLACIERS WITH LARSEN DATA =====\n",
    "dc_shp_subset['larsen_fullfn'] = np.nan\n",
    "larsen_fullfn_dict = {}\n",
    "if 'larsen' in input.mb_datasets:\n",
    "    mb_summary = pd.read_csv(input.larsen_fp + input.larsen_fn)\n",
    "    \n",
    "    # Find glaciers that are debris-covered\n",
    "    larsen_dc_rgiid = [value for value in list(mb_summary.RGIId.values) \n",
    "                       if value in list(dc_shp_subset.RGIId.values)]\n",
    "\n",
    "    mb_summary_dc = mb_summary[mb_summary['RGIId'].isin(larsen_dc_rgiid)]\n",
    "    mb_summary_dc = mb_summary_dc.sort_values('RGIId')\n",
    "    mb_summary_dc.reset_index(inplace=True, drop=True)\n",
    "    mb_summary_dc.loc[mb_summary_dc['name'] == 'Maclaren', 'name'] = 'MacLaren'\n",
    "    mb_summary_dc.loc[mb_summary_dc['name'] == 'Tlikakila Fork', 'name'] = 'TlikakilaGlacierFork'\n",
    "    mb_summary_dc.loc[mb_summary_dc['name'] == 'Tlikakila N. Fork', 'name'] = 'TlikakilaNorthFork'\n",
    "    mb_summary_dc['larsen_fullfn'] = np.nan\n",
    "    \n",
    "    for n, glac_name in enumerate(mb_summary_dc.name.values):\n",
    "#     for n, glac_name in enumerate([mb_summary_dc.name.values[47]]):\n",
    "#         print(n, glac_name)\n",
    "            \n",
    "        glac_name = glac_name.replace(' ', '')\n",
    "        glac_fns = []\n",
    "        start_yr = []\n",
    "        end_yr = []\n",
    "        for i in os.listdir(input.larsen_binned_fp):\n",
    "            if i.startswith(glac_name):\n",
    "                glac_fns.append(i)\n",
    "                start_yr.append(i.split('.')[1][0:4])\n",
    "                end_yr.append(i.split('.')[2][0:4])\n",
    "                \n",
    "        if len(glac_fns) > 0:\n",
    "            yr_dif = np.array(end_yr).astype(int) - np.array(start_yr).astype(int)\n",
    "            mb_fn = glac_fns[np.where(yr_dif == yr_dif.max())[0][0]]\n",
    "            \n",
    "            # ===== Process Larsen dataset =====\n",
    "            larsen_data_raw = np.genfromtxt(input.larsen_binned_fp + mb_fn, skip_header=3)\n",
    "            larsen_data_header = ['E', 'DZ', 'DZ25', 'DZ75', 'AAD', 'MassChange', 'MassBal', 'NumData']\n",
    "            larsen_data = pd.DataFrame(larsen_data_raw, columns=larsen_data_header)\n",
    "            larsen_data['std from DZ25'] = np.absolute(larsen_data['DZ'] - larsen_data['DZ25']) / 0.67\n",
    "            larsen_data['std from DZ75'] = np.absolute(larsen_data['DZ'] - larsen_data['DZ75']) / 0.67\n",
    "            larsen_data[' dhdt_bin_std_ma'] = (larsen_data['std from DZ25'] + larsen_data['std from DZ75']) / 2\n",
    "            larsen_data[' mb_bin_std_mwea'] = larsen_data[' dhdt_bin_std_ma'] * 900 / 1000\n",
    "            larsen_data['AAD'] = larsen_data['AAD'] / 1e6\n",
    "            larsen_data['startyear'] = int(mb_fn.split('.')[1][0:4])\n",
    "            larsen_data['endyear'] = int(mb_fn.split('.')[2][0:4])\n",
    "            larsen_data = larsen_data.rename({'E': '# bin_center_elev_m',\n",
    "                                              'DZ': ' dhdt_bin_mean_ma',\n",
    "                                              'MassBal': ' mb_bin_mean_mwea',\n",
    "                                              'AAD': ' z1_bin_area_valid_km2',\n",
    "                                             }, axis='columns')\n",
    "            new_fn = mb_summary_dc.loc[n,'RGIId'].split('-')[1][1:] + '_larsen_mb_bins.csv'\n",
    "            larsen_data.to_csv(input.larsen_binned_fp + new_fn, index=False)\n",
    "            \n",
    "            mb_summary_dc.loc[n, 'larsen_fullfn'] = input.larsen_binned_fp + new_fn\n",
    "            \n",
    "        else:\n",
    "            print(n, glac_name, 'has no file\\n')\n",
    "\n",
    "    mb_summary_dc.dropna(subset=['larsen_fullfn'], inplace=True)\n",
    "    mb_summary_dc.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    print('Larsen debris-covered glaciers:', mb_summary_dc.shape[0], '\\n\\n')\n",
    "    \n",
    "    larsen_fullfn_dict = dict(zip(mb_summary_dc['RGIId'].values, mb_summary_dc['larsen_fullfn'].values))\n",
    "#     print(larsen_fullfn_dict)\n",
    "    dc_shp_subset['larsen_fullfn'] = dc_shp_subset.RGIId.map(larsen_fullfn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LOAD GLACIERS WITH BRAUN DATA =====\n",
    "dc_shp_subset['braun_fullfn'] = np.nan\n",
    "braun_fullfn_dict = {}\n",
    "if 'braun' in input.mb_datasets:\n",
    "    mb_binned_fp = input.main_directory + '/../mb_data/Braun/binned_data/'\n",
    "    \n",
    "    mb_fns = []\n",
    "    braun_rgiids = []\n",
    "    for i in os.listdir(mb_binned_fp):\n",
    "        if i.endswith('_mb_bins.csv'):\n",
    "            mb_fns.append(mb_binned_fp + i)\n",
    "            rgiid_raw = i.split('_')[0]\n",
    "            rgiid = 'RGI60-' + rgiid_raw[0].zfill(2) + '.' + rgiid_raw.split('.')[1]\n",
    "            braun_rgiids.append(rgiid)\n",
    "    braun_fn_df = pd.DataFrame(np.zeros((len(mb_fns),2)), columns=['RGIId', 'braun_fn'])\n",
    "    braun_fn_df['RGIId'] = braun_rgiids\n",
    "    braun_fn_df['braun_fullfn'] = mb_fns\n",
    "    \n",
    "    # Find glaciers that are debris-covered\n",
    "    braun_dc_rgiid = [value for value in list(braun_fn_df.RGIId.values) \n",
    "                       if value in list(dc_shp_subset.RGIId.values)]\n",
    "    braun_fn_df_dc = braun_fn_df[braun_fn_df['RGIId'].isin(braun_dc_rgiid)]\n",
    "    braun_fn_df_dc = braun_fn_df_dc.sort_values('RGIId')\n",
    "    \n",
    "    print('Braun debris-covered glaciers:', braun_fn_df_dc.shape[0], '\\n\\n')\n",
    "    \n",
    "    braun_fullfn_dict = dict(zip(braun_fn_df_dc['RGIId'].values, braun_fn_df_dc['braun_fullfn'].values))\n",
    "#     print(braun_fullfn_dict)\n",
    "\n",
    "    dc_shp_subset['braun_fullfn'] = dc_shp_subset.RGIId.map(braun_fullfn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shean debris-covered glaciers: 2935 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== LOAD GLACIERS WITH BRAUN DATA =====\n",
    "dc_shp_subset['shean_fullfn'] = np.nan\n",
    "shean_fullfn_dict = {}\n",
    "if 'shean' in input.mb_datasets:\n",
    "    mb_binned_fp = input.main_directory + '/../mb_data/Shean_2019_0213/mb_combined_20190213_nmad_bins/'\n",
    "    \n",
    "    mb_fns = []\n",
    "    rgiids = []\n",
    "    for i in os.listdir(mb_binned_fp):\n",
    "        if i.endswith('_mb_bins.csv'):\n",
    "            mb_fns.append(mb_binned_fp + i)\n",
    "            rgiid_raw = i.split('_')[0]\n",
    "            rgiid = 'RGI60-' + rgiid_raw.split('.')[0].zfill(2) + '.' + rgiid_raw.split('.')[1]\n",
    "            rgiids.append(rgiid)\n",
    "    mb_fn_df = pd.DataFrame(np.zeros((len(mb_fns),2)), columns=['RGIId', 'mb_fn'])\n",
    "    mb_fn_df['RGIId'] = rgiids\n",
    "    mb_fn_df['mb_fullfn'] = mb_fns\n",
    "    \n",
    "    # Find glaciers that are debris-covered\n",
    "    mb_dc_rgiid = [value for value in list(mb_fn_df.RGIId.values) \n",
    "                   if value in list(dc_shp_subset.RGIId.values)]\n",
    "    mb_fn_df_dc = mb_fn_df[mb_fn_df['RGIId'].isin(mb_dc_rgiid)]\n",
    "    mb_fn_df_dc = mb_fn_df_dc.sort_values('RGIId')\n",
    "    \n",
    "    print('shean debris-covered glaciers:', mb_fn_df_dc.shape[0], '\\n\\n')\n",
    "    \n",
    "    shean_fullfn_dict = dict(zip(mb_fn_df_dc['RGIId'].values, mb_fn_df_dc['mb_fullfn'].values))\n",
    "#     print(shea_fullfn_dict)\n",
    "    dc_shp_subset['shean_fullfn'] = dc_shp_subset.RGIId.map(shean_fullfn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dictionaries together\n",
    "mb_fn_dict = dict(list(larsen_fullfn_dict.items()) + list(braun_fullfn_dict.items()) + \n",
    "                  list(shean_fullfn_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique lat/lons: 690 \n",
      "\n",
      "\n",
      "[(44.75, 80.0), (44.0, 83.5), (44.0, 83.75), (44.0, 84.0), (43.75, 84.5), (43.75, 84.75), (43.75, 85.0), (43.75, 85.75), (43.5, 85.0), (43.25, 77.5), (43.0, 76.75), (43.0, 77.0), (43.0, 77.25), (43.0, 77.5), (43.0, 77.75), (42.75, 76.75), (42.75, 77.0), (42.75, 77.25), (42.75, 82.75), (42.5, 74.5), (42.5, 74.75), (42.5, 75.0), (42.5, 75.25), (42.5, 80.5), (42.5, 80.75), (42.5, 81.0), (42.5, 81.25), (42.5, 81.75), (42.5, 82.0), (42.5, 82.25), (42.5, 82.5), (42.5, 85.25), (42.25, 78.25), (42.25, 78.5), (42.25, 78.75), (42.25, 79.0), (42.25, 79.25), (42.25, 79.5), (42.25, 79.75), (42.25, 80.0), (42.25, 80.25), (42.25, 80.5), (42.25, 80.75), (42.25, 81.0), (42.25, 81.25), (42.25, 81.5), (42.25, 81.75), (42.0, 72.0), (42.0, 76.5), (42.0, 76.75), (42.0, 77.0), (42.0, 77.25), (42.0, 77.5), (42.0, 77.75), (42.0, 78.0), (42.0, 78.25), (42.0, 78.5), (42.0, 78.75), (42.0, 79.5), (42.0, 79.75), (42.0, 80.0), (42.0, 80.25), (42.0, 80.5), (42.0, 80.75), (41.75, 77.25), (41.75, 77.5), (41.75, 78.25), (41.75, 78.5), (41.75, 78.75), (41.75, 79.0), (41.75, 80.0), (41.75, 80.25), (41.5, 77.25), (41.5, 77.5), (41.5, 77.75), (41.5, 78.75), (41.25, 77.75), (41.25, 78.25), (41.25, 78.5), (41.0, 75.75), (41.0, 77.5), (41.0, 77.75), (40.75, 74.25), (40.75, 76.75), (40.0, 72.5), (39.75, 70.5), (39.75, 70.75), (39.75, 71.25), (39.75, 71.5), (39.75, 71.75), (39.75, 72.0), (39.75, 72.5), (39.5, 70.0), (39.5, 70.25), (39.5, 70.5), (39.5, 70.75), (39.5, 71.0), (39.5, 71.25), (39.5, 71.5), (39.5, 72.0), (39.5, 72.5), (39.5, 72.75), (39.5, 73.0), (39.5, 73.25), (39.5, 73.5), (39.5, 73.75), (39.5, 74.0), (39.25, 69.5), (39.25, 69.75), (39.25, 70.0), (39.25, 70.25), (39.25, 71.0), (39.25, 71.75), (39.25, 72.0), (39.25, 72.25), (39.25, 72.5), (39.25, 72.75), (39.25, 73.0), (39.25, 73.25), (39.25, 73.5), (39.25, 74.25), (39.25, 74.5), (39.25, 74.75), (39.25, 75.0), (39.0, 68.0), (39.0, 68.5), (39.0, 70.75), (39.0, 71.0), (39.0, 71.25), (39.0, 71.5), (39.0, 71.75), (39.0, 72.0), (39.0, 72.25), (39.0, 72.5), (39.0, 72.75), (39.0, 73.0), (39.0, 73.75), (39.0, 74.75), (39.0, 75.0), (38.75, 71.0), (38.75, 71.25), (38.75, 71.5), (38.75, 71.75), (38.75, 72.0), (38.75, 72.25), (38.75, 72.5), (38.75, 72.75), (38.75, 73.0), (38.75, 73.25), (38.75, 73.75), (38.75, 75.0), (38.75, 75.25), (38.75, 75.5), (38.5, 71.25), (38.5, 71.5), (38.5, 71.75), (38.5, 72.0), (38.5, 72.25), (38.5, 72.5), (38.5, 72.75), (38.5, 73.25), (38.5, 73.5), (38.5, 75.25), (38.5, 75.5), (38.25, 71.0), (38.25, 71.25), (38.25, 71.75), (38.25, 72.0), (38.25, 72.25), (38.25, 72.5), (38.25, 72.75), (38.25, 73.25), (38.25, 75.0), (38.25, 75.25), (38.25, 75.5), (38.0, 71.0), (38.0, 71.75), (38.0, 72.0), (38.0, 72.25), (38.0, 72.5), (38.0, 72.75), (38.0, 73.0), (38.0, 73.25), (38.0, 87.25), (38.0, 87.5), (37.75, 71.75), (37.75, 72.0), (37.75, 72.25), (37.75, 72.75), (37.5, 71.25), (37.5, 72.0), (37.5, 72.25), (37.5, 75.25), (37.25, 71.75), (37.25, 72.5), (37.25, 73.25), (37.25, 73.5), (37.25, 73.75), (37.25, 75.0), (37.25, 75.25), (37.25, 85.75), (37.25, 86.0), (37.0, 71.25), (37.0, 71.75), (37.0, 72.0), (37.0, 72.25), (37.0, 72.75), (37.0, 73.0), (37.0, 73.25), (37.0, 73.5), (37.0, 73.75), (37.0, 74.0), (37.0, 74.25), (37.0, 74.75), (37.0, 75.0), (37.0, 75.25), (36.75, 72.0), (36.75, 72.25), (36.75, 72.5), (36.75, 72.75), (36.75, 73.0), (36.75, 73.25), (36.75, 73.5), (36.75, 73.75), (36.75, 74.0), (36.75, 74.25), (36.75, 74.5), (36.75, 74.75), (36.75, 75.0), (36.75, 75.25), (36.75, 75.5), (36.75, 75.75), (36.75, 76.0), (36.75, 76.25), (36.75, 77.25), (36.75, 77.75), (36.75, 78.25), (36.75, 78.5), (36.75, 84.5), (36.75, 84.75), (36.75, 85.0), (36.75, 85.25), (36.5, 70.5), (36.5, 71.5), (36.5, 71.75), (36.5, 72.0), (36.5, 72.25), (36.5, 72.75), (36.5, 73.0), (36.5, 73.25), (36.5, 73.5), (36.5, 73.75), (36.5, 74.0), (36.5, 74.25), (36.5, 74.5), (36.5, 74.75), (36.5, 75.0), (36.5, 75.25), (36.5, 75.5), (36.5, 75.75), (36.5, 76.0), (36.5, 77.5), (36.5, 77.75), (36.5, 78.25), (36.25, 69.75), (36.25, 70.25), (36.25, 70.5), (36.25, 71.0), (36.25, 71.25), (36.25, 71.75), (36.25, 72.0), (36.25, 72.25), (36.25, 72.5), (36.25, 72.75), (36.25, 73.0), (36.25, 74.0), (36.25, 74.25), (36.25, 74.5), (36.25, 74.75), (36.25, 75.0), (36.25, 75.25), (36.25, 75.5), (36.25, 75.75), (36.25, 76.0), (36.25, 76.25), (36.25, 78.5), (36.25, 78.75), (36.25, 79.0), (36.25, 79.25), (36.25, 79.5), (36.25, 82.0), (36.25, 82.25), (36.25, 82.5), (36.0, 70.5), (36.0, 70.75), (36.0, 71.0), (36.0, 71.25), (36.0, 72.25), (36.0, 72.5), (36.0, 72.75), (36.0, 73.0), (36.0, 74.5), (36.0, 74.75), (36.0, 75.0), (36.0, 75.25), (36.0, 75.5), (36.0, 75.75), (36.0, 76.0), (36.0, 76.25), (36.0, 76.5), (36.0, 76.75), (36.0, 79.5), (36.0, 79.75), (36.0, 80.0), (36.0, 80.25), (36.0, 80.75), (36.0, 81.0), (36.0, 81.25), (36.0, 81.5), (36.0, 81.75), (35.75, 70.5), (35.75, 70.75), (35.75, 71.0), (35.75, 71.25), (35.75, 72.25), (35.75, 72.5), (35.75, 72.75), (35.75, 73.0), (35.75, 73.25), (35.75, 74.75), (35.75, 75.0), (35.75, 75.25), (35.75, 75.5), (35.75, 75.75), (35.75, 76.0), (35.75, 76.25), (35.75, 76.5), (35.75, 76.75), (35.75, 77.0), (35.75, 80.25), (35.75, 80.5), (35.5, 70.75), (35.5, 72.5), (35.5, 72.75), (35.5, 73.0), (35.5, 75.0), (35.5, 75.25), (35.5, 75.5), (35.5, 75.75), (35.5, 76.0), (35.5, 76.25), (35.5, 76.5), (35.5, 76.75), (35.5, 77.0), (35.5, 77.25), (35.5, 77.5), (35.5, 78.0), (35.5, 80.75), (35.5, 81.0), (35.25, 72.75), (35.25, 73.0), (35.25, 73.5), (35.25, 74.5), (35.25, 74.75), (35.25, 75.0), (35.25, 75.25), (35.25, 76.25), (35.25, 76.5), (35.25, 76.75), (35.25, 77.0), (35.25, 77.25), (35.25, 77.5), (35.25, 77.75), (35.0, 73.25), (35.0, 73.5), (35.0, 74.25), (35.0, 74.5), (35.0, 74.75), (35.0, 76.75), (35.0, 77.0), (35.0, 77.25), (35.0, 77.5), (35.0, 77.75), (35.0, 78.25), (34.75, 73.75), (34.75, 74.0), (34.75, 76.75), (34.75, 77.0), (34.75, 77.25), (34.75, 77.5), (34.75, 77.75), (34.75, 78.0), (34.75, 78.25), (34.75, 78.5), (34.5, 75.5), (34.5, 77.0), (34.5, 77.25), (34.5, 77.75), (34.5, 78.0), (34.5, 78.25), (34.25, 75.25), (34.25, 75.5), (34.25, 75.75), (34.25, 76.0), (34.25, 78.0), (34.25, 78.25), (34.25, 78.5), (34.0, 75.75), (34.0, 76.0), (34.0, 76.25), (34.0, 76.5), (34.0, 78.25), (33.75, 75.75), (33.75, 76.0), (33.75, 76.25), (33.75, 77.5), (33.75, 78.25), (33.75, 78.5), (33.5, 76.0), (33.5, 76.25), (33.5, 76.5), (33.5, 76.75), (33.5, 77.25), (33.5, 91.25), (33.5, 94.75), (33.25, 76.25), (33.25, 76.5), (33.25, 76.75), (33.25, 77.0), (33.25, 91.25), (33.25, 92.0), (33.0, 76.25), (33.0, 76.5), (33.0, 76.75), (33.0, 77.0), (33.0, 77.25), (33.0, 78.25), (33.0, 78.5), (33.0, 92.0), (32.75, 76.5), (32.75, 76.75), (32.75, 77.0), (32.75, 77.25), (32.75, 77.5), (32.75, 77.75), (32.5, 76.5), (32.5, 76.75), (32.5, 77.0), (32.5, 77.25), (32.5, 77.5), (32.5, 77.75), (32.5, 78.0), (32.5, 78.5), (32.5, 78.75), (32.5, 79.0), (32.25, 76.75), (32.25, 77.0), (32.25, 77.25), (32.25, 77.5), (32.25, 77.75), (32.25, 78.5), (32.0, 77.5), (32.0, 77.75), (32.0, 78.0), (32.0, 78.5), (32.0, 78.75), (31.75, 77.5), (31.75, 77.75), (31.75, 78.0), (31.75, 78.25), (31.75, 78.75), (31.75, 94.75), (31.75, 99.0), (31.5, 78.5), (31.5, 100.25), (31.25, 78.25), (31.25, 78.5), (31.25, 78.75), (31.0, 78.5), (31.0, 78.75), (31.0, 79.0), (31.0, 79.25), (31.0, 79.5), (31.0, 79.75), (31.0, 81.25), (31.0, 93.75), (30.75, 78.75), (30.75, 79.0), (30.75, 79.25), (30.75, 79.5), (30.75, 79.75), (30.75, 80.0), (30.75, 91.5), (30.75, 94.0), (30.75, 94.25), (30.75, 94.5), (30.75, 94.75), (30.75, 95.0), (30.75, 95.25), (30.75, 99.5), (30.5, 79.75), (30.5, 80.0), (30.5, 80.25), (30.5, 80.5), (30.5, 80.75), (30.5, 81.25), (30.5, 83.25), (30.5, 86.5), (30.5, 90.5), (30.5, 93.25), (30.5, 93.5), (30.5, 93.75), (30.5, 94.0), (30.5, 94.25), (30.5, 94.5), (30.5, 94.75), (30.5, 95.0), (30.5, 95.25), (30.5, 99.5), (30.25, 79.75), (30.25, 80.0), (30.25, 80.25), (30.25, 80.5), (30.25, 80.75), (30.25, 81.5), (30.25, 81.75), (30.25, 82.0), (30.25, 82.25), (30.25, 90.5), (30.25, 93.5), (30.25, 93.75), (30.25, 94.0), (30.25, 94.25), (30.25, 94.5), (30.25, 94.75), (30.25, 95.0), (30.25, 95.25), (30.25, 95.5), (30.25, 95.75), (30.0, 80.5), (30.0, 81.0), (30.0, 81.25), (30.0, 81.5), (30.0, 82.0), (30.0, 82.25), (30.0, 82.5), (30.0, 84.5), (30.0, 85.0), (30.0, 90.0), (30.0, 94.25), (30.0, 94.5), (30.0, 94.75), (30.0, 95.0), (30.0, 95.25), (30.0, 95.5), (30.0, 95.75), (30.0, 96.0), (29.75, 81.0), (29.75, 81.5), (29.75, 82.25), (29.75, 82.5), (29.75, 82.75), (29.75, 83.0), (29.75, 84.5), (29.75, 84.75), (29.75, 94.75), (29.75, 95.0), (29.75, 95.25), (29.75, 95.75), (29.75, 96.0), (29.75, 96.5), (29.75, 97.25), (29.75, 99.5), (29.75, 101.75), (29.75, 102.0), (29.5, 82.5), (29.5, 82.75), (29.5, 95.0), (29.5, 95.25), (29.5, 96.0), (29.5, 96.25), (29.5, 96.5), (29.5, 96.75), (29.5, 97.0), (29.5, 97.25), (29.5, 97.5), (29.5, 101.75), (29.5, 102.0), (29.25, 82.5), (29.25, 82.75), (29.25, 95.0), (29.25, 96.0), (29.25, 96.25), (29.25, 96.5), (29.25, 96.75), (29.25, 97.0), (29.25, 97.25), (29.0, 83.5), (29.0, 83.75), (29.0, 84.25), (29.0, 90.25), (29.0, 96.25), (29.0, 96.5), (29.0, 96.75), (29.0, 97.0), (29.0, 97.25), (29.0, 97.5), (29.0, 97.75), (28.75, 83.0), (28.75, 83.25), (28.75, 83.5), (28.75, 83.75), (28.75, 84.0), (28.75, 84.25), (28.75, 84.5), (28.75, 85.5), (28.75, 93.5), (28.75, 96.5), (28.75, 97.75), (28.75, 98.25), (28.5, 83.75), (28.5, 84.0), (28.5, 84.25), (28.5, 84.5), (28.5, 84.75), (28.5, 85.0), (28.5, 85.25), (28.5, 85.5), (28.5, 85.75), (28.5, 96.5), (28.5, 97.5), (28.5, 98.25), (28.5, 98.5), (28.5, 98.75), (28.25, 85.0), (28.25, 85.25), (28.25, 85.5), (28.25, 85.75), (28.25, 86.0), (28.25, 86.25), (28.25, 86.5), (28.25, 86.75), (28.25, 87.5), (28.25, 90.0), (28.25, 90.25), (28.25, 90.5), (28.25, 90.75), (28.25, 91.25), (28.25, 91.5), (28.25, 92.75), (28.25, 97.0), (28.25, 97.5), (28.25, 98.75), (28.0, 86.0), (28.0, 86.25), (28.0, 86.5), (28.0, 86.75), (28.0, 87.0), (28.0, 87.25), (28.0, 87.5), (28.0, 87.75), (28.0, 88.0), (28.0, 88.25), (28.0, 88.5), (28.0, 88.75), (28.0, 89.0), (28.0, 89.5), (28.0, 89.75), (28.0, 90.0), (28.0, 90.25), (28.0, 90.5), (28.0, 90.75), (28.0, 91.25), (28.0, 91.5), (28.0, 91.75), (28.0, 92.5), (28.0, 92.75), (27.75, 86.5), (27.75, 86.75), (27.75, 87.0), (27.75, 87.25), (27.75, 87.75), (27.75, 88.0), (27.75, 88.25), (27.75, 88.75), (27.75, 89.25), (27.75, 92.25), (27.75, 92.5), (27.5, 88.0), (27.5, 88.25)]\n"
     ]
    }
   ],
   "source": [
    "# ===== SELECT GLACIERS WITH DATA ====\n",
    "dc_shp_subset_wdata = dc_shp_subset.dropna(subset=['larsen_fullfn', 'braun_fullfn', 'shean_fullfn'], how='all').copy()\n",
    "dc_shp_subset_wdata.reset_index(inplace=True, drop=True)\n",
    "ds = xr.open_dataset(input.metdata_fp + '../' + input.metdata_elev_fn)\n",
    "#  argmin() finds the minimum distance between the glacier lat/lon and the GCM pixel\n",
    "lat_nearidx = (np.abs(dc_shp_subset_wdata['CenLat'].values[:,np.newaxis] - \n",
    "                      ds['latitude'][:].values).argmin(axis=1))\n",
    "lon_nearidx = (np.abs(dc_shp_subset_wdata['CenLon_360'].values[:,np.newaxis] - \n",
    "                      ds['longitude'][:].values).argmin(axis=1))\n",
    "\n",
    "latlon_nearidx = list(zip(lat_nearidx, lon_nearidx))\n",
    "latlon_nearidx_unique = sorted(list(set(latlon_nearidx)))\n",
    "dc_shp_subset_wdata['latlon_nearidx'] = latlon_nearidx\n",
    "latlon_unique_dict = dict(zip(latlon_nearidx_unique,np.arange(0,len(latlon_nearidx_unique))))\n",
    "latlon_unique_dict_reversed = dict(zip(np.arange(0,len(latlon_nearidx_unique)),latlon_nearidx_unique))\n",
    "dc_shp_subset_wdata['latlon_unique_no'] = dc_shp_subset_wdata['latlon_nearidx'].map(latlon_unique_dict)\n",
    "\n",
    "print('unique lat/lons:', len(np.unique(dc_shp_subset_wdata['latlon_unique_no'])), '\\n\\n')\n",
    "# print(dc_shp_subset_wdata.loc[0:5,['RGIId', 'CenLat', 'CenLon', 'larsen_fn', 'braun_fn', 'latlon_unique_no']])\n",
    "\n",
    "lat_list = np.array([ds.latitude[x[0]].values for x in latlon_nearidx_unique])\n",
    "lon_list = np.array([ds.longitude[x[1]].values for x in latlon_nearidx_unique])\n",
    "latlon_list = list(tuple(zip(list(lat_list), list(lon_list))))\n",
    "\n",
    "print(latlon_list)\n",
    "\n",
    "# Pickle unique lat/lons that will be used for melt model\n",
    "with open(input.latlon_unique_fp + input.latlon_unique_dict[input.roi], 'wb') as f:\n",
    "    pickle.dump(latlon_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1090 glaciers in region 13 are included in this model run: ['00604', '00611', '00643', '00713', '00757', '00761', '00763', '00777', '00788', '00809', '00830', '00834', '00838', '00880', '00884', '00885', '00891', '00905', '00906', '00940', '00949', '00951', '00954', '00956', '00964', '00965', '00967', '00972', '00982', '00995', '00997', '00999', '01019', '01022', '01023', '01027', '01038', '01044', '01045', '01050', '01098', '01099', '01113', '01124', '01129', '01136', '01144', '01145', '01148', '01150'] and more\n",
      "1041 glaciers in region 14 are included in this model run: ['00005', '00018', '00032', '00036', '00043', '00057', '00072', '00104', '00145', '00163', '00222', '00287', '00353', '00363', '00471', '00543', '00548', '00555', '00595', '00700', '00722', '00742', '00764', '00767', '00796', '00805', '00850', '00891', '00899', '00952', '01001', '01022', '01070', '01075', '01165', '01191', '01206', '01226', '01228', '01244', '01285', '01361', '01379', '01391', '01400', '01409', '01425', '01454', '01474', '01489'] and more\n",
      "804 glaciers in region 15 are included in this model run: ['00026', '00055', '00057', '00186', '00232', '00233', '00234', '00355', '00356', '00368', '00379', '00399', '00406', '00423', '00475', '00503', '00612', '00617', '00621', '00655', '00835', '00850', '00868', '00869', '00872', '00880', '00881', '00885', '00894', '00898', '00899', '00909', '00910', '00911', '00920', '00957', '00996', '01004', '01024', '01030', '01031', '01032', '01062', '01077', '01078', '01087', '01089', '01094', '01096', '01098'] and more\n",
      "This study is focusing on 2935 glaciers in region [13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "# ===== Load Glaciers =====\n",
    "rgiid_list = [x.split('-')[1] for x in dc_shp_subset_wdata['RGIId'].values]\n",
    "main_glac_rgi = input.selectglaciersrgitable(rgiid_list)\n",
    "# add filenames\n",
    "main_glac_rgi['mb_fn'] = np.nan\n",
    "main_glac_rgi['mb_fn'] = main_glac_rgi.RGIId.map(mb_fn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n\\nDELETE ME AFTER TESTING\\n\\n')\n",
    "# # rgiid_list = ['01.15645']\n",
    "# rgiid_list = ['15.03473']\n",
    "# main_glac_rgi = input.selectglaciersrgitable(rgiid_list)\n",
    "# # add filenames\n",
    "# main_glac_rgi['mb_fn'] = np.nan\n",
    "# main_glac_rgi['mb_fn'] = main_glac_rgi.RGIId.map(mb_fn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 RGI60-13.00604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "HACK TO BYPASS VALID AREA\n",
      "\n",
      "\n",
      "1 1 RGI60-13.00611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 RGI60-13.00643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3 RGI60-13.00713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 RGI60-13.00757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5 RGI60-13.00761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6 RGI60-13.00763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 7 RGI60-13.00777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8 RGI60-13.00788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 9 RGI60-13.00809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 RGI60-13.00830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 11 RGI60-13.00834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12 RGI60-13.00838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 13 RGI60-13.00880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 14 RGI60-13.00884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 15 RGI60-13.00885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 16 RGI60-13.00891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 17 RGI60-13.00905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 18 RGI60-13.00906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 19 RGI60-13.00940\n"
     ]
    }
   ],
   "source": [
    "for nglac, glac_idx in enumerate(main_glac_rgi.index.values):\n",
    "# for nglac, glac_idx in enumerate(main_glac_rgi.index.values[784:]):\n",
    "# for nglac, glac_idx in enumerate([main_glac_rgi.index.values[502]]):\n",
    "# for nglac, glac_idx in enumerate([main_glac_rgi.index.values[0]]):\n",
    "    glac_str = main_glac_rgi.loc[glac_idx,'rgino_str']\n",
    "    rgiid = main_glac_rgi.loc[glac_idx,'RGIId']\n",
    "    \n",
    "    print(nglac, glac_idx, rgiid)\n",
    "    \n",
    "    out_csv_fn = os.path.join(outdir_csv, glac_str + csv_ending)\n",
    "    if verbose:\n",
    "        print('output_fn:', out_csv_fn)\n",
    "\n",
    "    if not os.path.exists(out_csv_fn):\n",
    "    \n",
    "        region = glac_str.split('.')[0]\n",
    "\n",
    "        # Shape layer processing\n",
    "        glac_shp_init = gpd.read_file(input.glac_shp_fn_dict[region])\n",
    "        dc_shp_init = gpd.read_file(input.debriscover_fp + input.debriscover_fn_dict[input.roi])\n",
    "        if verbose:\n",
    "            print('Shp init crs:', glac_shp_init.crs)\n",
    "\n",
    "        glac_shp_single = glac_shp_init[glac_shp_init['RGIId'] == rgiid]\n",
    "        glac_shp_single = glac_shp_single.reset_index()\n",
    "        dc_shp_single = dc_shp_init[dc_shp_init['RGIId'] == rgiid]\n",
    "        dc_shp_single = dc_shp_single.reset_index()\n",
    "\n",
    "        # Project shapefile\n",
    "        huss_dir = input.huss_dir_sample.replace('XXXX',str(region.zfill(2)))\n",
    "        huss_fn = input.huss_fn_sample.replace('XXXX',glac_str)\n",
    "\n",
    "        proj_fn = os.path.join(huss_dir, huss_fn) # THIS PROJECTION IS KEY!\n",
    "        ds = gdal.Open(proj_fn)\n",
    "        prj = ds.GetProjection()\n",
    "        srs = osr.SpatialReference(wkt=prj)\n",
    "        aea_srs = srs\n",
    "\n",
    "        # If projected shapefile already exists, then skip projection\n",
    "        glac_shp_proj_fn = input.glac_shp_proj_fp + glac_str + '_crs' + str(aea_srs.GetAttrValue(\"AUTHORITY\", 1)) + '.shp'\n",
    "        dc_shp_proj_fn = input.glac_shp_proj_fp + glac_str + '_dc_crs' + str(aea_srs.GetAttrValue(\"AUTHORITY\", 1)) + '.shp'\n",
    "\n",
    "        if os.path.exists(glac_shp_proj_fn) == False:\n",
    "            glac_shp_proj = glac_shp_single.to_crs({'init': 'epsg:' + str(aea_srs.GetAttrValue(\"AUTHORITY\", 1))})\n",
    "            glac_shp_proj.to_file(glac_shp_proj_fn)\n",
    "            \n",
    "        if os.path.exists(dc_shp_proj_fn) == False:\n",
    "            dc_shp_proj = dc_shp_single.to_crs({'init': 'epsg:' + str(aea_srs.GetAttrValue(\"AUTHORITY\", 1))})\n",
    "            dc_shp_proj.to_file(dc_shp_proj_fn)\n",
    "        \n",
    "\n",
    "        glac_shp_ds = ogr.Open(glac_shp_proj_fn, 0)\n",
    "        glac_shp_lyr = glac_shp_ds.GetLayer()\n",
    "        #This should be contained in features\n",
    "        glac_shp_srs = glac_shp_lyr.GetSpatialRef()\n",
    "        feat_count = glac_shp_lyr.GetFeatureCount()\n",
    "        if verbose:\n",
    "            print(\"Input glacier polygon count: %i\" % feat_count)\n",
    "            \n",
    "        dc_shp_ds = ogr.Open(dc_shp_proj_fn, 0)\n",
    "        dc_shp_lyr = dc_shp_ds.GetLayer()\n",
    "        #This should be contained in features\n",
    "        dc_shp_srs = dc_shp_lyr.GetSpatialRef()\n",
    "        feat_count = dc_shp_lyr.GetFeatureCount()\n",
    "        if verbose:\n",
    "            print(\"Input glacier polygon count (debris cover): %i\" % feat_count)\n",
    "\n",
    "        # Load DEM\n",
    "        z1_dir = input.z1_dir_sample.replace('XXXX',str(region.zfill(2)))\n",
    "        z1_fn = input.z1_fn_sample.replace('XXXX',glac_str)\n",
    "        z1_ds = gdal.Open(z1_dir + z1_fn)\n",
    "        z1_int_geom = geolib.ds_geom_intersection([z1_ds, z1_ds], t_srs=glac_shp_srs)\n",
    "\n",
    "        glacfeat_list = []\n",
    "        glacname_fieldname = \"Name\"\n",
    "        #RGIId (String) = RGI50-01.00004\n",
    "        glacnum_fieldname = \"RGIId\"\n",
    "        glacnum_fmt = '%08.5f'\n",
    "\n",
    "        for n, feat in enumerate(glac_shp_lyr):\n",
    "            gf = GlacFeat(feat, glacname_fieldname, glacnum_fieldname)\n",
    "            if verbose:\n",
    "                print(\"%i of %i: %s\" % (n+1, feat_count, gf.feat_fn))\n",
    "            #NOTE: Input must be in projected coordinate system, ideally equal area\n",
    "            #Should check this and reproject\n",
    "            gf.geom_attributes(srs=aea_srs)\n",
    "            glacfeat_list.append(gf)\n",
    "\n",
    "        if verbose:\n",
    "            print(gf.feat_fn)\n",
    "        \n",
    "        fn_dict = OrderedDict()\n",
    "        #We at least want to warp the two input DEMs\n",
    "        fn_dict['z1'] = os.path.join(z1_dir, z1_fn)\n",
    "\n",
    "        if extra_layers and (gf.glac_area_km2 > input.min_glac_area_writeout):\n",
    "            if verbose:\n",
    "                print(gf.glacnum)\n",
    "\n",
    "            # Ice thickness data\n",
    "            ice_thick_fn = os.path.join(huss_dir, huss_fn)\n",
    "            if os.path.exists(ice_thick_fn):\n",
    "                fn_dict['ice_thick'] = ice_thick_fn\n",
    "\n",
    "            if verbose:\n",
    "                print(fn_dict['ice_thick'])\n",
    "\n",
    "            # Surface velocity\n",
    "            if os.path.exists(input.v_dir + input.vx_fn_dict[input.roi]):\n",
    "                fn_dict['vx'] = input.v_dir + input.vx_fn_dict[input.roi]\n",
    "                fn_dict['vy'] = input.v_dir + input.vy_fn_dict[input.roi]\n",
    "                \n",
    "\n",
    "    #         if os.path.exists(ts_fullfn):\n",
    "    #             fn_dict['ts'] = ts_fullfn\n",
    "\n",
    "    #         if os.path.exists(debris_fullfn):\n",
    "    #             fn_dict['debris_thick_ts'] = debris_fullfn\n",
    "\n",
    "        #Expand extent to include buffered region around glacier polygon\n",
    "        warp_extent = geolib.pad_extent(gf.glac_geom_extent, width=input.buff_dist)\n",
    "        if verbose:\n",
    "            print(\"Expanding extent\")\n",
    "            print(gf.glac_geom_extent)\n",
    "            print(warp_extent)\n",
    "            print(aea_srs)\n",
    "\n",
    "        #Warp everything to common res/extent/proj\n",
    "        ds_list = warplib.memwarp_multi_fn(fn_dict.values(), res='min', \\\n",
    "                extent=warp_extent, t_srs=aea_srs, verbose=verbose, \\\n",
    "                r='cubic')\n",
    "\n",
    "        ds_dict = dict(zip(fn_dict.keys(), ds_list))\n",
    "\n",
    "        if verbose:\n",
    "            print(ds_list)\n",
    "            print(fn_dict.keys())\n",
    "\n",
    "        #Prepare mask for all glaciers within buffered area, not just the current glacier polygon\n",
    "        glac_shp_ds = ogr.Open(glac_shp_proj_fn, 0)\n",
    "        glac_shp_lyr = glac_shp_ds.GetLayer()\n",
    "        \n",
    "        dc_shp_ds = ogr.Open(dc_shp_proj_fn, 0)\n",
    "        dc_shp_lyr = dc_shp_ds.GetLayer()\n",
    "\n",
    "        #Get global glacier mask\n",
    "        #Want this to be True over ALL glacier surfaces, not just the current polygon\n",
    "        glac_shp_lyr_mask = geolib.lyr2mask(glac_shp_lyr, ds_dict['ice_thick'])\n",
    "        dc_shp_lyr_mask = geolib.lyr2mask(dc_shp_lyr, ds_dict['ice_thick'])\n",
    "\n",
    "        #Create buffer around glacier polygon\n",
    "        glac_geom_buff = gf.glac_geom.Buffer(input.buff_dist)\n",
    "        #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "        glac_geom_buff_mask = geolib.geom2mask(glac_geom_buff, ds_dict['ice_thick'])\n",
    "\n",
    "        # ds masks\n",
    "        ds_list_masked = [iolib.ds_getma(i) for i in ds_list]\n",
    "        dem1 = np.ma.masked_less_equal(ds_list_masked[0], 0)\n",
    "        dems_mask = dem1.mask\n",
    "        if verbose:\n",
    "            print('list of datasets:', len(ds_list_masked), fn_dict.values())\n",
    "\n",
    "        #Combine to identify ~1 km buffer around glacier polygon over static rock\n",
    "        static_buffer_mask = np.ma.mask_or(~glac_shp_lyr_mask, glac_geom_buff_mask)\n",
    "        static_shp_lyr_mask = np.ma.mask_or(static_buffer_mask, dems_mask)\n",
    "        \n",
    "        if 'z1' in ds_dict:\n",
    "            #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "            glac_geom_mask = geolib.geom2mask(gf.glac_geom, ds_dict['z1'])\n",
    "            gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1']))\n",
    "            #gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1']), mask=glac_geom_mask)\n",
    "            \n",
    "            # Debris cover\n",
    "            dc_mask = np.ma.mask_or(dc_shp_lyr_mask, glac_geom_mask)\n",
    "            gf.dc_area = np.ma.array(iolib.ds_getma(ds_dict['z1']), mask=dc_mask)\n",
    "\n",
    "            # Check if DEM has huge errors or not - replace if necessary\n",
    "            if input.roi in ['01']:\n",
    "                \n",
    "                gf.z1_check = np.ma.array(iolib.ds_getma(ds_dict['z1']), mask=glac_geom_mask)\n",
    "                if gf.z1_check.min() < 0:\n",
    "                    \n",
    "                    # Add backup DEM for regions with known poor quality (ex. Alaska)\n",
    "                    print('switching DEMs')\n",
    "                    fn_dict['z1_backup'] = input.z1_backup_dict[input.roi]\n",
    "                    # Warp everything to common res/extent/proj (a second time)\n",
    "                    ds_list = warplib.memwarp_multi_fn(fn_dict.values(), res='min', \\\n",
    "                            extent=warp_extent, t_srs=aea_srs, verbose=verbose, \\\n",
    "                            r='cubic')\n",
    "                    ds_dict = dict(zip(fn_dict.keys(), ds_list))\n",
    "#                     glac_geom_mask = geolib.geom2mask(gf.glac_geom, ds_dict['z1'])\n",
    "#                     gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1_backup']))\n",
    "                    \n",
    "#                     # Debris cover\n",
    "#                     dc_mask = np.ma.mask_or(dc_shp_lyr_mask, glac_geom_mask)\n",
    "#                     gf.dc_area = np.ma.array(iolib.ds_getma(ds_dict['z1']), mask=dc_mask)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(ds_list)\n",
    "                        print(fn_dict.keys())\n",
    "\n",
    "                    #Prepare mask for all glaciers within buffered area, not just the current glacier polygon\n",
    "                    glac_shp_ds = ogr.Open(glac_shp_proj_fn, 0)\n",
    "                    glac_shp_lyr = glac_shp_ds.GetLayer()\n",
    "\n",
    "                    dc_shp_ds = ogr.Open(dc_shp_proj_fn, 0)\n",
    "                    dc_shp_lyr = dc_shp_ds.GetLayer()\n",
    "\n",
    "                    #Get global glacier mask\n",
    "                    #Want this to be True over ALL glacier surfaces, not just the current polygon\n",
    "                    glac_shp_lyr_mask = geolib.lyr2mask(glac_shp_lyr, ds_dict['ice_thick'])\n",
    "                    dc_shp_lyr_mask = geolib.lyr2mask(dc_shp_lyr, ds_dict['ice_thick'])\n",
    "\n",
    "                    #Create buffer around glacier polygon\n",
    "                    glac_geom_buff = gf.glac_geom.Buffer(input.buff_dist)\n",
    "                    #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "                    glac_geom_buff_mask = geolib.geom2mask(glac_geom_buff, ds_dict['ice_thick'])\n",
    "\n",
    "                    # ds masks\n",
    "                    ds_list_masked = [iolib.ds_getma(i) for i in ds_list]\n",
    "                    dem1 = np.ma.masked_less_equal(ds_list_masked[0], 0)\n",
    "                    dems_mask = dem1.mask\n",
    "                    if verbose:\n",
    "                        print('list of datasets:', len(ds_list_masked), fn_dict.values())\n",
    "\n",
    "                    #Combine to identify ~1 km buffer around glacier polygon over static rock\n",
    "                    static_buffer_mask = np.ma.mask_or(~glac_shp_lyr_mask, glac_geom_buff_mask)\n",
    "                    static_shp_lyr_mask = np.ma.mask_or(static_buffer_mask, dems_mask)\n",
    "                    \n",
    "                    #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "                    glac_geom_mask = geolib.geom2mask(gf.glac_geom, ds_dict['z1_backup'])\n",
    "                    gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1_backup']))\n",
    "                    #gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1']), mask=glac_geom_mask)\n",
    "\n",
    "                    # Debris cover\n",
    "                    dc_mask = np.ma.mask_or(dc_shp_lyr_mask, glac_geom_mask)\n",
    "                    gf.dc_area = np.ma.array(iolib.ds_getma(ds_dict['z1_backup']), mask=dc_mask)\n",
    "\n",
    "\n",
    "            if verbose:\n",
    "                print('\\n\\n# z1 pixels:', gf.z1.count(), '\\n')\n",
    "            if gf.z1.count() == 0:\n",
    "                if verbose:\n",
    "                    print(\"No z1 pixels\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Unable to load z1 ds\")\n",
    "            \n",
    "        \n",
    "        \n",
    "        if nglac == 0:\n",
    "            print('\\n\\nHACK TO BYPASS VALID AREA\\n\\n')\n",
    "        gf.valid_area_perc = 100\n",
    "\n",
    "        if gf.valid_area_perc < (100. * input.min_valid_area_perc):\n",
    "            if verbose:\n",
    "                print(\"Not enough valid pixels. %0.1f%% percent of glacier polygon area\" % (gf.valid_area_perc))\n",
    "        #     return None\n",
    "\n",
    "        else:\n",
    "            #Filter dz - throw out abs differences >150 m\n",
    "\n",
    "            #Compute dz, volume change, mass balance and stats\n",
    "            gf.z1_stats = malib.get_stats(gf.z1)\n",
    "            z1_elev_med = gf.z1_stats[5]\n",
    "            z1_elev_min, z1_elev_max = malib.calcperc(gf.z1, (0.1, 99.9))\n",
    "\n",
    "            #Caluclate stats for aspect and slope using z2\n",
    "            #Requires GDAL 2.1+\n",
    "            gf.z1_aspect = np.ma.array(geolib.gdaldem_mem_ds(ds_dict['z1'], processing='aspect', returnma=True), mask=glac_geom_mask)\n",
    "            gf.z1_aspect_stats = malib.get_stats(gf.z1_aspect)\n",
    "            z1_aspect_med = gf.z1_aspect_stats[5]\n",
    "            gf.z1_slope = np.ma.array(geolib.gdaldem_mem_ds(ds_dict['z1'], processing='slope', returnma=True), mask=glac_geom_mask)\n",
    "            gf.z1_slope_stats = malib.get_stats(gf.z1_slope)\n",
    "            z1_slope_med = gf.z1_slope_stats[5]\n",
    "\n",
    "            #Can estimate ELA values computed from hypsometry and typical AAR\n",
    "            #For now, assume ELA is mean\n",
    "            gf.z1_ela = None\n",
    "            gf.z1_ela = gf.z1_stats[3]\n",
    "            #Note: in theory, the ELA should get higher with mass loss\n",
    "            #In practice, using mean and same polygon, ELA gets lower as glacier surface thins\n",
    "\n",
    "            if extra_layers and (gf.glac_area_km2 > input.min_glac_area_writeout):\n",
    "                if 'ice_thick' in ds_dict:\n",
    "                    #Load ice thickness\n",
    "                    gf.H = np.ma.array(iolib.ds_getma(ds_dict['ice_thick']), mask=glac_geom_mask)\n",
    "                    gf.H_mean = gf.H.mean()\n",
    "                    if verbose:\n",
    "                        print('mean ice thickness [m]:', gf.H_mean)\n",
    "\n",
    "                if 'vx' in ds_dict and 'vy' in ds_dict:\n",
    "                    #Load surface velocity maps\n",
    "                    gf.vx = np.ma.array(iolib.ds_getma(ds_dict['vx']), mask=glac_geom_mask)\n",
    "                    gf.vy = np.ma.array(iolib.ds_getma(ds_dict['vy']), mask=glac_geom_mask)\n",
    "                    gf.vm = np.ma.sqrt(gf.vx**2 + gf.vy**2)\n",
    "                    gf.vm_mean = gf.vm.mean()\n",
    "                    if verbose:\n",
    "                        print('mean velocity [m/s]:', gf.vm_mean)\n",
    "\n",
    "                    if gf.H is not None:\n",
    "                        #Compute flux\n",
    "                        gf.Q = gf.H * input.v_col_f * np.array([gf.vx, gf.vy])\n",
    "                        #Note: np.gradient returns derivatives relative to axis number, so (y, x) in this case\n",
    "                        #Want x-derivative of x component\n",
    "                        gf.divQ = np.gradient(gf.Q[0])[1] + np.gradient(gf.Q[1])[0]\n",
    "\n",
    "        #                 gf.divQ = gf.H*(np.gradient(v_col_f*gf.vx)[1] + np.gradient(v_col_f*gf.vy)[0]) \\\n",
    "        #                         + v_col_f*gf.vx*(np.gradient(gf.H)[1]) + v_col_f*gf.vy*(np.gradient(gf.H)[0])\n",
    "\n",
    "                        #Should smooth divQ, better handling of data gaps\n",
    "                \n",
    "\n",
    "                if 'ts' in ds_dict:\n",
    "                    #Load surface temperature maps\n",
    "                    gf.ts = np.ma.array(iolib.ds_getma(ds_dict['ts']), mask=glac_geom_mask)\n",
    "                else:\n",
    "                    gf.ts = None\n",
    "\n",
    "                if 'debris_thick_ts' in ds_dict:\n",
    "                    # Load debris thickness map\n",
    "                    gf.debris_thick_ts = np.ma.array(iolib.ds_getma(ds_dict['debris_thick_ts']), mask=glac_geom_mask)\n",
    "                    gf.meltfactor_ts = None\n",
    "                else:\n",
    "                    gf.debris_thick_ts = None\n",
    "                    gf.meltfactor_ts = None\n",
    "\n",
    "            if verbose:\n",
    "                print('Area [km2]:', gf.glac_area / 1e6)\n",
    "                print('-------------------------------')\n",
    "\n",
    "\n",
    "            # Plots\n",
    "    #         titles = ['Z1']\n",
    "    #         z1_full2plot = gf.z1\n",
    "    #         z1_full2plot.mask = dems_mask\n",
    "    #         clim = malib.calcperc(z1_full2plot, (2,98))\n",
    "    #         plot_array(z1_full2plot, clim, titles, 'inferno', 'Elevation (m WGS84)', fn=outdir_fig + glac_str + '_dem.png')\n",
    "\n",
    "            #Now apply glacier mask AND mask NaN values\n",
    "            glac_geom_mask = np.ma.mask_or(glac_geom_mask, dems_mask)\n",
    "            # nan_mask = np.ma.masked_invalid(gf.dz)\n",
    "            # glac_geom_mask = np.ma.mask_or(glac_geom_mask, nan_mask.mask)\n",
    "            gf.z1 = np.ma.array(gf.z1, mask=glac_geom_mask)\n",
    "            \n",
    "#             # Debris cover mask\n",
    "#             dc_mask = np.ma.mask_or(dc_shp_lyr_mask, glac_geom_mask)\n",
    "\n",
    "            gf.res = geolib.get_res(ds_dict['z1'])\n",
    "\n",
    "            titles = ['Z1 (masked)']\n",
    "            clim = malib.calcperc(gf.z1, (2,98))\n",
    "            plot_array(gf.z1, clim, titles, 'inferno', 'Elevation (m WGS84)', fn=outdir_fig + glac_str + '_dem.png')\n",
    "\n",
    "            if verbose:\n",
    "                print(gf.z1.shape)\n",
    "                \n",
    "#             titles = ['Vx']\n",
    "#             var_full2plot = gf.vx\n",
    "#             var_full2plot.mask = glac_geom_mask\n",
    "#             clim = malib.calcperc(var_full2plot, (2,98))\n",
    "#             plot_array(var_full2plot, clim, titles, 'inferno', 'vx', fn=outdir_fig + gf.feat_fn +'_vx.png')\n",
    "\n",
    "#             titles = ['Vy']\n",
    "#             var_full2plot = gf.vy\n",
    "#             var_full2plot.mask = glac_geom_mask\n",
    "#             clim = malib.calcperc(var_full2plot, (2,98))\n",
    "#             plot_array(var_full2plot, clim, titles, 'inferno', 'vy', fn=outdir_fig + gf.feat_fn +'_vy.png')\n",
    "\n",
    "            gf.vtot = (gf.vx**2 + gf.vy**2)**0.5\n",
    "\n",
    "            titles = ['Velocity (m/yr)']\n",
    "            var_full2plot = gf.vtot\n",
    "            var_full2plot.mask = glac_geom_mask\n",
    "            clim = malib.calcperc(var_full2plot, (2,98))\n",
    "            plot_array(var_full2plot, clim, titles, 'inferno', 'Velocity (m/yr)', fn=outdir_fig + glac_str +'_velocity.png',\n",
    "                       close_fig=close_fig)\n",
    "\n",
    "            titles = ['Ice thickness']\n",
    "            var_full2plot = gf.H\n",
    "            var_full2plot.mask = glac_geom_mask\n",
    "#             var_full2plot.mask = dc_mask\n",
    "            clim = malib.calcperc(var_full2plot, (2,98))\n",
    "            plot_array(var_full2plot, clim, titles, 'inferno', 'H', fn=outdir_fig + gf.feat_fn +'_ice_thickness.png',\n",
    "                      close_fig=close_fig)\n",
    "        \n",
    "            \n",
    "            titles = ['Debris cover']\n",
    "            var_full2plot = gf.dc_area\n",
    "            clim = (0,1)\n",
    "            plot_array(var_full2plot, clim, titles, 'inferno', '', fn=outdir_fig + gf.feat_fn +'_debriscover.png',\n",
    "                      close_fig=close_fig)\n",
    "            \n",
    "            titles = ['Flux']\n",
    "            divQ_full2plot = gf.divQ\n",
    "            divQ_full2plot.mask = glac_geom_mask\n",
    "            clim = malib.calcperc(divQ_full2plot, (2,98))\n",
    "            plot_array(divQ_full2plot, clim, titles, 'inferno', 'divQ', fn=outdir_fig + glac_str +'_divQ.png')\n",
    "\n",
    "            # ===== \"COREGISTER\" SURFACE LOWERING WITH DEM USED FOR ICE THICKNESS =====\n",
    "            # Load Mass Balance Data and find displacement =====\n",
    "            if verbose:\n",
    "                print('\\nREALLY THIS SHOULD BE DONE BY COREGISTRATION OF THE TWO DEMS\\n')\n",
    "            mb_df = pd.read_csv(main_glac_rgi.loc[glac_idx, 'mb_fn'])\n",
    "            mb_df.loc[:,:] = mb_df.values.astype(np.float64)\n",
    "            try:\n",
    "                mb_bin0_km2 = mb_df.loc[0,' z1_bin_area_perc'] / 100 * main_glac_rgi.loc[glac_idx,'Area']\n",
    "            except:\n",
    "                mb_bin0_km2 = mb_df.loc[0,' z1_bin_area_valid_km2']\n",
    "            mb_bin_size = mb_df.loc[1,'# bin_center_elev_m'] - mb_df.loc[0,'# bin_center_elev_m']\n",
    "            pix_km2 = gf.res[0] * gf.res[1] / (1000)**2\n",
    "            if verbose:\n",
    "                print('total glacier area [km2]:', main_glac_rgi.loc[glac_idx,'Area'])\n",
    "                print('initial bin area [km2]:', mb_bin0_km2)\n",
    "                print('bin size [m]:', mb_bin_size)\n",
    "                print('pixel size [km2]:', pix_km2)\n",
    "            # Find displacement\n",
    "            if len(gf.z1.compressed()) > 0:\n",
    "                z1 = gf.z1.compressed()\n",
    "                z1_min = z1[z1>0].min()\n",
    "                z1_max = z1[z1>0].max()\n",
    "                z1_km2 = 0\n",
    "                elev = int(z1_min)\n",
    "                while z1_km2 < mb_bin0_km2 and elev < z1_max:\n",
    "                    elev += 1\n",
    "                    z1_idx = np.where((z1 > 0) & (z1 < elev))\n",
    "                    if len(z1_idx[0]) > 0:\n",
    "                        z1_km2 = len(z1_idx[0]) * pix_km2\n",
    "#                         print(elev, z1_km2)        \n",
    "                if verbose:\n",
    "                    print(elev, z1_km2, 'vs', mb_df.loc[0,'# bin_center_elev_m'], mb_bin0_km2)\n",
    "                mb_bin0_upper =  mb_df.loc[0,'# bin_center_elev_m'] + mb_bin_size / 2\n",
    "                z1_offset = elev - mb_bin0_upper\n",
    "                if verbose:\n",
    "                    print('z1_offset:', z1_offset)\n",
    "                # Update z1 with the offset\n",
    "                mask_offset = np.ma.array(np.zeros(gf.z1.mask.shape) - z1_offset, mask=np.ma.getmask(gf.z1))\n",
    "                gf.z1[~gf.z1.mask] = gf.z1[~gf.z1.mask] + mask_offset[~mask_offset.mask]\n",
    "                \n",
    "                mask_offset_dc = np.ma.array(np.zeros(gf.dc_area.mask.shape) - z1_offset, mask=np.ma.getmask(gf.dc_area))\n",
    "                gf.dc_area[~gf.dc_area.mask] = gf.dc_area[~gf.dc_area.mask] + mask_offset[~mask_offset_dc.mask]\n",
    "\n",
    "                # ===== EMERGENCE VELOCITY =====\n",
    "                vx = np.ma.filled(gf.vx,0)\n",
    "                vy = np.ma.filled(gf.vy,0)\n",
    "                H = np.ma.filled(gf.H,0)\n",
    "                vx[gf.z1 > gf.z1.max()] = 0\n",
    "                vy[gf.z1 > gf.z1.max()] = 0\n",
    "                H[gf.z1 > gf.z1.max()] = 0\n",
    "                vmax = np.nanmax((vx**2 + vy**2)**0.5)\n",
    "\n",
    "                # Emergence computation\n",
    "                emvel = emergence_pixels(gf, vx, vy, H, gf.res[0], gf.res[1], \n",
    "                                         positive_is_east=True, positive_is_north=True, \n",
    "                                         constant_icethickness=False, max_velocity=vmax, vel_min=0, debug=False)\n",
    "                # 3x3 filter to reduce\n",
    "                if input.emvel_filter_pixsize > 0:\n",
    "                    emvel = ndimage.filters.convolve(emvel, weights=np.full((input.emvel_filter_pixsize, input.emvel_filter_pixsize), \n",
    "                                                                            1.0/input.emvel_filter_pixsize**2))\n",
    "                # Add to glacier feature\n",
    "                gf.emvel = np.ma.masked_array(emvel, mask=np.ma.getmask(gf.z1))\n",
    "\n",
    "                # ===== EXPORT BINNED STATISTICS =====\n",
    "                #Do AED for all\n",
    "                #Compute mb using scaled AED vs. polygon\n",
    "                #Check for valid pixel count vs. feature area, fill if appropriate\n",
    "                if gf.glac_area_km2 > input.min_glac_area_writeout:\n",
    "                    outbins_df, z_bin_edges = hist_plot(gf, bin_width=mb_bin_size, csv_ending=csv_ending,\n",
    "                                                        mb_df=mb_df)\n",
    "\n",
    "#                     if verbose:\n",
    "#                         print(outbins_df.loc[0:10,['bin_center_elev_m', ' vm_med',' vm_mad', ' H_mean', ' H_std', \n",
    "#                                                    ' emvel_mean', ' emvel_std',' emvel_med', ' emvel_mad']])\n",
    "            else:\n",
    "                print('\\n' + glac_str + ' HAS NO GLACIER AREA!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n\\nDONE\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DEBRIS ELEVATION STATS ====================================================================================\n",
    "# Glaciers with data\n",
    "glac_wobs_fns = []\n",
    "rgiid_wobs = []\n",
    "for i in os.listdir(outdir_csv):\n",
    "    if i.endswith('_mb_bins_wdc_emvel_offset.csv'):\n",
    "        rgiid_reg = int(i.split('.')[0])\n",
    "        if int(rgiid_reg) in input.roi_rgidict[input.roi]:\n",
    "            glac_wobs_fns.append(i)\n",
    "            if rgiid_reg < 10:\n",
    "                rgiid_wobs.append(i[0:7])\n",
    "            else:\n",
    "                rgiid_wobs.append(i[0:8])\n",
    "        \n",
    "glac_wobs_fns = sorted(glac_wobs_fns)\n",
    "rgiid_wobs = sorted(rgiid_wobs)\n",
    "\n",
    "# ===== SELECT GLACIERS WITH DATA =====\n",
    "main_glac_rgi_wobs = input.selectglaciersrgitable(rgiid_wobs)\n",
    "main_glac_rgi_wobs['mb_bin_fn'] = glac_wobs_fns \n",
    "main_glac_rgi_wobs['CenLon_360'] = 360 + main_glac_rgi_wobs['CenLon']\n",
    "main_glac_rgi_wobs['CenLon_360'] = main_glac_rgi_wobs['CenLon']\n",
    "main_glac_rgi_wobs.loc[main_glac_rgi_wobs['CenLon_360'] < 0, 'CenLon_360'] = (\n",
    "    360 + main_glac_rgi_wobs.loc[main_glac_rgi_wobs['CenLon_360'] < 0, 'CenLon_360'])\n",
    "ds = xr.open_dataset(input.metdata_fp + '../' + input.metdata_elev_fn)\n",
    "#  argmin() finds the minimum distance between the glacier lat/lon and the GCM pixel\n",
    "lat_nearidx = (np.abs(main_glac_rgi_wobs['CenLat'].values[:,np.newaxis] - \n",
    "                      ds['latitude'][:].values).argmin(axis=1))\n",
    "lon_nearidx = (np.abs(main_glac_rgi_wobs['CenLon_360'].values[:,np.newaxis] - \n",
    "                      ds['longitude'][:].values).argmin(axis=1))\n",
    "latlon_nearidx = list(zip(lat_nearidx, lon_nearidx))\n",
    "latlon_nearidx_unique = sorted(list(set(latlon_nearidx)))\n",
    "main_glac_rgi_wobs['latlon_nearidx'] = latlon_nearidx\n",
    "latlon_unique_dict = dict(zip(latlon_nearidx_unique,np.arange(0,len(latlon_nearidx_unique))))\n",
    "latlon_unique_dict_reversed = dict(zip(np.arange(0,len(latlon_nearidx_unique)),latlon_nearidx_unique))\n",
    "main_glac_rgi_wobs['latlon_unique_no'] = main_glac_rgi_wobs['latlon_nearidx'].map(latlon_unique_dict)\n",
    "\n",
    "print('unique lat/lons:', len(np.unique(main_glac_rgi_wobs['latlon_unique_no'])), '\\n\\n')\n",
    "# print(dc_shp_subset_wdata.loc[0:5,['RGIId', 'CenLat', 'CenLon', 'larsen_fn', 'braun_fn', 'latlon_unique_no']])\n",
    "\n",
    "lat_list = np.array([ds.latitude[x[0]].values for x in latlon_nearidx_unique])\n",
    "lon_list = np.array([ds.longitude[x[1]].values for x in latlon_nearidx_unique])\n",
    "latlon_list = list(tuple(zip(list(lat_list), list(lon_list))))\n",
    "\n",
    "# ===== CALCULATE DEBRIS ELEVATION STATS FOR GLACIERS WITH DATA FOR EACH UNIQUE LAT/LON ======\n",
    "elev_stats_latlon_dict = {}\n",
    "latlon_list_updated = []\n",
    "for nlatlon, latlon_unique in enumerate(np.unique(main_glac_rgi_wobs.latlon_unique_no)):\n",
    "# for nlatlon, latlon_unique in enumerate([np.unique(main_glac_rgi_wobs.latlon_unique_no)[6]]):\n",
    "\n",
    "    main_glac_rgi_subset = main_glac_rgi_wobs[main_glac_rgi_wobs['latlon_unique_no'] == latlon_unique]\n",
    "    \n",
    "    # Debris elevation stats should be done by lat/lon\n",
    "    df_all = None\n",
    "    elev_list_all = []\n",
    "    df_idx_count = 0\n",
    "    for nglac, glac_fn in enumerate(main_glac_rgi_subset.mb_bin_fn.values):\n",
    "        df_raw = pd.read_csv(outdir_csv + glac_fn)\n",
    "        df = df_raw.dropna(subset=[' mb_bin_mean_mwea'])\n",
    "        df_debris = df[(df[' vm_med'] < input.vel_threshold) & (df['debris_perc'] > input.debrisperc_threshold)\n",
    "                       & (df[' dc_bin_count_valid'] > 0)]\n",
    "\n",
    "        df_idx = df_debris.index.values\n",
    "        df_idx_count += len(df_idx)\n",
    "        \n",
    "        if len(df_idx) > 0:\n",
    "            # only work with terminus\n",
    "            df_idx_dif = list(df_idx[1:] - df_idx[:-1])\n",
    "            if np.sum(df_idx_dif) == len(df_idx)-1:\n",
    "                df_idx_nojump = df_idx\n",
    "            else:\n",
    "                idx_jumpinbins = df_idx_dif.index(next(filter(lambda x: x>1, df_idx_dif)))\n",
    "                df_idx_nojump = df_idx[0:idx_jumpinbins+1]\n",
    "\n",
    "            df_debris_nojump = df_debris.loc[df_idx_nojump,:]\n",
    "            df_debris_nojump.reset_index(inplace=True, drop=True)\n",
    "            \n",
    "            for nelev, elev in enumerate(list(df_debris_nojump['bin_center_elev_m'].values)):\n",
    "                elev_list_single = list(np.repeat(elev, df_debris_nojump.loc[nelev,' dc_bin_count_valid']))\n",
    "                elev_list_all.extend(elev_list_single)\n",
    "        \n",
    "    if df_idx_count > 0:\n",
    "        dc_zmean = np.mean(elev_list_all)\n",
    "        dc_zstd = np.std(elev_list_all)\n",
    "        dc_zmed = malib.fast_median(elev_list_all)\n",
    "        dc_zmad = malib.mad(elev_list_all)\n",
    "\n",
    "        lat_deg = float(ds.latitude[latlon_unique_dict_reversed[latlon_unique][0]].values)\n",
    "        lon_deg = float(ds.longitude[latlon_unique_dict_reversed[latlon_unique][1]].values)\n",
    "        elev_stats_latlon_dict[lat_deg,lon_deg] = [dc_zmean, dc_zstd, dc_zmed, dc_zmad]\n",
    "        latlon_list_updated.append((lat_deg, lon_deg))\n",
    "\n",
    "# Update pickle of unique lat/lons that will be used for melt model\n",
    "with open(input.latlon_unique_fp + input.latlon_unique_dict[input.roi], 'wb') as f:\n",
    "    pickle.dump(latlon_list_updated, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CHECK THE HMA DATA AND SEE IF IT NEEDS TO ADD THESE STATS - I BELIEVE IT DOES')\n",
    "print('  --> grab a copy of files first just in case')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.75 213.0 exists\n",
      "63.5 212.5 exists\n",
      "63.5 213.25 exists\n",
      "63.5 213.5 exists\n",
      "63.5 214.5 exists\n",
      "63.25 209.5 exists\n",
      "63.25 214.5 exists\n",
      "63.0 209.25 exists\n",
      "63.0 210.0 exists\n",
      "62.75 208.0 exists\n",
      "62.75 208.25 exists\n",
      "62.75 208.5 exists\n",
      "62.75 208.75 exists\n",
      "62.75 209.25 exists\n",
      "62.5 207.75 exists\n",
      "61.75 212.5 exists\n",
      "61.5 212.0 exists\n",
      "61.5 213.0 exists\n",
      "61.5 217.0 exists\n",
      "61.5 219.0 exists\n",
      "61.25 213.0 exists\n",
      "61.25 218.5 exists\n",
      "61.25 219.75 exists\n",
      "61.0 207.25 exists\n",
      "61.0 214.25 exists\n",
      "61.0 214.75 exists\n",
      "61.0 219.5 exists\n",
      "61.0 220.25 exists\n",
      "60.75 206.5 exists\n",
      "60.75 206.75 exists\n",
      "60.75 214.75 exists\n",
      "60.75 219.25 exists\n",
      "60.75 221.0 exists\n",
      "60.5 215.0 exists\n",
      "60.5 218.0 exists\n",
      "60.25 219.5 exists\n",
      "60.0 207.0 exists\n",
      "60.0 220.25 exists\n",
      "60.0 220.75 exists\n",
      "60.0 221.0 exists\n",
      "60.0 221.75 exists\n",
      "60.0 222.0 exists\n",
      "59.75 220.5 exists\n",
      "59.75 220.75 exists\n",
      "59.75 221.0 exists\n",
      "59.75 221.25 exists\n",
      "59.75 221.75 exists\n",
      "59.75 222.25 exists\n",
      "59.75 222.5 exists\n",
      "59.75 222.75 exists\n",
      "59.75 223.25 exists\n",
      "59.75 223.75 exists\n",
      "59.75 224.25 exists\n",
      "59.75 224.5 exists\n",
      "59.75 224.75 exists\n",
      "59.5 221.0 exists\n",
      "59.5 221.25 exists\n",
      "59.5 221.5 exists\n",
      "59.5 221.75 exists\n",
      "59.5 222.0 exists\n",
      "59.5 222.25 exists\n",
      "59.5 222.5 exists\n",
      "59.5 222.75 exists\n",
      "59.5 223.0 exists\n",
      "59.5 223.25 exists\n",
      "59.5 223.5 exists\n",
      "59.5 224.0 exists\n",
      "59.5 224.25 exists\n",
      "59.5 224.5 exists\n",
      "59.5 224.75 exists\n",
      "59.5 225.0 exists\n",
      "59.5 225.25 exists\n",
      "59.5 225.5 exists\n",
      "59.25 221.5 exists\n",
      "59.25 221.75 exists\n",
      "59.25 222.0 exists\n",
      "59.25 222.25 exists\n",
      "59.25 222.75 exists\n",
      "59.25 223.0 exists\n",
      "59.25 223.25 exists\n",
      "59.25 223.5 exists\n",
      "59.25 223.75 exists\n",
      "59.25 224.0 exists\n",
      "59.25 224.25 exists\n",
      "59.25 224.5 exists\n",
      "59.25 224.75 exists\n",
      "59.25 225.0 exists\n",
      "59.25 225.25 exists\n",
      "59.25 225.5 exists\n",
      "59.25 225.75 exists\n",
      "59.0 206.5 exists\n",
      "59.0 222.25 exists\n",
      "59.0 222.75 exists\n",
      "59.0 223.0 exists\n",
      "59.0 223.25 exists\n",
      "59.0 223.5 exists\n",
      "59.0 223.75 exists\n",
      "59.0 224.0 exists\n",
      "59.0 224.25 exists\n",
      "59.0 224.5 exists\n",
      "59.0 225.0 exists\n",
      "59.0 225.25 exists\n",
      "59.0 225.75 exists\n",
      "59.0 226.0 exists\n",
      "59.0 226.25 exists\n",
      "59.0 226.5 exists\n",
      "58.75 206.25 exists\n",
      "58.75 222.25 exists\n",
      "58.75 222.5 exists\n",
      "58.75 223.0 exists\n",
      "58.75 223.25 exists\n",
      "58.75 223.5 exists\n",
      "58.75 223.75 exists\n",
      "58.75 224.0 exists\n",
      "58.75 224.25 exists\n",
      "58.75 224.5 exists\n",
      "58.75 225.25 exists\n",
      "58.75 225.5 exists\n",
      "58.75 225.75 exists\n",
      "58.75 226.0 exists\n",
      "58.75 226.25 exists\n",
      "58.75 226.5 exists\n",
      "58.5 205.5 exists\n",
      "58.5 222.75 exists\n",
      "58.5 223.0 exists\n",
      "58.5 223.5 exists\n",
      "58.5 224.5 exists\n",
      "58.5 224.75 exists\n",
      "58.5 226.25 exists\n",
      "58.5 226.5 exists\n",
      "58.5 226.75 exists\n",
      "58.5 227.0 exists\n",
      "58.5 227.25 exists\n",
      "58.5 227.5 exists\n",
      "58.25 204.5 exists\n",
      "58.25 204.75 exists\n",
      "58.25 205.0 exists\n",
      "58.25 205.25 exists\n",
      "58.25 205.5 exists\n",
      "58.25 226.0 exists\n",
      "58.25 226.25 exists\n",
      "58.25 226.75 exists\n",
      "58.25 227.0 exists\n",
      "58.25 227.25 exists\n",
      "58.25 227.5 exists\n",
      "58.0 227.0 exists\n",
      "58.0 227.25 exists\n",
      "58.0 227.5 exists\n",
      "58.0 227.75 exists\n",
      "58.0 228.0 exists\n",
      "57.75 226.5 exists\n",
      "57.75 227.0 exists\n",
      "57.75 227.5 exists\n",
      "57.75 227.75 exists\n",
      "57.75 228.0 exists\n",
      "57.75 229.25 exists\n",
      "57.75 229.5 exists\n",
      "57.5 227.0 exists\n",
      "57.5 227.25 exists\n",
      "57.5 227.5 exists\n",
      "57.5 227.75 exists\n",
      "57.5 228.0 exists\n",
      "57.5 228.5 exists\n",
      "57.5 228.75 exists\n",
      "57.5 229.25 exists\n",
      "57.25 227.25 exists\n",
      "57.25 227.5 exists\n",
      "57.25 227.75 exists\n",
      "57.25 228.0 exists\n",
      "57.25 228.25 exists\n",
      "57.25 228.5 exists\n",
      "57.25 228.75 exists\n",
      "57.25 229.0 exists\n",
      "57.25 229.25 exists\n",
      "57.25 229.5 exists\n",
      "57.25 230.0 exists\n",
      "57.25 230.75 exists\n",
      "57.0 225.0 exists\n",
      "57.0 227.5 exists\n",
      "57.0 228.0 exists\n",
      "57.0 228.5 exists\n",
      "57.0 228.75 exists\n",
      "57.0 229.0 exists\n",
      "57.0 229.25 exists\n",
      "57.0 229.5 exists\n",
      "57.0 230.0 exists\n",
      "57.0 230.25 exists\n",
      "57.0 230.5 exists\n",
      "57.0 230.75 exists\n",
      "56.75 227.75 exists\n",
      "56.75 228.5 exists\n",
      "56.75 228.75 exists\n",
      "56.75 229.0 exists\n",
      "56.75 229.5 exists\n",
      "56.75 230.5 exists\n",
      "56.75 230.75 exists\n",
      "56.5 228.25 exists\n",
      "56.5 228.5 exists\n",
      "56.5 228.75 exists\n",
      "56.5 229.0 exists\n",
      "56.5 229.25 exists\n",
      "56.5 229.5 exists\n",
      "56.5 229.75 exists\n",
      "56.5 230.0 exists\n",
      "56.25 228.75 exists\n",
      "56.25 229.5 exists\n",
      "56.25 229.75 exists\n",
      "56.25 230.0 exists\n",
      "56.25 230.25 exists\n",
      "56.25 230.5 exists\n",
      "56.0 229.5 exists\n",
      "56.0 229.75 exists\n",
      "56.0 230.25 exists\n",
      "56.0 230.5 exists\n",
      "55.75 230.25 exists\n",
      "55.75 230.75 exists\n",
      "55.25 230.5 exists\n",
      "55.25 230.75 exists\n"
     ]
    }
   ],
   "source": [
    "# ===== ADD DEBRIS ELEVATION STATS TO MET DATA ======\n",
    "for nlatlon, latlon in enumerate(latlon_list_updated):\n",
    "# for nlatlon, latlon in enumerate([latlon_list_updated[0]]):\n",
    "    \n",
    "    lat_deg = latlon[0]\n",
    "    lon_deg = latlon[1]\n",
    "\n",
    "    # ===== Meteorological data =====\n",
    "    metdata_fn = input.metdata_fn_sample.replace('XXXX', \n",
    "                                                 str(int(lat_deg*100)) + 'N-' + str(int(lon_deg*100)) + 'E-')\n",
    "    \n",
    "    ds = xr.open_dataset(input.metdata_fp + metdata_fn)\n",
    "    if 'dc_zmean' not in list(ds.keys()):\n",
    "        # Add stats\n",
    "        ds['dc_zmean'] = elev_stats_latlon_dict[latlon][0]\n",
    "        ds['dc_zmean'].attrs = {'units':'m a.s.l.', 'long_name':'Mean debris cover elevation', 'comment':'converted from debris cover with data that will be used for subdebris melt inversion'}\n",
    "        ds['dc_zstd'] = elev_stats_latlon_dict[latlon][1]\n",
    "        ds['dc_zstd'].attrs = {'units':'m a.s.l.', 'long_name':'Standard deviation of debris cover elevation', 'comment':'converted from debris cover with data that will be used for subdebris melt inversion'}\n",
    "        ds['dc_zmed'] = elev_stats_latlon_dict[latlon][2]\n",
    "        ds['dc_zmed'].attrs = {'units':'m a.s.l.', 'long_name':'Median debris cover elevation', 'comment':'converted from debris cover with data that will be used for subdebris melt inversion'}\n",
    "        ds['dc_zmad'] = elev_stats_latlon_dict[latlon][3]\n",
    "        ds['dc_zmad'].attrs = {'units':'m a.s.l.', 'long_name':'Median absolute deviation of debris cover elevation', 'comment':'converted from debris cover with data that will be used for subdebris melt inversion'}\n",
    "\n",
    "        try:\n",
    "            ds.close()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Export updated dataset\n",
    "        ds.to_netcdf(input.metdata_fp + metdata_fn, mode='a')\n",
    "    else:\n",
    "        print(lat_deg, lon_deg, 'exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TO-DO LIST:\n",
      "- Isolate the debris-covered areas for dh/dt (add as separate column to have the option)\n",
      "  --> requires the raw dh/dt grids for the region (ex. dont have for Larsen)\n"
     ]
    }
   ],
   "source": [
    "print('TO-DO LIST:')\n",
    "print('- Isolate the debris-covered areas for dh/dt (add as separate column to have the option)')\n",
    "print('  --> requires the raw dh/dt grids for the region (ex. dont have for Larsen)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rgiid_list = []\n",
    "# rgiid_fn_list = []\n",
    "# for i in os.listdir(outdir):\n",
    "#     if i.endswith('mb_bins.csv'):\n",
    "#         rgiid_list.append(i[0:8])\n",
    "#         rgiid_fn_list.append(i)\n",
    "        \n",
    "# rgiid_list = sorted(rgiid_list)\n",
    "# rgiid_fn_list = sorted(rgiid_fn_list)\n",
    "\n",
    "# print(len(rgiid_list))\n",
    "\n",
    "# main_glac_rgi = selectglaciersrgitable(rgiid_list)\n",
    "# main_glac_rgi['bin_fn'] = rgiid_fn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(main_glac_rgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group datasets by nearest lat/lon\n",
    "# ds = xr.open_dataset(met_sample_fullfn)\n",
    "# #  argmin() finds the minimum distance between the glacier lat/lon and the GCM pixel\n",
    "# lat_nearidx = (np.abs(main_glac_rgi['CenLat'].values[:,np.newaxis] - \n",
    "#                       ds['latitude'][:].values).argmin(axis=1))\n",
    "# lon_nearidx = (np.abs(main_glac_rgi['CenLon'].values[:,np.newaxis] - \n",
    "#                       ds['longitude'][:].values).argmin(axis=1))\n",
    "\n",
    "# latlon_nearidx = list(zip(lat_nearidx, lon_nearidx))\n",
    "# latlon_nearidx_unique = sorted(list(set(latlon_nearidx)))\n",
    "\n",
    "# main_glac_rgi['latlon_nearidx'] = latlon_nearidx\n",
    "# latlon_unique_dict = dict(zip(latlon_nearidx_unique,np.arange(0,len(latlon_nearidx_unique))))\n",
    "# latlon_unique_dict_reversed = dict(zip(np.arange(0,len(latlon_nearidx_unique)),latlon_nearidx_unique))\n",
    "# main_glac_rgi['latlon_unique_no'] = main_glac_rgi['latlon_nearidx'].map(latlon_unique_dict)\n",
    "# print(main_glac_rgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process each group and derive elevation statistics for the debris cover\n",
    "# elevstats_mean = np.zeros((len(ds.latitude.values), len(ds.longitude.values)))\n",
    "# elevstats_std = np.zeros((len(ds.latitude.values), len(ds.longitude.values)))\n",
    "# elevstats_med = np.zeros((len(ds.latitude.values), len(ds.longitude.values)))\n",
    "# elevstats_mad = np.zeros((len(ds.latitude.values), len(ds.longitude.values)))\n",
    "# elevstats_min = np.zeros((len(ds.latitude.values), len(ds.longitude.values)))\n",
    "# elevstats_max = np.zeros((len(ds.latitude.values), len(ds.longitude.values)))\n",
    "\n",
    "# for nlatlon, latlon_idx in enumerate(list(np.arange(0,len(latlon_nearidx_unique)))):\n",
    "# # for nlatlon, latlon_idx in enumerate([251]):\n",
    "#     main_glac_rgi_subset = main_glac_rgi[main_glac_rgi['latlon_unique_no'] == latlon_idx]\n",
    "#     bin_fns = main_glac_rgi_subset['bin_fn'].values\n",
    "    \n",
    "#     lat_idx, lon_idx = latlon_unique_dict_reversed[latlon_idx]\n",
    "#     print(nlatlon, lat_idx, lon_idx)\n",
    "\n",
    "#     df_all = None\n",
    "#     for n, i in enumerate(bin_fns):\n",
    "#         df = pd.read_csv(outdir + i)\n",
    "        \n",
    "# #         print(i)\n",
    "        \n",
    "#         # Process only glaciers with debris\n",
    "#         debris_switch=False\n",
    "#         if ' perc_debris' in df.columns:\n",
    "#             # Process dataframe\n",
    "#             output_cns = ['# bin_center_elev_m', ' z1_bin_area_valid_km2', ' perc_debris']\n",
    "#             df = df[output_cns]\n",
    "#             df['# bin_center_elev_m'] = df['# bin_center_elev_m'].astype(np.float) \n",
    "#             df[' z1_bin_area_valid_km2'] = df[' z1_bin_area_valid_km2'].astype(np.float)\n",
    "            \n",
    "#             # Remove nan values\n",
    "#             df[' perc_debris'] = df[' perc_debris'].astype(np.float)\n",
    "#             df.fillna(0, inplace=True)\n",
    "            \n",
    "# #             print(i, df[' perc_debris'].max())\n",
    "            \n",
    "#             df['area_debris_km2'] = df[' z1_bin_area_valid_km2'] * df[' perc_debris'] / 100\n",
    "            \n",
    "#             if df[' perc_debris'].max() > 10:\n",
    "#                 debris_switch = True\n",
    "#                 debris_idx = np.where(df[' perc_debris'] > 50)[0]\n",
    "                \n",
    "#         if debris_switch:\n",
    "# #             print('processing', i)\n",
    "\n",
    "#             elev_min = df['# bin_center_elev_m'].values[0]\n",
    "#             elev_max = df['# bin_center_elev_m'].values[-1]\n",
    "\n",
    "#             binsize = df['# bin_center_elev_m'].values[1] - df['# bin_center_elev_m'].values[0]\n",
    "\n",
    "#             # Merge datasets together to compute elevation stats for each lat/lon\n",
    "#             if df_all is None:\n",
    "#                 df_all = df\n",
    "#                 if len(debris_idx) > 1:\n",
    "#                     zmin_debris = df['# bin_center_elev_m'].values[debris_idx[0]]\n",
    "#                     zmax_debris = df['# bin_center_elev_m'].values[debris_idx[-1]]\n",
    "#             else:\n",
    "#                 # If new min elevation lower than min old elevation\n",
    "#                 if elev_min < df_all['# bin_center_elev_m'].values[0]:\n",
    "#                     elev2add = np.arange(elev_min,df_all['# bin_center_elev_m'].values[0], binsize)\n",
    "#                     df_all_add = pd.DataFrame(np.zeros((len(elev2add), len(df.columns))), columns=df.columns)\n",
    "#                     df_all_add['# bin_center_elev_m'] = elev2add\n",
    "#                     df_all = pd.concat([df_all,df_all_add], axis=0)\n",
    "#                     df_all.sort_values('# bin_center_elev_m', inplace=True)\n",
    "\n",
    "#                 # If new max elevation higher than max old elevation\n",
    "#                 if elev_max > df_all['# bin_center_elev_m'].values[-1]:\n",
    "#                     elev2add = np.arange(df_all['# bin_center_elev_m'].values[-1] + binsize, elev_max + 1, binsize)\n",
    "#                     df_all_add = pd.DataFrame(np.zeros((len(elev2add), len(df.columns))), columns=df.columns)\n",
    "#                     df_all_add['# bin_center_elev_m'] = elev2add\n",
    "#                     df_all = pd.concat([df_all,df_all_add], axis=0)\n",
    "#                     df_all.sort_values('# bin_center_elev_m', inplace=True)\n",
    "\n",
    "#                 # If new min elevation higher than min old elevation\n",
    "#                 if df_all['# bin_center_elev_m'].values[0] < elev_min:\n",
    "#                     elev2add = np.arange(df_all['# bin_center_elev_m'].values[0], elev_min, binsize)\n",
    "#                     df_add = pd.DataFrame(np.zeros((len(elev2add), len(df.columns))), columns=df.columns)\n",
    "#                     df_add['# bin_center_elev_m'] = elev2add\n",
    "#                     df = pd.concat([df,df_add], axis=0)\n",
    "#                     df.sort_values('# bin_center_elev_m', inplace=True)\n",
    "\n",
    "#                 # If new max elevation lower than max old elevation \n",
    "#                 if df_all['# bin_center_elev_m'].values[-1] > elev_max:\n",
    "#                     elev2add = np.arange(elev_max + binsize, df_all['# bin_center_elev_m'].values[-1] + 1, binsize)\n",
    "#                     df_add = pd.DataFrame(np.zeros((len(elev2add), len(df.columns))), columns=df.columns)\n",
    "#                     df_add['# bin_center_elev_m'] = elev2add\n",
    "#                     df = pd.concat([df,df_add], axis=0)\n",
    "#                     df.sort_values('# bin_center_elev_m', inplace=True)\n",
    "\n",
    "#                 df_all.reset_index(inplace=True, drop=True)\n",
    "#                 df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#                 # Merge bins (area-weighted)\n",
    "#                 df_all[' z1_bin_area_valid_km2'] = df_all[' z1_bin_area_valid_km2'] + df[' z1_bin_area_valid_km2']\n",
    "#                 df_all['area_debris_km2'] = df_all['area_debris_km2'] + df['area_debris_km2']\n",
    "#                 df_all[' perc_debris'] = df_all['area_debris_km2'] / df_all[' z1_bin_area_valid_km2'] * 100\n",
    "                \n",
    "#                 if len(debris_idx) > 1:\n",
    "#                     if df['# bin_center_elev_m'].values[debris_idx[0]] < zmin_debris:\n",
    "#                         zmin_debris = df['# bin_center_elev_m'].values[debris_idx[0]]\n",
    "#                     if df['# bin_center_elev_m'].values[debris_idx[-1]] > zmax_debris:\n",
    "#                         zmax_debris = df['# bin_center_elev_m'].values[debris_idx[-1]]\n",
    "\n",
    "#     # Area-weighted statistics\n",
    "#     if df_all is not None:\n",
    "#         # Assume 10 m horizontal resolution for computing the area-weighted statistics of the debris elevation\n",
    "#         pixel_res = 10\n",
    "\n",
    "#         # Estimate pixels in each bin\n",
    "#         df_all['pixels_debris'] = np.round(df_all['area_debris_km2'] / (pixel_res / 1000)**2, 0)\n",
    "\n",
    "#         elev_list_all = []\n",
    "#         for nelev, elev in enumerate(df_all['# bin_center_elev_m']):\n",
    "#             elev_list_single = list(np.repeat(elev, df_all.loc[nelev,'pixels_debris']))\n",
    "#             elev_list_all.extend(elev_list_single)\n",
    "\n",
    "#         # Compute statistics\n",
    "#         elev_mean = np.mean(elev_list_all)\n",
    "#         elev_std = np.std(elev_list_all)\n",
    "#         elev_med = malib.fast_median(elev_list_all)\n",
    "#         elev_mad = malib.mad(elev_list_all)\n",
    "\n",
    "        \n",
    "\n",
    "#         # Update array\n",
    "#         elevstats_mean[lat_idx,lon_idx] = elev_mean\n",
    "#         elevstats_std[lat_idx,lon_idx] = elev_std\n",
    "#         elevstats_med[lat_idx,lon_idx] = elev_med\n",
    "#         elevstats_mad[lat_idx,lon_idx] = elev_mad\n",
    "#         elevstats_min[lat_idx,lon_idx] = zmin_debris\n",
    "#         elevstats_max[lat_idx,lon_idx] = zmax_debris\n",
    "        \n",
    "#         print('mean +/- std:', np.round(elev_mean,0), '+/-', np.round(elev_std,0), \n",
    "#               ';  med +/- mad:', np.round(elev_med,0), '+/-', np.round(elev_mad,0), \n",
    "#               ';  zmin:', zmin_debris, 'zmax:', zmax_debris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export to dataset\n",
    "# ds_elevstats = xr.Dataset({'zmean': (['latitude', 'longitude'], elevstats_mean),\n",
    "#                            'zstd': (['latitude', 'longitude'], elevstats_std),\n",
    "#                            'zmed': (['latitude', 'longitude'], elevstats_med),\n",
    "#                            'zmad': (['latitude', 'longitude'], elevstats_mad),\n",
    "#                            'zmin': (['latitude', 'longitude'], elevstats_min),\n",
    "#                            'zmax': (['latitude', 'longitude'], elevstats_max),},\n",
    "#                           coords={'latitude': ds.latitude.values,\n",
    "#                                   'longitude': ds.longitude.values})\n",
    "# attrs_dict={\n",
    "#      'zmean':{'units':'m a.s.l.',\n",
    "#          'long_name':'mean elevation',\n",
    "#          'comment': 'mean elevation associated with the debris for the given lat/lon'},\n",
    "#      'zstd':{'units':'m a.s.l.',\n",
    "#          'long_name':'standard deviation of the debris elevation',\n",
    "#          'comment': 'standard deviation of the debris elevation associated with the debris for the given lat/lon'},\n",
    "#      'zmed':{'units':'m a.s.l.',\n",
    "#          'long_name':'median elevation',\n",
    "#          'comment': 'median elevation associated with the debris for the given lat/lon'},\n",
    "#      'zmad':{'units':'m a.s.l.',\n",
    "#          'long_name':'median absolute deviation of the debris elevation',\n",
    "#          'comment': 'median absolute deviation of the debris elevation associated with the debris for the given lat/lon'},\n",
    "#      'zmin':{'units':'m a.s.l.',\n",
    "#          'long_name':'minimum elevation',\n",
    "#          'comment': 'minimum elevation with >50% debris cover for the given lat/lon'},\n",
    "#      'zmax':{'units':'m a.s.l.',\n",
    "#          'long_name':'maximum elevation',\n",
    "#          'comment': 'maximum elevation with >50% debris cover for the given lat/lon'}}\n",
    "\n",
    "# for vn in ['zmean', 'zstd', 'zmed', 'zmad']:\n",
    "#     ds_elevstats[vn].attrs = attrs_dict[vn]\n",
    "    \n",
    "# ds_elevstats.to_netcdf(debris_elevstats_fullfn.replace('.nc','v2.nc'))\n",
    "                \n",
    "# print(ds_elevstats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lat_idx = 68\n",
    "# # lon_idx = 88\n",
    "# lat_idx = 37\n",
    "# lon_idx = 46\n",
    "# print(ds['latitude'][lat_idx].values, ds['longitude'][lon_idx].values,\n",
    "#       '\\n', ds_elevstats['zmean'][lat_idx,lon_idx].values, ds_elevstats['zstd'][lat_idx,lon_idx].values, \n",
    "#       ds_elevstats['zmed'][lat_idx,lon_idx].values, ds_elevstats['zmad'][lat_idx,lon_idx].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latlon_unique_dict[(68,88)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
