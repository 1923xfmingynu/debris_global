{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\"\"\"\n",
    "Compute debris thickness through sub-debris and temperature inversion methods\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from scipy import ndimage\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import median_absolute_deviation\n",
    "import xarray as xr\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "from pygeotools.lib import malib, warplib, geolib, iolib, timelib\n",
    "\n",
    "\n",
    "import debrisglobal.globaldebris_input as debris_prms\n",
    "from debrisglobal.glacfeat import GlacFeat, create_glacfeat\n",
    "from meltcurves import melt_fromdebris_func\n",
    "from meltcurves import debris_frommelt_func\n",
    "from spc_split_lists import split_list\n",
    "\n",
    "\n",
    "debug=False\n",
    "verbose=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, variance**0.5\n",
    "\n",
    "\n",
    "def weighted_percentile(sorted_list, weights, percentile):\n",
    "    \"\"\"\n",
    "    Calculate weighted percentile of a sorted list\n",
    "    \"\"\"\n",
    "    weights_cumsum_norm_high = np.cumsum(weights) / np.sum(weights)\n",
    "#     print(weights_cumsum_norm_high)\n",
    "    weights_norm = weights / np.sum(weights)\n",
    "    weights_cumsum_norm_low = weights_cumsum_norm_high - weights_norm\n",
    "#     print(weights_cumsum_norm_low)\n",
    "    \n",
    "    percentile_idx_high = np.where(weights_cumsum_norm_high >= percentile)[0][0]\n",
    "#     print(percentile_idx_high)\n",
    "    percentile_idx_low = np.where(weights_cumsum_norm_low <= percentile)[0][-1]\n",
    "#     print(percentile_idx_low)\n",
    "    \n",
    "    if percentile_idx_low == percentile_idx_high:\n",
    "        value_percentile = sorted_list[percentile_idx_low]\n",
    "    else:\n",
    "        value_percentile = np.mean([sorted_list[percentile_idx_low], sorted_list[percentile_idx_high]])\n",
    "\n",
    "    return value_percentile\n",
    "\n",
    "\n",
    "def pickle_data(fn, data):\n",
    "    \"\"\"Pickle data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fn : str\n",
    "        filename including filepath\n",
    "    data : list, etc.\n",
    "        data to be pickled\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    .pkl file\n",
    "        saves .pkl file of the data\n",
    "    \"\"\"\n",
    "    with open(fn, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "#Function to generate a 3-panel plot for input arrays\n",
    "def plot_array(dem, clim=None, titles=None, cmap='inferno', label=None, overlay=None, fn=None, close_fig=True):\n",
    "    fig, ax = plt.subplots(1,1, sharex=True, sharey=True, figsize=(10,5))\n",
    "    alpha = 1.0\n",
    "    #Gray background\n",
    "    ax.set_facecolor('0.5')\n",
    "    #Force aspect ratio to match images\n",
    "    ax.set(aspect='equal')\n",
    "    #Turn off axes labels/ticks\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if titles is not None:\n",
    "        ax.set_title(titles[0])\n",
    "    #Plot background shaded relief map\n",
    "    if overlay is not None:\n",
    "        alpha = 0.7\n",
    "        ax.imshow(overlay, cmap='gray', clim=(1,255))\n",
    "    #Plot each array\n",
    "    im_list = [ax.imshow(dem, clim=clim, cmap=cmap, alpha=alpha)]\n",
    "    fig.tight_layout()\n",
    "    fig.colorbar(im_list[0], label=label, extend='both', shrink=0.5)\n",
    "    if fn is not None:\n",
    "        fig.savefig(fn, bbox_inches='tight', pad_inches=0, dpi=150)\n",
    "    if close_fig:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roi: 01\n",
      "hd_mean (+/- std): 0.4 +/- 0.61\n",
      "  hd percentile (0.025): 0.0\n",
      "  hd percentile (0.05): 0.0\n",
      "  hd percentile (0.16): 0.03\n",
      "  hd percentile (0.25): 0.05\n",
      "  hd percentile (0.5): 0.18\n",
      "  hd percentile (0.75): 0.46\n",
      "  hd percentile (0.84): 0.66\n",
      "  hd percentile (0.95): 1.73\n",
      "  hd percentile (0.975): 3.0\n",
      "hd_low_mean (+/- std): 0.24 +/- 0.33\n",
      "  hd_low percentile (0.025): 0\n",
      "  hd_low percentile (0.05): 0\n",
      "  hd_low percentile (0.16): 0.01\n",
      "  hd_low percentile (0.25): 0.03\n",
      "  hd_low percentile (0.5): 0.12\n",
      "  hd_low percentile (0.75): 0.3\n",
      "  hd_low percentile (0.84): 0.42\n",
      "  hd_low percentile (0.95): 1.01\n",
      "  hd_low percentile (0.975): 1.51\n",
      "hd_high_mean (+/- std): 0.7 +/- 1.41\n",
      "  hd_high percentile (0.025): 0.04\n",
      "  hd_high percentile (0.05): 0.04\n",
      "  hd_high percentile (0.16): 0.06\n"
     ]
    }
   ],
   "source": [
    "# Glaciers optimized\n",
    "overwrite = False\n",
    "# rois = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14', '15', '16','17','18']\n",
    "# rois = ['13','14','15']\n",
    "rois = ['01','02','03','04','05','06','07','08','09','10']\n",
    "\n",
    "# Percentiles\n",
    "percentiles = [0.025, 0.05, 0.16, 0.25, 0.5, 0.75, 0.84, 0.95, 0.975]\n",
    "\n",
    "# Uncertainty dataframe and dictionary for bounds\n",
    "hd_uncertainty_fullfn = debris_prms.output_fp + 'hd_uncertainty_bnds_1std.csv'\n",
    "hd_uncertainty_df = pd.read_csv(hd_uncertainty_fullfn)\n",
    "hd_uncertainty_dict_low = dict(zip([int(np.round(x*100)) for x in hd_uncertainty_df['hd_m']], \n",
    "                                   list(hd_uncertainty_df['hd_bndlow_both'].values)))\n",
    "hd_uncertainty_dict_low[0] = 0\n",
    "hd_uncertainty_dict_low[1] = 0\n",
    "hd_uncertainty_dict_high = dict(zip([int(np.round(x*100)) for x in hd_uncertainty_df['hd_m']], \n",
    "                                   list(hd_uncertainty_df['hd_bndhigh_both'].values)))\n",
    "hd_uncertainty_dict_high[0] = hd_uncertainty_df.loc[0,'hd_bndhigh_both']\n",
    "hd_uncertainty_dict_high[1] = hd_uncertainty_df.loc[0,'hd_bndhigh_both']\n",
    "\n",
    "\n",
    "\n",
    "# Regional stats dataframe\n",
    "reg_stats_fullfn = debris_prms.output_fp + 'reg_stats_hd_mf.csv'\n",
    "reg_stats_cns = ['roi', 'dc_area_km2',\n",
    "                 'hd_mean', 'hd_std', \n",
    "                 'hd_025', 'hd_05', 'hd_16', 'hd_25', 'hd_med', 'hd_75', 'hd_84', 'hd_95', 'hd_975',\n",
    "                 'hd_low_mean', 'hd_low_std', \n",
    "                 'hd_low_025', 'hd_low_05', 'hd_low_16', 'hd_low_25', 'hd_low_med', 'hd_low_75', 'hd_low_84', 'hd_low_95', 'hd_low_975',\n",
    "                 'hd_high_mean', 'hd_high_std', \n",
    "                 'hd_high_025', 'hd_high_05', 'hd_high_16', 'hd_high_25', 'hd_high_med', 'hd_high_75', 'hd_high_84', 'hd_high_95', 'hd_high_975',\n",
    "                 'mf_mean', 'mf_std', \n",
    "                 'mf_025', 'mf_05', 'mf_16', 'mf_25', 'mf_med', 'mf_75', 'mf_84', 'mf_95', 'mf_975',\n",
    "                 'mf_low_mean', 'mf_low_std', \n",
    "                 'mf_low_025', 'mf_low_05', 'mf_low_16', 'mf_low_25', 'mf_low_med', 'mf_low_75', 'mf_low_84', 'mf_low_95', 'mf_low_975',\n",
    "                 'mf_high_mean', 'mf_high_std', \n",
    "                 'mf_high_025', 'mf_high_05', 'mf_high_16', 'mf_high_25', 'mf_high_med', 'mf_high_75', 'mf_high_84', 'mf_high_95', 'mf_high_975']\n",
    "reg_stats_df = pd.DataFrame(np.zeros((len(rois)+1,len(reg_stats_cns))), columns=reg_stats_cns)\n",
    "\n",
    "\n",
    "## ===== REGIONAL MELT FACTOR STATISTICS =====\n",
    "hd_list_all_global = []\n",
    "hd_list_all_low_global = []\n",
    "hd_list_all_high_global = []\n",
    "mf_list_all_global = []\n",
    "mf_list_all_low_global = []\n",
    "mf_list_all_high_global = []\n",
    "area_m2_list_all_global = []\n",
    "for nroi, roi in enumerate(rois):\n",
    "        \n",
    "    print('roi:', roi)\n",
    "    \n",
    "    # Load file if it already exists\n",
    "    list_fp = debris_prms.output_fp + 'pickle_datasets/'\n",
    "    if not os.path.exists(list_fp):\n",
    "        os.makedirs(list_fp)\n",
    "    hd_list_all_fullfn = list_fp + roi + '_hd_list_all.pkl'\n",
    "    mf_list_all_fullfn = list_fp + roi + '_mf_list_all.pkl'\n",
    "    area_m2_list_all_fullfn = list_fp + roi + '_area_m2_list_all.pkl'\n",
    "    \n",
    "\n",
    "    if os.path.exists(hd_list_all_fullfn.replace('.pkl','_low.pkl')) and not overwrite:\n",
    "        # Debris thickness\n",
    "        with open(hd_list_all_fullfn, 'rb') as f:\n",
    "            hd_list_all = pickle.load(f)\n",
    "        with open(hd_list_all_fullfn.replace('.pkl','_low.pkl'), 'rb') as f:\n",
    "            hd_list_all_low = pickle.load(f)\n",
    "        with open(hd_list_all_fullfn.replace('.pkl','_high.pkl'), 'rb') as f:\n",
    "            hd_list_all_high = pickle.load(f)\n",
    "        # Melt factor\n",
    "        with open(mf_list_all_fullfn, 'rb') as f:\n",
    "            mf_list_all = pickle.load(f)\n",
    "        with open(mf_list_all_fullfn.replace('.pkl','_low.pkl'), 'rb') as f:\n",
    "            mf_list_all_low = pickle.load(f)\n",
    "        with open(mf_list_all_fullfn.replace('.pkl','_high.pkl'), 'rb') as f:\n",
    "            mf_list_all_high = pickle.load(f)\n",
    "        # Area\n",
    "        with open(area_m2_list_all_fullfn, 'rb') as f:\n",
    "            area_m2_list_all = pickle.load(f)\n",
    "    else:\n",
    "        \n",
    "        rgiids = []\n",
    "        hd_fns = []\n",
    "        # Filepaths\n",
    "        if roi in ['13', '14', '15']:\n",
    "            hd_fp = debris_prms.output_fp + 'ts_tif/hd_tifs/HMA/'\n",
    "            hdopt_prms_fp = debris_prms.output_fp + 'hd_opt_prms/HMA/'\n",
    "        else:\n",
    "            hd_fp = debris_prms.output_fp + 'ts_tif/hd_tifs/' + roi + '/'\n",
    "            hdopt_prms_fp = debris_prms.output_fp + 'hd_opt_prms/' + roi + '/'\n",
    "        hd_fp_extrap = hd_fp + 'extrap/'\n",
    "        hdopt_prms_fp_extrap = hdopt_prms_fp + '/_extrap/'\n",
    "        mf_fp = hd_fp + 'meltfactor/'\n",
    "        mf_fp_extrap = hd_fp_extrap + 'meltfactor/'\n",
    "\n",
    "        # Glaciers optimized\n",
    "        glac_hd_fullfns = []\n",
    "        for i in os.listdir(hd_fp):\n",
    "            if i.endswith('hdts_m.tif'):\n",
    "                reg_str = str(int(i.split('.')[0])).zfill(2)\n",
    "                if reg_str == roi:\n",
    "                    hd_fns.append(i)\n",
    "                    rgiids.append(i.split('_')[0])\n",
    "\n",
    "        # Glaciers extrapolated\n",
    "        for i in os.listdir(hd_fp_extrap):\n",
    "            if i.endswith('hdts_m_extrap.tif'):\n",
    "                reg_str = str(int(i.split('.')[0])).zfill(2)\n",
    "                if reg_str == roi:\n",
    "                    hd_fns.append(i)\n",
    "                    rgiids.append(i.split('_')[0])\n",
    "\n",
    "        # Sorted files        \n",
    "        hd_fns = [x for _,x in sorted(zip(rgiids, hd_fns))]\n",
    "        rgiids = sorted(rgiids)     \n",
    "\n",
    "        main_glac_rgi = debris_prms.selectglaciersrgitable(rgiids)\n",
    "        main_glac_rgi['CenLon_360'] = main_glac_rgi['CenLon']\n",
    "        main_glac_rgi.loc[main_glac_rgi['CenLon_360'] < 0, 'CenLon_360'] = (\n",
    "            360 + main_glac_rgi.loc[main_glac_rgi['CenLon_360'] < 0, 'CenLon_360'])\n",
    "        main_glac_rgi['hd_fn'] = hd_fns\n",
    "        \n",
    "        hd_list_all = []\n",
    "        hd_list_all_low = []\n",
    "        hd_list_all_high = []\n",
    "        mf_list_all = []\n",
    "        mf_list_all_low = []\n",
    "        mf_list_all_high = []\n",
    "        area_m2_list_all = []\n",
    "        for nglac, glac_idx in enumerate(main_glac_rgi.index.values):\n",
    "#         for nglac, glac_idx in enumerate(main_glac_rgi.index.values[613:614]):\n",
    "            glac_str = main_glac_rgi.loc[glac_idx,'rgino_str']\n",
    "            rgiid = main_glac_rgi.loc[glac_idx,'RGIId']\n",
    "            region = glac_str.split('.')[0]\n",
    "\n",
    "            if int(region) < 10:\n",
    "                glac_str_noleadzero = str(int(glac_str.split('.')[0])) + '.' + glac_str.split('.')[1]\n",
    "            else:\n",
    "                glac_str_noleadzero = glac_str\n",
    "\n",
    "            if nglac%1000 == 0:\n",
    "#             if nglac%1 == 0:\n",
    "                print(nglac, glac_str)\n",
    "\n",
    "            # Create glacier feature from ice thickness raster\n",
    "            thick_dir = debris_prms.oggm_fp + 'thickness/RGI60-' + str(region.zfill(2)) + '/'\n",
    "            thick_fn = 'RGI60-' + str(region.zfill(2)) + '.' + rgiid.split('.')[1] + '_thickness.tif'\n",
    "\n",
    "            gf = create_glacfeat(thick_dir, thick_fn)\n",
    "\n",
    "            # =====FILENAMES =====\n",
    "            # Add the filenames\n",
    "            fn_dict = OrderedDict()\n",
    "            # DEM\n",
    "            z1_fp = debris_prms.oggm_fp + 'dems/RGI60-' + str(region.zfill(2)) + '/'\n",
    "            z1_fn = 'RGI60-' + str(region.zfill(2)) + '.' + rgiid.split('.')[1] + '_dem.tif'\n",
    "            fn_dict['z1'] = z1_fp + z1_fn\n",
    "\n",
    "            # Debris thickness and melt factors\n",
    "            hd_fn = main_glac_rgi.loc[glac_idx, 'hd_fn']\n",
    "            if '_extrap' not in hd_fn:\n",
    "                hd_fullfn = hd_fp + hd_fn\n",
    "                mf_fullfn = mf_fp + hd_fn.replace('hdts_m', 'meltfactor')\n",
    "                hdopt_prms_fullfn = hdopt_prms_fp + glac_str_noleadzero + '_hdopt_prms.csv'\n",
    "            else:\n",
    "                hd_fullfn = hd_fp_extrap + hd_fn\n",
    "                mf_fullfn = mf_fp_extrap + hd_fn.replace('hdts_m', 'meltfactor')\n",
    "                hdopt_prms_fullfn = hdopt_prms_fp_extrap + glac_str + '_hdopt_prms_extrap.csv'\n",
    "                \n",
    "            fn_dict['debris_thick_ts'] = hd_fullfn\n",
    "            fn_dict['meltfactor_ts'] = mf_fullfn\n",
    "\n",
    "            # Ice thickness\n",
    "            thick_dir = debris_prms.oggm_fp + 'thickness/RGI60-' + str(region.zfill(2)) + '/'\n",
    "            thick_fn = 'RGI60-' + str(region.zfill(2)) + '.' + rgiid.split('.')[1] + '_thickness.tif'\n",
    "            fn_dict['ice_thick'] = thick_dir + thick_fn\n",
    "\n",
    "            # ===== PROCESS THE DATA =====\n",
    "            #Expand extent to include buffered region around glacier polygon\n",
    "            warp_extent = geolib.pad_extent(gf.glac_geom_extent, width=debris_prms.buff_dist)\n",
    "            if verbose:\n",
    "                print(\"Expanding extent\")\n",
    "                print(gf.glac_geom_extent)\n",
    "                print(warp_extent)\n",
    "                print(gf.aea_srs)\n",
    "\n",
    "            #Warp everything to common res/extent/proj\n",
    "            z1_gt = gdal.Open(fn_dict['z1']).GetGeoTransform()\n",
    "            z1_res = np.min([z1_gt[1], -z1_gt[5]])\n",
    "            # resampling algorithm\n",
    "            r_resampling = 'cubic'\n",
    "            ds_list = warplib.memwarp_multi_fn(fn_dict.values(), res=z1_res, extent=warp_extent, \n",
    "                                               t_srs=gf.aea_srs, verbose=verbose, r=r_resampling)\n",
    "            ds_dict = dict(zip(fn_dict.keys(), ds_list))\n",
    "            gf.ds_dict = ds_dict\n",
    "\n",
    "            if verbose:\n",
    "                print(ds_list)\n",
    "                print(fn_dict.keys())\n",
    "\n",
    "            glac_geom_mask = geolib.geom2mask(gf.glac_geom, ds_dict['z1'])\n",
    "            gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1']), mask=glac_geom_mask)\n",
    "            \n",
    "            # Debris thickness values of 0 are masked (use meltfactor mask instead)\n",
    "            gf.meltfactor_ts = np.ma.array(iolib.ds_getma(ds_dict['meltfactor_ts']), mask=glac_geom_mask)\n",
    "            gf.debris_thick_ts = np.ma.array(iolib.ds_getma(ds_dict['debris_thick_ts']), mask=glac_geom_mask)\n",
    "            gf.debris_thick_ts = np.ma.array(gf.debris_thick_ts.data, mask=gf.meltfactor_ts.mask)\n",
    "            \n",
    "#             # Melt factors are masked so only calculate over areas with debris > 0\n",
    "#             gf.debris_thick_ts = np.ma.array(iolib.ds_getma(ds_dict['debris_thick_ts']), mask=glac_geom_mask)\n",
    "#             gf.meltfactor_ts = np.ma.array(iolib.ds_getma(ds_dict['meltfactor_ts']), mask=glac_geom_mask)       \n",
    "#             gf.meltfactor_ts = np.ma.array(gf.meltfactor_ts.data, mask=gf.debris_thick_ts.mask)\n",
    "            \n",
    "            gf.res = geolib.get_res(ds_dict['z1'])\n",
    "\n",
    "            if verbose:\n",
    "                print('\\n\\n# z1 pixels:', gf.z1.count(), '\\n')\n",
    "                \n",
    "            \n",
    "\n",
    "            # ===== PLOTS =====\n",
    "            show_plots = False\n",
    "            if debug and show_plots:\n",
    "                # DEM\n",
    "                var_full2plot = gf.z1.copy()\n",
    "                clim = malib.calcperc(var_full2plot, (2,98))\n",
    "                plot_array(var_full2plot, clim, [glac_str + ' DEM'], 'inferno', 'elev (masl)', close_fig=False)\n",
    "                # Debris thickness\n",
    "                var_full2plot = gf.debris_thick_ts.copy()\n",
    "                clim = (0,1)\n",
    "                plot_array(var_full2plot, clim, [gf.glacnum + ' hd (from ts)'], 'inferno', 'hd (m)', \n",
    "                           close_fig=False)\n",
    "                # Melt factor\n",
    "                var_full2plot = gf.meltfactor_ts.copy()\n",
    "                clim = (0,1)\n",
    "                plot_array(var_full2plot, clim, [gf.glacnum + ' meltfactor'], 'inferno', 'mf (-)',\n",
    "                           close_fig=False)\n",
    "    #             # Surface temperature\n",
    "    #             var_full2plot = gf.ts.copy()\n",
    "    #             clim = malib.calcperc(var_full2plot, (2,98))\n",
    "    #             plot_array(var_full2plot, clim, [glac_str + ' Ts'], 'inferno', 'ts (degC)', close_fig=False)\n",
    "\n",
    "            # Get list of values\n",
    "            hd_list = list(gf.debris_thick_ts.compressed())\n",
    "            mf_list = list(gf.meltfactor_ts.compressed())\n",
    "            \n",
    "            # remove nan values\n",
    "            hd_list = [0 if np.isnan(x) else x for x in hd_list]\n",
    "            \n",
    "            if len(hd_list) > 0:\n",
    "                # Remove nan values\n",
    "                hd_array_nonan = np.array(hd_list)\n",
    "                nan_idx_list = [x[0] for x in list(np.argwhere(np.isnan(hd_array_nonan)))]\n",
    "                if len(nan_idx_list) > 0:\n",
    "                    hd_list = [y for x,y in enumerate(hd_list) if x not in nan_idx_list]\n",
    "                    mf_list = [y for x,y in enumerate(mf_list) if x not in nan_idx_list]\n",
    "\n",
    "                assert len(hd_list) == len(mf_list), 'hd_list and mf_list differ; NEED TO MASK THESE VALUES OR RE-PROCESS'\n",
    "                rounding_err = 1e-6\n",
    "                assert np.max(hd_list) <= debris_prms.hd_max + rounding_err and np.min(hd_list) >= -rounding_err, 'hd outside of bounds' # rounding error may give -1e-12 for some values\n",
    "                assert np.min(mf_list) >= -rounding_err, 'negative melt factor' \n",
    "                assert np.max(mf_list) <= 10, 'melt factor greater than 10!'\n",
    "\n",
    "                pixel_m2 = abs(gf.res[0] * gf.res[1])\n",
    "                area_m2_list = [pixel_m2] * len(hd_list)\n",
    "\n",
    "                # Append to existing\n",
    "                hd_list_all.extend(hd_list)\n",
    "                mf_list_all.extend(mf_list)\n",
    "                area_m2_list_all.extend(area_m2_list)\n",
    "                \n",
    "                \n",
    "                # ----- Uncertainty: hd_list and mf_list -----\n",
    "                # Uncertainty for lower and upper bounds\n",
    "                hd_list_low = [hd_uncertainty_dict_low[x] for x in list(np.round(np.array(hd_list)*100,0).astype(int))]\n",
    "                hd_list_high = [hd_uncertainty_dict_high[x] for x in list(np.round(np.array(hd_list)*100,0).astype(int))]\n",
    "\n",
    "\n",
    "                # Optimized parameters for melt factor uncertainties\n",
    "                df_opt = pd.read_csv(hdopt_prms_fullfn)\n",
    "                melt_2cm = df_opt.loc[0,'melt_mwea_2cm']\n",
    "                melt_cleanice = df_opt.loc[0,'melt_mwea_clean']\n",
    "                func_coeff = [df_opt.loc[0,'b0'], df_opt.loc[0,'k']]\n",
    "\n",
    "                # Melt factor (lower bound)\n",
    "                mf_array_low = melt_fromdebris_func(np.array(hd_list_low), func_coeff[0], func_coeff[1]) / melt_cleanice\n",
    "                # limit melt rates to modeled 2 cm rate\n",
    "                mf_array_low[mf_array_low > melt_2cm / melt_cleanice] = melt_2cm / melt_cleanice\n",
    "                # Linearly interpolate between 0 cm and 2 cm for the melt rate\n",
    "                def meltfactor_0to2cm_adjustment(mf, melt_clean, melt_2cm, hd):\n",
    "                    \"\"\" Linearly interpolate melt factors between 0 and 2 cm \n",
    "                        based on clean ice and 2 cm sub-debris melt \"\"\"\n",
    "                    mf = np.nan_to_num(mf,0)\n",
    "                    mf[(hd >= 0) & (hd < 0.02)] = (\n",
    "                        1 + hd[(hd >= 0) & (hd < 0.02)] / 0.02 * (melt_2cm - melt_clean) / melt_clean)\n",
    "                    return mf\n",
    "                mf_array_low = meltfactor_0to2cm_adjustment(mf_array_low, melt_cleanice, melt_2cm, np.array(hd_list_low))\n",
    "\n",
    "                # Melt factor (lower bound)\n",
    "                mf_array_high = melt_fromdebris_func(np.array(hd_list_high), func_coeff[0], func_coeff[1]) / melt_cleanice\n",
    "                mf_array_high[mf_array_high > melt_2cm / melt_cleanice] = melt_2cm / melt_cleanice\n",
    "                mf_array_high = meltfactor_0to2cm_adjustment(mf_array_high, melt_cleanice, melt_2cm, np.array(hd_list_high))\n",
    "\n",
    "                # Append lists\n",
    "                hd_list_all_low.extend(hd_list_low)\n",
    "                hd_list_all_high.extend(hd_list_high)\n",
    "                mf_list_all_low.extend(list(mf_array_low))\n",
    "                mf_list_all_high.extend(list(mf_array_high))\n",
    "\n",
    "        # ===== EXPORT LISTS =====\n",
    "        pickle_data(hd_list_all_fullfn, hd_list_all)\n",
    "        pickle_data(hd_list_all_fullfn.replace('.pkl','_low.pkl'), hd_list_all_low)\n",
    "        pickle_data(hd_list_all_fullfn.replace('.pkl','_high.pkl'), hd_list_all_high)\n",
    "        pickle_data(mf_list_all_fullfn, mf_list_all)\n",
    "        pickle_data(mf_list_all_fullfn.replace('.pkl','_low.pkl'), mf_list_all_low)\n",
    "        pickle_data(mf_list_all_fullfn.replace('.pkl','_high.pkl'), mf_list_all_high)\n",
    "        pickle_data(area_m2_list_all_fullfn, area_m2_list_all)\n",
    "        \n",
    "    \n",
    "    # Aggregate global data\n",
    "    hd_list_all_global.extend(hd_list_all)\n",
    "    hd_list_all_low_global.extend(hd_list_all_low)\n",
    "    hd_list_all_high_global.extend(hd_list_all_high)\n",
    "    mf_list_all_global.extend(mf_list_all)\n",
    "    mf_list_all_low_global.extend(mf_list_all_low)\n",
    "    mf_list_all_high_global.extend(mf_list_all_high)\n",
    "    area_m2_list_all_global.extend(area_m2_list_all)\n",
    "    \n",
    "    \n",
    "    def reg_stats_weighted_fromlist(list_all, area_m2_list_all, percentiles, print_name=None):\n",
    "        \"\"\" Compute weighted regional stats based on list of debris thickness or melt factors and area\"\"\"\n",
    "        # Sort for weighted statistics\n",
    "        sorted_area_m2 = [x for _,x in sorted(zip(list_all, area_m2_list_all))]\n",
    "        sorted_list = sorted(list_all)\n",
    "        \n",
    "        # Regional statistics\n",
    "        list_mean, list_std = weighted_avg_and_std(sorted_list, weights=sorted_area_m2)\n",
    "        if print_name is not None:\n",
    "            print(print_name + '_mean (+/- std): ' + str(np.round(list_mean,2)) + ' +/- ' + str(np.round(list_std,2)))\n",
    "        reg_stats_values = []\n",
    "        reg_stats_values.append(list_mean)\n",
    "        reg_stats_values.append(list_std)\n",
    "        for percentile in percentiles:\n",
    "            value_percentile = weighted_percentile(sorted_list, sorted_area_m2, percentile)\n",
    "            reg_stats_values.append(value_percentile)\n",
    "            print('  ' + print_name + ' percentile (' + str(percentile) + '): ' +  str(np.round(value_percentile,2)))\n",
    "        return reg_stats_values\n",
    "    \n",
    "    # Compute regional statistics\n",
    "    reg_stats_values = [roi, np.sum(area_m2_list_all) / 1e6]\n",
    "    # ----- Debris thickness -----\n",
    "    reg_stats_subset = reg_stats_weighted_fromlist(hd_list_all, area_m2_list_all, percentiles, print_name='hd')\n",
    "    reg_stats_values.extend(reg_stats_subset)\n",
    "    # Debris thickness (low uncertainty)\n",
    "    reg_stats_subset = reg_stats_weighted_fromlist(hd_list_all_low, area_m2_list_all, percentiles, print_name='hd_low')\n",
    "    reg_stats_values.extend(reg_stats_subset)\n",
    "    # Debris thickness (high uncertainty)\n",
    "    reg_stats_subset = reg_stats_weighted_fromlist(hd_list_all_high, area_m2_list_all, percentiles, print_name='hd_high')\n",
    "    reg_stats_values.extend(reg_stats_subset)\n",
    "    # ----- Melt factor -----\n",
    "    reg_stats_subset = reg_stats_weighted_fromlist(mf_list_all, area_m2_list_all, percentiles, print_name='mf')\n",
    "    reg_stats_values.extend(reg_stats_subset)\n",
    "    # Melt factor (low uncertainty)\n",
    "    reg_stats_subset = reg_stats_weighted_fromlist(mf_list_all_low, area_m2_list_all, percentiles, print_name='mf_low')\n",
    "    reg_stats_values.extend(reg_stats_subset)\n",
    "    # Melt factor\n",
    "    reg_stats_subset = reg_stats_weighted_fromlist(mf_list_all_high, area_m2_list_all, percentiles, print_name='mf_high')\n",
    "    reg_stats_values.extend(reg_stats_subset)\n",
    "    \n",
    "    # Record regional stats\n",
    "    reg_stats_df.loc[nroi,:] = reg_stats_values\n",
    "    \n",
    "#     print(reg_stats_values)\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "# # GLOBAL STATISTICS\n",
    "# nroi += 1\n",
    "# hd_list_all = hd_list_all_global\n",
    "# hd_list_all_low = hd_list_all_low_global\n",
    "# hd_list_all_high = hd_list_all_high_global\n",
    "# mf_list_all = mf_list_all_global\n",
    "# mf_list_all_low = mf_list_all_low_global\n",
    "# mf_list_all_high = mf_list_all_high_global\n",
    "# area_m2_list_all = area_m2_list_all_global\n",
    "\n",
    "# # Compute regional statistics\n",
    "# reg_stats_values = ['all', np.sum(area_m2_list_all) / 1e6]\n",
    "# # ----- Debris thickness -----\n",
    "# reg_stats_subset = reg_stats_weighted_fromlist(hd_list_all, area_m2_list_all, percentiles, print_name='hd')\n",
    "# reg_stats_values.extend(reg_stats_subset)\n",
    "# # Debris thickness (low uncertainty)\n",
    "# reg_stats_subset = reg_stats_weighted_fromlist(hd_list_all_low, area_m2_list_all, percentiles, print_name='hd_low')\n",
    "# reg_stats_values.extend(reg_stats_subset)\n",
    "# # Debris thickness (high uncertainty)\n",
    "# reg_stats_subset = reg_stats_weighted_fromlist(hd_list_all_high, area_m2_list_all, percentiles, print_name='hd_high')\n",
    "# reg_stats_values.extend(reg_stats_subset)\n",
    "# # ----- Melt factor -----\n",
    "# reg_stats_subset = reg_stats_weighted_fromlist(mf_list_all, area_m2_list_all, percentiles, print_name='mf')\n",
    "# reg_stats_values.extend(reg_stats_subset)\n",
    "# # Melt factor (low uncertainty)\n",
    "# reg_stats_subset = reg_stats_weighted_fromlist(mf_list_all_low, area_m2_list_all, percentiles, print_name='mf_low')\n",
    "# reg_stats_values.extend(reg_stats_subset)\n",
    "# # Melt factor\n",
    "# reg_stats_subset = reg_stats_weighted_fromlist(mf_list_all_high, area_m2_list_all, percentiles, print_name='mf_high')\n",
    "# reg_stats_values.extend(reg_stats_subset)\n",
    "\n",
    "# reg_stats_df.loc[nroi,:] = reg_stats_values\n",
    "\n",
    "# ==== Export regional stats =====\n",
    "# reg_stats_df.to_csv(reg_stats_fullfn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n  - BETTER TO QUANTIFY STATISTICS OVER DEBRIS-COVERED AREAS THAT ARE ESTIMATED!!!!!')\n",
    "print('    --> use debris thickness to mask melt factors as opposed to the other way around!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Melt factor stats\n",
    "#     mf_mean, mf_std = weighted_avg_and_std(mf_list_all, weights=area_m2_list_all)\n",
    "#     print('hd_mean (+/- std): ' + str(np.round(mf_mean,2)) + ' +/- ' + str(np.round(mf_std,2)))\n",
    "\n",
    "#     percentiles = [0.025, 0.05, 0.16, 0.25, 0.5, 0.75, 0.84, 0.95, 0.975]\n",
    "#     for percentile in percentiles:\n",
    "#         value_percentile = weighted_percentile(sorted_mf, sorted_area_m2_4mf)\n",
    "#         print('  mf percentile (' + str(percentile) + '): ' +  str(np.round(value_percentile,2)))\n",
    "# batman = []\n",
    "# for nweight, weight in enumerate(area_m2_list_all):\n",
    "#     batman.extend([hd_list_all[nweight]] * int(weight))\n",
    "# print(len(batman))\n",
    "# np.median(batman)\n",
    "# print(np.median(batman), np.mean(batman))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for the region of debris-covered data\n",
    "rois = ['01','02','03','04','05','06','07','08','09','10','11','12','HMA','16','17','18']\n",
    "for roi in rois:\n",
    "    dc_shp = gpd.read_file(debris_prms.debriscover_fp + debris_prms.debriscover_fn_dict[roi])\n",
    "    dc_shp = dc_shp.sort_values(by=['RGIId'])\n",
    "\n",
    "    dc_rgiids = [str(int(x.split('-')[1].split('.')[0])) + '.' + x.split('-')[1].split('.')[1] for x in dc_shp.RGIId]\n",
    "    \n",
    "    mb_bin_all_fp = debris_prms.output_fp + 'mb_bins_all/csv/' + roi + '/'\n",
    "    dc_rgiids_wdata = []\n",
    "    for i in os.listdir(mb_bin_all_fp):\n",
    "        glac_str = i.split('_')[0]\n",
    "        if glac_str in dc_rgiids:\n",
    "            dc_rgiids_wdata.append(glac_str)\n",
    "            \n",
    "    # Select glaciers\n",
    "    main_glac_rgi_dc = debris_prms.selectglaciersrgitable(dc_rgiids)\n",
    "    main_glac_rgi_dc_wdata = debris_prms.selectglaciersrgitable(dc_rgiids_wdata)\n",
    "\n",
    "    # Add debris stats to area\n",
    "    dc_areaperc_dict = dict(zip(dc_shp.RGIId.values,dc_shp['DC_Area__1'].values))\n",
    "    dc_area_dict = dict(zip(dc_shp.RGIId.values,dc_shp['DC_Area_v2'].values))\n",
    "    \n",
    "    main_glac_rgi_dc['DC_Area_%'] = main_glac_rgi_dc.RGIId.map(dc_areaperc_dict).fillna(0)\n",
    "    main_glac_rgi_dc_wdata['DC_Area_%'] = main_glac_rgi_dc_wdata.RGIId.map(dc_areaperc_dict).fillna(0)\n",
    "    main_glac_rgi_dc['DC_Area_v2'] = main_glac_rgi_dc['Area'] * main_glac_rgi_dc['DC_Area_%'] / 100\n",
    "    main_glac_rgi_dc_wdata['DC_Area_v2'] = main_glac_rgi_dc_wdata['Area'] * main_glac_rgi_dc_wdata['DC_Area_%'] / 100\n",
    "\n",
    "    # Subset of glaciers\n",
    "    main_glac_rgi_dc_gt2km2 = (\n",
    "        main_glac_rgi_dc[((main_glac_rgi_dc['DC_Area_%'] > debris_prms.dc_percarea_threshold) |\n",
    "                          (main_glac_rgi_dc['DC_Area_v2'] / 1e6 > debris_prms.dc_area_threshold))\n",
    "                         & (main_glac_rgi_dc['Area'] > debris_prms.min_glac_area)].copy())\n",
    "    \n",
    "    # Statistics of interest\n",
    "    print('\\n', roi + ': ', main_glac_rgi_dc.shape[0], 'glaciers -',\n",
    "          str(np.round(main_glac_rgi_dc['DC_Area_v2'].sum(),1)), 'km2')  \n",
    "    \n",
    "    print('  (> 2 km2): ', main_glac_rgi_dc_gt2km2.shape[0], 'glaciers -',\n",
    "          str(np.round(main_glac_rgi_dc_gt2km2['DC_Area_v2'].sum(),1)), 'km2')  \n",
    "    \n",
    "    print('  (w data): ', main_glac_rgi_dc_wdata.shape[0], 'glaciers -',\n",
    "          str(np.round(main_glac_rgi_dc_wdata['DC_Area_v2'].sum(),1)), 'km2\\n\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of debris-covered area less than 1 m\n",
    "print('find fraction less than 1 m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         # Create glacier feature from ice thickness raster\n",
    "#         thick_dir = debris_prms.oggm_fp + 'thickness/RGI60-' + str(region.zfill(2)) + '/'\n",
    "#         thick_fn = 'RGI60-' + str(region.zfill(2)) + '.' + rgiid.split('.')[1] + '_thickness.tif'\n",
    "#         gf = create_glacfeat(thick_dir, thick_fn)\n",
    "\n",
    "#         # Debris shape layer processing\n",
    "#         dc_shp_proj_fn = (debris_prms.glac_shp_proj_fp + glac_str + '_dc_crs' + \n",
    "#                           str(gf.aea_srs.GetAttrValue(\"AUTHORITY\", 1)) + '.shp')\n",
    "#         if not os.path.exists(dc_shp_proj_fn):\n",
    "#             dc_shp_init = gpd.read_file(debris_prms.debriscover_fp + \n",
    "#                                         debris_prms.debriscover_fn_dict[debris_prms.roi])\n",
    "#             dc_shp_single = dc_shp_init[dc_shp_init['RGIId'] == rgiid]\n",
    "#             dc_shp_single = dc_shp_single.reset_index()\n",
    "#             dc_shp_proj = dc_shp_single.to_crs({'init': 'epsg:' + \n",
    "#                                                 str(gf.aea_srs.GetAttrValue(\"AUTHORITY\", 1))})\n",
    "#             dc_shp_proj.to_file(dc_shp_proj_fn)\n",
    "#         dc_shp_ds = ogr.Open(dc_shp_proj_fn, 0)\n",
    "#         dc_shp_lyr = dc_shp_ds.GetLayer()\n",
    "\n",
    "#         # Add layers\n",
    "#         gf.add_layers(dc_shp_lyr, gf_add_dhdt=True, dhdt_fn=dhdt_fn_wglacier, gf_add_vel=True, \n",
    "#                       vx_fn=vx_fn_wglacier, gf_add_ts=True, ts_fn=ts_fn, gf_add_slope_aspect=False)\n",
    "\n",
    "#         # ===== PLOTS =====\n",
    "#         if debug:\n",
    "#             # DEM\n",
    "#             var_full2plot = gf.z1.copy()\n",
    "#             clim = malib.calcperc(var_full2plot, (2,98))\n",
    "#             plot_array(var_full2plot, clim, [glac_str + ' DEM'], 'inferno', 'elev (masl)', close_fig=False)\n",
    "#             # Surface temperature\n",
    "#             var_full2plot = gf.ts.copy()\n",
    "#             clim = malib.calcperc(var_full2plot, (2,98))\n",
    "#             plot_array(var_full2plot, clim, [glac_str + ' Ts'], 'inferno', 'ts (degC)', close_fig=False)\n",
    "#             # Surface temperature (debris-covered)\n",
    "#             var_full2plot = gf.ts.copy()\n",
    "#             var_full2plot.mask = gf.dc_mask\n",
    "#             clim = malib.calcperc(var_full2plot, (2,98))\n",
    "#             plot_array(var_full2plot, clim, [glac_str + ' Ts'], 'inferno', 'ts (degC)', close_fig=False)\n",
    "\n",
    "#             # Debris thickness\n",
    "#             var_full2plot = gf.debris_thick_ts.copy()\n",
    "#             clim = (0,1)\n",
    "#             plot_array(var_full2plot, clim, [gf.glacnum + ' hd (from ts)'], 'inferno', 'hd (m)', \n",
    "#                        fn=debris_prms.output_fig_fp + debris_prms.roi + '/' + gf.feat_fn +'_hd_ts.png', \n",
    "#                        close_fig=close_fig)\n",
    "\n",
    "#             # Melt factor\n",
    "#             titles = [gf.glacnum + ' melt factor']\n",
    "#             var_full2plot = gf.meltfactor_ts.copy()\n",
    "#             clim = (0,1.25)\n",
    "#             plot_array(var_full2plot, clim, titles, 'inferno', 'melt factor (-)', \n",
    "#                        fn=debris_prms.output_fig_fp + debris_prms.roi + '/' + gf.feat_fn +'_mf.png', \n",
    "#                        close_fig=True)   \n",
    "\n",
    "\n",
    "# #         # ===== Export debris thickness and melt factor maps ===== \n",
    "# #         hd_fp = debris_prms.hd_fp\n",
    "# #         if not os.path.exists(hd_fp):\n",
    "# #             os.makedirs(hd_fp)\n",
    "# #         gf.debris_thick_ts.mask = gf.dc_mask\n",
    "# #         debris_fullfn = hd_fp + debris_prms.hd_fn_sample.replace('XXXX',gf.glacnum)\n",
    "# #         iolib.writeGTiff(gf.debris_thick_ts, debris_fullfn, gf.ds_dict['z1'])\n",
    "\n",
    "# #         if add_meltfactor:\n",
    "# #             mf_fp = hd_fp + 'meltfactor/'\n",
    "# #             if not os.path.exists(mf_fp):\n",
    "# #                 os.makedirs(mf_fp)\n",
    "# #             gf.meltfactor_ts.mask = gf.dc_mask\n",
    "# #             meltfactor_fullfn = mf_fp + debris_prms.mf_fn_sample.replace('XXXX',gf.glacnum)\n",
    "# #             iolib.writeGTiff(gf.meltfactor_ts, meltfactor_fullfn, gf.ds_dict['z1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DONE\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\nDONE\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:debris_thickness_global]",
   "language": "python",
   "name": "conda-env-debris_thickness_global-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
