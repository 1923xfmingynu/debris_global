{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate a 3-panel plot for input arrays\n",
    "def plot_array(dem, clim=None, titles=None, cmap='inferno', label=None, overlay=None, fn=None, close_fig=True):\n",
    "    fig, ax = plt.subplots(1,1, sharex=True, sharey=True, figsize=(10,5))\n",
    "    alpha = 1.0\n",
    "    #Gray background\n",
    "    ax.set_facecolor('0.5')\n",
    "    #Force aspect ratio to match images\n",
    "    ax.set(aspect='equal')\n",
    "    #Turn off axes labels/ticks\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if titles is not None:\n",
    "        ax.set_title(titles[0])\n",
    "    #Plot background shaded relief map\n",
    "    if overlay is not None:\n",
    "        alpha = 0.7\n",
    "        ax.imshow(overlay, cmap='gray', clim=(1,255))\n",
    "    #Plot each array\n",
    "    im_list = [ax.imshow(dem, clim=clim, cmap=cmap, alpha=alpha)]\n",
    "    fig.tight_layout()\n",
    "    fig.colorbar(im_list[0], label=label, extend='both', shrink=0.5)\n",
    "    if fn is not None:\n",
    "        fig.savefig(fn, bbox_inches='tight', pad_inches=0, dpi=150)\n",
    "    if close_fig:\n",
    "        plt.close(fig)\n",
    "        \n",
    "def selectglaciersrgitable(glac_no=None,\n",
    "                           rgi_regionsO1=None,\n",
    "                           rgi_regionsO2=None,\n",
    "                           rgi_glac_number=None,\n",
    "#                            rgi_fp=input.rgi_fp,\n",
    "                           rgi_fp = '/Users/davidrounce/Documents/Dave_Rounce/HiMAT/RGI/rgi60/00_rgi60_attribs/',\n",
    "                           rgi_cols_drop=['GLIMSId','BgnDate','EndDate','Status','Connect','Linkages','Name'],\n",
    "                           rgi_O1Id_colname='glacno',\n",
    "                           rgi_glacno_float_colname='RGIId_float',\n",
    "                           indexname='GlacNo'):\n",
    "    \"\"\"\n",
    "    Select all glaciers to be used in the model run according to the regions and glacier numbers defined by the RGI\n",
    "    glacier inventory. This function returns the rgi table associated with all of these glaciers.\n",
    "\n",
    "    glac_no : list of strings\n",
    "        list of strings of RGI glacier numbers (e.g., ['1.00001', '13.00001'])\n",
    "    rgi_regionsO1 : list of integers\n",
    "        list of integers of RGI order 1 regions (e.g., [1, 13])\n",
    "    rgi_regionsO2 : list of integers or 'all'\n",
    "        list of integers of RGI order 2 regions or simply 'all' for all the order 2 regions\n",
    "    rgi_glac_number : list of strings\n",
    "        list of RGI glacier numbers without the region (e.g., ['00001', '00002'])\n",
    "\n",
    "    Output: Pandas DataFrame of the glacier statistics for each glacier in the model run\n",
    "    (rows = GlacNo, columns = glacier statistics)\n",
    "    \"\"\"\n",
    "    if glac_no is not None:\n",
    "        glac_no_byregion = {}\n",
    "        rgi_regionsO1 = [int(i.split('.')[0]) for i in glac_no]\n",
    "        rgi_regionsO1 = list(set(rgi_regionsO1))\n",
    "        for region in rgi_regionsO1:\n",
    "            glac_no_byregion[region] = []\n",
    "        for i in glac_no:\n",
    "            region = i.split('.')[0]\n",
    "            glac_no_only = i.split('.')[1]\n",
    "            glac_no_byregion[int(region)].append(glac_no_only)\n",
    "\n",
    "        for region in rgi_regionsO1:\n",
    "            glac_no_byregion[region] = sorted(glac_no_byregion[region])\n",
    "\n",
    "    # Create an empty dataframe\n",
    "    rgi_regionsO1 = sorted(rgi_regionsO1)\n",
    "    glacier_table = pd.DataFrame()\n",
    "    for region in rgi_regionsO1:\n",
    "\n",
    "        if glac_no is not None:\n",
    "            rgi_glac_number = glac_no_byregion[region]\n",
    "\n",
    "#        if len(rgi_glac_number) < 50:\n",
    "\n",
    "        for i in os.listdir(rgi_fp):\n",
    "            if i.startswith(str(region).zfill(2)) and i.endswith('.csv'):\n",
    "                rgi_fn = i\n",
    "        try:\n",
    "            csv_regionO1 = pd.read_csv(rgi_fp + rgi_fn)\n",
    "        except:\n",
    "            csv_regionO1 = pd.read_csv(rgi_fp + rgi_fn, encoding='latin1')\n",
    "        \n",
    "        # Populate glacer_table with the glaciers of interest\n",
    "        if rgi_regionsO2 == 'all' and rgi_glac_number == 'all':\n",
    "            print(\"All glaciers within region(s) %s are included in this model run.\" % (region))\n",
    "            if glacier_table.empty:\n",
    "                glacier_table = csv_regionO1\n",
    "            else:\n",
    "                glacier_table = pd.concat([glacier_table, csv_regionO1], axis=0)\n",
    "        elif rgi_regionsO2 != 'all' and rgi_glac_number == 'all':\n",
    "            print(\"All glaciers within subregion(s) %s in region %s are included in this model run.\" %\n",
    "                  (rgi_regionsO2, region))\n",
    "            for regionO2 in rgi_regionsO2:\n",
    "                if glacier_table.empty:\n",
    "                    glacier_table = csv_regionO1.loc[csv_regionO1['O2Region'] == regionO2]\n",
    "                else:\n",
    "                    glacier_table = (pd.concat([glacier_table, csv_regionO1.loc[csv_regionO1['O2Region'] ==\n",
    "                                                                                regionO2]], axis=0))\n",
    "        else:\n",
    "            if len(rgi_glac_number) < 20:\n",
    "                print(\"%s glaciers in region %s are included in this model run: %s\" % (len(rgi_glac_number), region,\n",
    "                                                                                       rgi_glac_number))\n",
    "            else:\n",
    "                print(\"%s glaciers in region %s are included in this model run: %s and more\" %\n",
    "                      (len(rgi_glac_number), region, rgi_glac_number[0:50]))\n",
    "                \n",
    "            rgiid_subset = ['RGI60-' + str(region).zfill(2) + '.' + x for x in rgi_glac_number] \n",
    "            rgiid_all = list(csv_regionO1.RGIId.values)\n",
    "            rgi_idx = [rgiid_all.index(x) for x in rgiid_subset]\n",
    "            if glacier_table.empty:\n",
    "                glacier_table = csv_regionO1.loc[rgi_idx]\n",
    "            else:\n",
    "                glacier_table = (pd.concat([glacier_table, csv_regionO1.loc[rgi_idx]],\n",
    "                                           axis=0))\n",
    "                    \n",
    "    glacier_table = glacier_table.copy()\n",
    "    # reset the index so that it is in sequential order (0, 1, 2, etc.)\n",
    "    glacier_table.reset_index(inplace=True)\n",
    "    # change old index to 'O1Index' to be easier to recall what it is\n",
    "    glacier_table.rename(columns={'index': 'O1Index'}, inplace=True)\n",
    "    # Record the reference date\n",
    "    glacier_table['RefDate'] = glacier_table['BgnDate']\n",
    "    # if there is an end date, then roughly average the year\n",
    "    enddate_idx = glacier_table.loc[(glacier_table['EndDate'] > 0), 'EndDate'].index.values\n",
    "    glacier_table.loc[enddate_idx,'RefDate'] = (\n",
    "            np.mean((glacier_table.loc[enddate_idx,['BgnDate', 'EndDate']].values / 10**4).astype(int),\n",
    "                    axis=1).astype(int) * 10**4 + 9999)\n",
    "    # drop columns of data that is not being used\n",
    "    glacier_table.drop(rgi_cols_drop, axis=1, inplace=True)\n",
    "    # add column with the O1 glacier numbers\n",
    "    glacier_table[rgi_O1Id_colname] = (\n",
    "            glacier_table['RGIId'].str.split('.').apply(pd.Series).loc[:,1].astype(int))\n",
    "    glacier_table['rgino_str'] = [x.split('-')[1] for x in glacier_table.RGIId.values]\n",
    "    glacier_table[rgi_glacno_float_colname] = (np.array([np.str.split(glacier_table['RGIId'][x],'-')[1]\n",
    "                                                    for x in range(glacier_table.shape[0])]).astype(float))\n",
    "    # set index name\n",
    "    glacier_table.index.name = indexname\n",
    "\n",
    "    print(\"This study is focusing on %s glaciers in region %s\" % (glacier_table.shape[0], rgi_regionsO1))\n",
    "\n",
    "    return glacier_table\n",
    "\n",
    "\n",
    "def nearest_nonzero_idx(a,x,y):\n",
    "    r,c = np.nonzero(a)\n",
    "    min_idx = ((r - x)**2 + (c - y)**2).argmin()\n",
    "    return r[min_idx], c[min_idx]\n",
    "\n",
    "\n",
    "def emergence_pixels(gf, vel_x_raw, vel_y_raw, icethickness_raw, xres, yres, \n",
    "                     vel_min=0, max_velocity=600, vel_depth_avg_factor=0.8, option_border=1,\n",
    "                     positive_is_east=True, positive_is_north=True, constant_icethickness=False, debug=True):\n",
    "    \"\"\" Compute the emergence velocity using an ice flux approach\n",
    "    \"\"\"\n",
    "    # Glacier mask\n",
    "    glac_mask = np.zeros(vel_x_raw.shape) + 1\n",
    "    glac_mask[gf.z1.mask] = 0\n",
    "    \n",
    "    # Modify vel_y by multiplying velocity by -1 such that matrix operations agree with flow direction\n",
    "    #    Specifically, a negative y velocity means the pixel is flowing south.\n",
    "    #    However, if you were to subtract that value from the rows, it would head north in the matrix.\n",
    "    #    This is due to the fact that the number of rows start at 0 at the top.\n",
    "    #    Therefore, multipylying by -1 aligns the matrix operations with the flow direction\n",
    "    if positive_is_north:\n",
    "        vel_y = -1*vel_y_raw * vel_depth_avg_factor\n",
    "    else:\n",
    "        vel_y = vel_y_raw * vel_depth_avg_factor\n",
    "    if positive_is_east:\n",
    "        vel_x = vel_x_raw * vel_depth_avg_factor\n",
    "    else:\n",
    "        vel_x = -1*vel_x_raw * vel_depth_avg_factor\n",
    "    vel_total = (vel_y**2 + vel_x**2)**0.5\n",
    "    # Ice thickness\n",
    "    icethickness = icethickness_raw.copy()\n",
    "    if constant_icethickness:\n",
    "        icethickness[:,:] = 1\n",
    "        icethickness = icethickness * glac_mask\n",
    "#     print('mean ice thickness:', np.round(icethickness.mean(),0), 'm')\n",
    "    # Compute the initial volume\n",
    "    volume_initial = icethickness * (xres * yres)\n",
    "    pix_maxres = xres\n",
    "    if yres > pix_maxres:\n",
    "        pix_maxres = yres\n",
    "    # Quality control options:\n",
    "    # Apply a border based on the max specified velocity to prevent errors associated with pixels going out of bounds\n",
    "    if option_border == 1:\n",
    "        border = int(max_velocity / pix_maxres) + 1\n",
    "        for r in range(vel_x.shape[0]):\n",
    "            for c in range(vel_x.shape[1]):\n",
    "                if (r < border) | (r >= vel_x.shape[0] - border) | (c < border) | (c >= vel_x.shape[1] - border):\n",
    "                    vel_x[r,c] = 0\n",
    "                    vel_y[r,c] = 0\n",
    "    # Minimum/maximum velocity bounds\n",
    "    vel_x[vel_total < vel_min] = 0\n",
    "    vel_y[vel_total < vel_min] = 0\n",
    "    vel_x[vel_total > max_velocity] = 0\n",
    "    vel_y[vel_total > max_velocity] = 0\n",
    "#     # Remove clusters of high velocity on stagnant portions of glaciers due to feature tracking of ice cliffs and ponds\n",
    "#     if option_stagnantbands == 1:\n",
    "#         vel_x[bands <= stagnant_band] = 0\n",
    "#         vel_y[bands <= stagnant_band] = 0        \n",
    "    # Compute displacement in units of pixels\n",
    "    vel_x_pix = vel_x / xres\n",
    "    vel_y_pix = vel_y / yres\n",
    "    # Compute the displacement and fraction of pixels moved for all columns (x-axis)\n",
    "    # col_x1 is the number of columns to the closest pixel receiving ice [ex. 2.6 returns 2, -2.6 returns -2]\n",
    "    #    int() automatically rounds towards zero\n",
    "    col_x1 = vel_x_pix.astype(int)\n",
    "    # col_x2 is the number of columns to the further pixel receiving ice [ex. 2.6 returns 3, -2.6 returns -3]\n",
    "    #    np.sign() returns a value of 1 or -1, so it's adding 1 pixel away from zero\n",
    "    col_x2 = (vel_x_pix + np.sign(vel_x_pix)).astype(int)\n",
    "    # rem_x2 is the fraction of the pixel that remains in the further pixel (col_x2) [ex. 2.6 returns 0.6, -2.6 returns 0.6]\n",
    "    #    np.sign() returns a value of 1 or -1, so multiplying by that ensures you have a positive value\n",
    "    #    then when you take the remainder using \"% 1\", you obtain the desired fraction\n",
    "    rem_x2 = np.multiply(np.sign(vel_x_pix), vel_x_pix) % 1\n",
    "    # rem_x1 is the fraction of the pixel that remains in the closer pixel (col_x1) [ex. 2.6 returns 0.4, -2.6 returns 0.4]\n",
    "    rem_x1 = 1 - rem_x2\n",
    "    # Repeat the displacement and fraction computations for all rows (y-axis)\n",
    "    row_y1 = vel_y_pix.astype(int)\n",
    "    row_y2 = (vel_y_pix + np.sign(vel_y_pix)).astype(int)\n",
    "    rem_y2 = np.multiply(np.sign(vel_y_pix), vel_y_pix) % 1\n",
    "    rem_y1 = 1 - rem_y2\n",
    "          \n",
    "    # Compute the mass flux for each pixel\n",
    "    volume_final = np.zeros(volume_initial.shape)\n",
    "    for r in range(vel_x.shape[0]):\n",
    "        for c in range(vel_x.shape[1]):\n",
    "            volume_final[r+row_y1[r,c], c+col_x1[r,c]] = (\n",
    "                volume_final[r+row_y1[r,c], c+col_x1[r,c]] + rem_y1[r,c]*rem_x1[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y2[r,c], c+col_x1[r,c]] = (\n",
    "                volume_final[r+row_y2[r,c], c+col_x1[r,c]] + rem_y2[r,c]*rem_x1[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y1[r,c], c+col_x2[r,c]] = (\n",
    "                volume_final[r+row_y1[r,c], c+col_x2[r,c]] + rem_y1[r,c]*rem_x2[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y2[r,c], c+col_x2[r,c]] = (\n",
    "                volume_final[r+row_y2[r,c], c+col_x2[r,c]] + rem_y2[r,c]*rem_x2[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "         \n",
    "    # Redistribute off-glacier volume back onto the nearest pixel on the glacier\n",
    "    offglac_row, offglac_col = np.where((glac_mask == 0) & (volume_final > 0))\n",
    "    for nidx in range(0,len(offglac_row)):\n",
    "        nrow = offglac_row[nidx]\n",
    "        ncol = offglac_col[nidx]\n",
    "        ridx, cidx = nearest_nonzero_idx(glac_mask, nrow, ncol)\n",
    "        # Add off-glacier volume back onto nearest pixel on glacier\n",
    "        volume_final[ridx,cidx] += volume_final[nrow,ncol]\n",
    "        volume_final[nrow,ncol] = 0\n",
    "            \n",
    "    # Check that mass is conserved (threshold = 0.1 m x pixel_size**2)\n",
    "    if debug:\n",
    "        print('Mass is conserved?', np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() < 0.01)\n",
    "        print(np.round(np.absolute(volume_final.sum() - volume_initial.sum()),1), \n",
    "              np.round(np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() * 100,2), '%')\n",
    "        \n",
    "    if np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() > 0.01:\n",
    "        print('MASS NOT CONSERVED FOR EMERGENCE VELOCITY')\n",
    "    # Final ice thickness\n",
    "    icethickness_final = volume_final / (xres * yres)\n",
    "    # Emergence velocity\n",
    "    emergence_velocity = icethickness_final - icethickness\n",
    "    return emergence_velocity\n",
    "\n",
    "\n",
    "\n",
    "class GlacFeat:\n",
    "    def __init__(self, feat, glacname_fieldname, glacnum_fieldname):\n",
    "\n",
    "        self.glacname = feat.GetField(glacname_fieldname)\n",
    "        if self.glacname is None:\n",
    "            self.glacname = \"\"\n",
    "        else:\n",
    "            #RGI has some nonstandard characters\n",
    "            #self.glacname = self.glacname.decode('unicode_escape').encode('ascii','ignore')\n",
    "            #glacname = re.sub(r'[^\\x00-\\x7f]',r'', glacname)\n",
    "            self.glacname = re.sub(r'\\W+', '', self.glacname)\n",
    "            self.glacname = self.glacname.replace(\" \", \"\")\n",
    "            self.glacname = self.glacname.replace(\"_\", \"\")\n",
    "            self.glacname = self.glacname.replace(\"/\", \"\")\n",
    "\n",
    "        self.glacnum = feat.GetField(glacnum_fieldname)\n",
    "        fn = feat.GetDefnRef().GetName()\n",
    "        #RGIId (String) = RGI50-01.00004\n",
    "        self.glacnum = '%0.5f' % float(self.glacnum.split('-')[-1])\n",
    "\n",
    "        if self.glacname:\n",
    "            self.feat_fn = \"%s_%s\" % (self.glacnum, self.glacname)\n",
    "        else:\n",
    "            self.feat_fn = str(self.glacnum)\n",
    "\n",
    "        self.glac_geom_orig = geolib.geom_dup(feat.GetGeometryRef())\n",
    "        self.glac_geom = geolib.geom_dup(self.glac_geom_orig)\n",
    "        #Hack to deal with fact that this is not preserved in geom when loaded from pickle on disk\n",
    "        self.glac_geom_srs_wkt = self.glac_geom.GetSpatialReference().ExportToWkt()\n",
    "\n",
    "        #Attributes written by mb_calc\n",
    "        self.z1 = None\n",
    "        self.z1_hs = None\n",
    "        self.z1_stats = None\n",
    "        self.z1_ela = None\n",
    "        self.z2 = None\n",
    "        self.z2_hs = None\n",
    "        self.z2_stats = None\n",
    "        self.z2_ela = None\n",
    "        self.z2_aspect = None\n",
    "        self.z2_aspect_stats = None\n",
    "        self.z2_slope = None\n",
    "        self.z2_slope_stats = None\n",
    "        self.res = None\n",
    "        self.dhdt = None\n",
    "        self.mb = None\n",
    "        self.mb_mean = None\n",
    "        self.t1 = None\n",
    "        self.t2 = None\n",
    "        self.dt = None\n",
    "        self.t1_mean = None\n",
    "        self.t2_mean = None\n",
    "        self.dt_mean = None\n",
    "\n",
    "        self.H = None\n",
    "        self.H_mean = np.nan\n",
    "        self.vx = None\n",
    "        self.vy = None\n",
    "        self.vm = None\n",
    "        self.vm_mean = np.nan\n",
    "        self.divQ = None\n",
    "        self.emvel = None\n",
    "        self.debris_class = None\n",
    "        self.debris_thick = None\n",
    "        self.debris_thick_mean = np.nan\n",
    "        self.perc_clean = np.nan\n",
    "        self.perc_debris = np.nan\n",
    "        self.perc_pond = np.nan\n",
    "\n",
    "    def geom_srs_update(self, srs=None):\n",
    "        if self.glac_geom.GetSpatialReference() is None:\n",
    "            if srs is None:\n",
    "                srs = osr.SpatialReference()\n",
    "                srs.ImportFromWkt(self.glac_geom_srs_wkt)\n",
    "            self.glac_geom.AssignSpatialReference(srs)\n",
    "\n",
    "    def geom_attributes(self, srs=None):\n",
    "        self.geom_srs_update()\n",
    "        if srs is not None:\n",
    "            #Should reproject here to equal area, before geom_attributes\n",
    "            #self.glac_geom.AssignSpatialReference(glac_shp_srs)\n",
    "            #self.glac_geom_local = geolib.geom2localortho(self.glac_geom)\n",
    "            geolib.geom_transform(self.glac_geom, srs)\n",
    "\n",
    "        self.glac_geom_extent = geolib.geom_extent(self.glac_geom)\n",
    "        self.glac_area = self.glac_geom.GetArea()\n",
    "        self.glac_area_km2 = self.glac_area / 1E6\n",
    "        self.cx, self.cy = self.glac_geom.Centroid().GetPoint_2D()\n",
    "        \n",
    "        \n",
    "#RGI uses 50 m bins\n",
    "def hist_plot(gf, bin_width=50.0, dz_clim=(-2.0, 2.0), exportcsv=True, csv_ending=''):\n",
    "    #print(\"Generating histograms\")\n",
    "    #Create bins for full range of input data and specified bin width\n",
    "\n",
    "    #NOTE: these counts/areas are for valid pixels only\n",
    "    #Not necessarily a true representation of actual glacier hypsometry\n",
    "    #Need a void-filled DEM for this\n",
    "\n",
    "    z_bin_edges, z_bin_centers = malib.get_bins(gf.z1, bin_width)\n",
    "    #Need to compress here, otherwise histogram uses masked values!\n",
    "    z1_bin_counts, z1_bin_edges = np.histogram(gf.z1.compressed(), bins=z_bin_edges)\n",
    "    z1_bin_areas = z1_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "    #RGI standard is integer thousandths of glaciers total area\n",
    "    #Should check to make sure sum of bin areas equals total area\n",
    "    #z1_bin_areas_perc = 100. * z1_bin_areas / np.sum(z1_bin_areas)\n",
    "    z1_bin_areas_perc = 100. * (z1_bin_areas / gf.glac_area_km2)\n",
    "\n",
    "    #If we only have one elevation grid with dhdt\n",
    "    if gf.z2 is not None:\n",
    "        z2_bin_counts, z2_bin_edges = np.histogram(gf.z2.compressed(), bins=z_bin_edges)\n",
    "        z2_bin_areas = z2_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "        #z2_bin_areas_perc = 100. * z2_bin_areas / np.sum(z2_bin_areas)\n",
    "        z2_bin_areas_perc = 100. * (z1_bin_areas / gf.glac_area_km2)\n",
    "    else:\n",
    "        z2_bin_counts = z1_bin_counts\n",
    "        z2_bin_edges = z1_bin_edges\n",
    "        z2_bin_areas = z1_bin_areas\n",
    "        z2_bin_areas_perc = z1_bin_areas_perc\n",
    "\n",
    "    #Create arrays to store output\n",
    "    slope_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "    slope_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    aspect_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "    aspect_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.dhdt is not None:\n",
    "        mb_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        np.ma.set_fill_value(mb_bin_med, np.nan)\n",
    "        mb_bin_mad = np.ma.masked_all_like(mb_bin_med)\n",
    "        mb_bin_mean = np.ma.masked_all_like(mb_bin_med)\n",
    "        mb_bin_std = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_med = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_mad = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_mean = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_std = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_count = np.ma.masked_all_like(mb_bin_med)\n",
    "    if gf.vm is not None:\n",
    "        vm_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        vm_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.H is not None:\n",
    "        H_bin_mean = np.ma.masked_all_like(z1_bin_areas)\n",
    "        H_bin_std = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.emvel is not None:\n",
    "        emvel_bin_mean = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_std = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.debris_class is not None:\n",
    "#         perc_clean = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         perc_debris = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         perc_pond = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_clean_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_debris_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_pond_bin_med = np.ma.masked_all_like(mz1_bin_areas)\n",
    "\n",
    "#         gf.dhdt_clean = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 1).data))\n",
    "#         gf.dhdt_debris = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 2).data))\n",
    "#         gf.dhdt_pond = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 3).data))\n",
    "\n",
    "    if gf.debris_thick_ts is not None:\n",
    "        debris_thick_ts_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_ts_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.meltfactor_ts is not None:\n",
    "        meltfactor_ts_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        meltfactor_ts_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "\n",
    "    #Bin sample count must be greater than this value\n",
    "    min_bin_samp_count = 9\n",
    "\n",
    "    #Loop through each bin and extract stats\n",
    "    idx = np.digitize(gf.z1, z_bin_edges)\n",
    "    for bin_n in range(z_bin_centers.size):\n",
    "        if gf.dhdt is not None:\n",
    "            mb_bin_samp = gf.mb_map[(idx == bin_n+1)]\n",
    "            if mb_bin_samp.count() > min_bin_samp_count:\n",
    "                mb_bin_med[bin_n] = malib.fast_median(mb_bin_samp)\n",
    "                mb_bin_mad[bin_n] = malib.mad(mb_bin_samp)\n",
    "                mb_bin_mean[bin_n] = mb_bin_samp.mean()\n",
    "                mb_bin_std[bin_n] = mb_bin_samp.std()\n",
    "            dhdt_bin_samp = gf.dhdt[(idx == bin_n+1)]\n",
    "            if dhdt_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_bin_med[bin_n] = malib.fast_median(dhdt_bin_samp)\n",
    "                dhdt_bin_mad[bin_n] = malib.mad(dhdt_bin_samp)\n",
    "                dhdt_bin_mean[bin_n] = dhdt_bin_samp.mean()\n",
    "                dhdt_bin_std[bin_n] = dhdt_bin_samp.std()\n",
    "                dhdt_bin_count[bin_n] = dhdt_bin_samp.count()\n",
    "        if gf.debris_thick is not None:\n",
    "            debris_thick_bin_samp = gf.debris_thick[(idx == bin_n+1)]\n",
    "            if debris_thick_bin_samp.size > min_bin_samp_count:\n",
    "                debris_thick_med[bin_n] = malib.fast_median(debris_thick_bin_samp)\n",
    "                debris_thick_mad[bin_n] = malib.mad(debris_thick_bin_samp)\n",
    "        \n",
    "        if gf.debris_thick_ts is not None:\n",
    "            debris_thick_ts_bin_samp = gf.debris_thick_ts[(idx == bin_n+1)]\n",
    "            if debris_thick_ts_bin_samp.size > min_bin_samp_count:\n",
    "                debris_thick_ts_med[bin_n] = malib.fast_median(debris_thick_ts_bin_samp)\n",
    "                debris_thick_ts_mad[bin_n] = malib.mad(debris_thick_ts_bin_samp)\n",
    "        if gf.meltfactor_ts is not None:\n",
    "            meltfactor_ts_bin_samp = gf.meltfactor_ts[(idx == bin_n+1)]\n",
    "            if meltfactor_ts_bin_samp.size > min_bin_samp_count:\n",
    "                meltfactor_ts_med[bin_n] = malib.fast_median(meltfactor_ts_bin_samp)\n",
    "                meltfactor_ts_mad[bin_n] = malib.mad(meltfactor_ts_bin_samp)\n",
    "                \n",
    "        if gf.debris_class is not None:\n",
    "            debris_class_bin_samp = gf.debris_class[(idx == bin_n+1)]\n",
    "            dhdt_clean_bin_samp = gf.dhdt_clean[(idx == bin_n+1)]\n",
    "            dhdt_debris_bin_samp = gf.dhdt_debris[(idx == bin_n+1)]\n",
    "            dhdt_pond_bin_samp = gf.dhdt_pond[(idx == bin_n+1)]\n",
    "            if debris_class_bin_samp.count() > min_bin_samp_count:\n",
    "                perc_clean[bin_n] = 100. * (debris_class_bin_samp == 1).sum()/debris_class_bin_samp.count()\n",
    "                perc_debris[bin_n] = 100. * (debris_class_bin_samp == 2).sum()/debris_class_bin_samp.count()\n",
    "                perc_pond[bin_n] = 100. * (debris_class_bin_samp == 3).sum()/debris_class_bin_samp.count()\n",
    "            if dhdt_clean_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_clean_bin_med[bin_n] = malib.fast_median(dhdt_clean_bin_samp)\n",
    "            if dhdt_debris_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_debris_bin_med[bin_n] = malib.fast_median(dhdt_debris_bin_samp)\n",
    "            if dhdt_pond_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_pond_bin_med[bin_n] = malib.fast_median(dhdt_pond_bin_samp)\n",
    "        if gf.vm is not None:\n",
    "            vm_bin_samp = gf.vm[(idx == bin_n+1)]\n",
    "            if vm_bin_samp.size > min_bin_samp_count:\n",
    "                vm_bin_med[bin_n] = malib.fast_median(vm_bin_samp)\n",
    "                vm_bin_mad[bin_n] = malib.mad(vm_bin_samp)\n",
    "        if gf.H is not None:\n",
    "            H_bin_samp = gf.H[(idx == bin_n+1)]\n",
    "            if H_bin_samp.size > min_bin_samp_count:\n",
    "                H_bin_mean[bin_n] = H_bin_samp.mean()\n",
    "                H_bin_std[bin_n] = H_bin_samp.std()\n",
    "        if gf.emvel is not None:\n",
    "            emvel_bin_samp = gf.emvel[(idx == bin_n+1)]\n",
    "            if emvel_bin_samp.size > min_bin_samp_count:\n",
    "                emvel_bin_mean[bin_n] = emvel_bin_samp.mean()\n",
    "                emvel_bin_std[bin_n] = emvel_bin_samp.std()\n",
    "                emvel_bin_med[bin_n] = malib.fast_median(emvel_bin_samp)\n",
    "                emvel_bin_mad[bin_n] = malib.mad(emvel_bin_samp)\n",
    "        slope_bin_samp = gf.z1_slope[(idx == bin_n+1)]\n",
    "        if slope_bin_samp.size > min_bin_samp_count:\n",
    "            slope_bin_med[bin_n] = malib.fast_median(slope_bin_samp)\n",
    "            slope_bin_mad[bin_n] = malib.mad(slope_bin_samp)\n",
    "        aspect_bin_samp = gf.z1_aspect[(idx == bin_n+1)]\n",
    "        if aspect_bin_samp.size > min_bin_samp_count:\n",
    "            aspect_bin_med[bin_n] = malib.fast_median(aspect_bin_samp)\n",
    "            aspect_bin_mad[bin_n] = malib.mad(aspect_bin_samp)\n",
    "\n",
    "    if gf.dhdt is not None:\n",
    "        dhdt_bin_areas = dhdt_bin_count * gf.res[0] * gf.res[1] / 1E6\n",
    "        #dhdt_bin_areas_perc = 100. * dhdt_bin_areas / np.sum(dhdt_bin_areas)\n",
    "        dhdt_bin_areas_perc = 100. * (dhdt_bin_areas / gf.glac_area_km2)\n",
    "\n",
    "    outbins_header = 'bin_center_elev_m, z1_bin_count_valid, z1_bin_area_valid_km2, z1_bin_area_perc, z2_bin_count_valid, z2_bin_area_valid_km2, z2_bin_area_perc, slope_bin_med, aspect_bin_med'\n",
    "    fmt = '%0.1f, %0.0f, %0.3f, %0.2f, %0.0f, %0.3f, %0.2f, %0.2f, %0.2f'\n",
    "    outbins = [z_bin_centers, z1_bin_counts, z1_bin_areas, z1_bin_areas_perc, z2_bin_counts, z2_bin_areas, z2_bin_areas_perc, slope_bin_med, aspect_bin_med]\n",
    "    if gf.dhdt is not None:\n",
    "        outbins_header += ', dhdt_bin_count, dhdt_bin_area_valid_km2, dhdt_bin_area_perc, dhdt_bin_med_ma, dhdt_bin_mad_ma, dhdt_bin_mean_ma, dhdt_bin_std_ma, mb_bin_med_mwea, mb_bin_mad_mwea, mb_bin_mean_mwea, mb_bin_std_mwea'\n",
    "        fmt += ', %0.0f, %0.3f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "        outbins.extend([dhdt_bin_count, dhdt_bin_areas, dhdt_bin_areas_perc, dhdt_bin_med, dhdt_bin_mad, dhdt_bin_mean, dhdt_bin_std, \\\n",
    "                        mb_bin_med, mb_bin_mad, mb_bin_mean, mb_bin_std])\n",
    "    if gf.debris_thick is not None:\n",
    "        outbins_header += ', debris_thick_med_m, debris_thick_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        debris_thick_med[debris_thick_med == -(np.inf)] = 0.00\n",
    "        debris_thick_mad[debris_thick_mad == -(np.inf)] = 0.00\n",
    "        outbins.extend([debris_thick_med, debris_thick_mad])\n",
    "    \n",
    "    if gf.debris_thick_ts is not None:\n",
    "        outbins_header += ',debris_thick_ts_med_m,debris_thick_ts_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        debris_thick_ts_med[debris_thick_ts_med == -(np.inf)] = 0.00\n",
    "        debris_thick_ts_mad[debris_thick_ts_mad == -(np.inf)] = 0.00\n",
    "        outbins.extend([debris_thick_ts_med, debris_thick_ts_mad])\n",
    "    if gf.meltfactor_ts is not None:\n",
    "        outbins_header += ',meltfactor_ts_med_m,meltfactor_ts_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        meltfactor_ts_med[meltfactor_ts_med == -(np.inf)] = 1\n",
    "        meltfactor_ts_med[meltfactor_ts_med > 1] = 1\n",
    "        meltfactor_ts_med[meltfactor_ts_med <= 0] = 1\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad == -(np.inf)] = 0\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad > 1] = 0\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad <= 0] = 0\n",
    "        outbins.extend([meltfactor_ts_med, meltfactor_ts_mad])\n",
    "    \n",
    "    if gf.debris_class is not None:\n",
    "        outbins_header += ', perc_debris, perc_pond, perc_clean, dhdt_debris_med, dhdt_pond_med, dhdt_clean_med'\n",
    "        fmt += ', %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "        outbins.extend([perc_debris, perc_pond, perc_clean, dhdt_debris_bin_med, dhdt_pond_bin_med, dhdt_clean_bin_med])\n",
    "    if gf.vm is not None:\n",
    "        outbins_header += ', vm_med, vm_mad'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        outbins.extend([vm_bin_med, vm_bin_mad])\n",
    "    if gf.H is not None:\n",
    "        outbins_header += ', H_mean, H_std'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        outbins.extend([H_bin_mean, H_bin_std])\n",
    "#         outbins_header += ', H_mean, H_std, emvel_mean, emvel_std'\n",
    "#         fmt += ', %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "#         outbins.extend([H_bin_mean, H_bin_std, emvel_bin_mean, emvel_bin_std])\n",
    "\n",
    "    if gf.emvel is not None:\n",
    "        outbins_header += ', emvel_mean, emvel_std, emvel_med, emvel_mad'\n",
    "        fmt += ', %0.3f, %0.3f, %0.3f, %0.3f'\n",
    "        outbins.extend([emvel_bin_mean, emvel_bin_std, emvel_bin_med, emvel_bin_mad])\n",
    "\n",
    "    outbins = np.ma.array(outbins).T.astype('float32')\n",
    "    np.ma.set_fill_value(outbins, np.nan)\n",
    "    outbins = outbins.filled(np.nan)\n",
    "    if exportcsv:\n",
    "        outbins_fn = os.path.join(outdir_csv, gf.feat_fn[0:8] + csv_ending)\n",
    "        np.savetxt(outbins_fn, outbins, fmt=fmt, delimiter=',', header=outbins_header)\n",
    "\n",
    "#     #Create plots of elevation bins\n",
    "#     #print(\"Generating aed plot\")\n",
    "#     #f,axa = plt.subplots(1,2, figsize=(6, 6))\n",
    "#     nsubplots = 0\n",
    "#     if gf.dhdt is not None:\n",
    "#         nsubplots += 1\n",
    "#     if gf.debris_thick is not None:\n",
    "#         nsubplots += 1\n",
    "#     if gf.vm is not None:\n",
    "#         nsubplots += 1\n",
    "#     if gf.H is not None:\n",
    "#         nsubplots += 1\n",
    "# #     print(nsubplots)\n",
    "#     f,axa = plt.subplots(1,nsubplots, squeeze=False, figsize=(10, 7.5))\n",
    "#     f.suptitle(gf.feat_fn)\n",
    "#     fs = 9\n",
    "#     nplot = -1\n",
    "#     if gf.dhdt is not None:\n",
    "#         nplot += 1\n",
    "#         axa[0,nplot].plot(z1_bin_areas, z_bin_centers, label='%0.2f' % gf.t1_mean)\n",
    "#         axa[0,nplot].axhline(gf.z1_ela, ls=':', c='C0')\n",
    "#         if gf.z2 is not None:\n",
    "#             axa[0,nplot].plot(z2_bin_areas, z_bin_centers, label='%0.2f' % gf.t2_mean)\n",
    "#             axa[0,nplot].axhline(gf.z2_ela, ls=':', c='C1')\n",
    "#         axa[0,nplot].legend(prop={'size':8}, loc='upper right')\n",
    "#         axa[0,nplot].set_ylabel('Elevation (m WGS84)', fontsize=fs)\n",
    "#         axa[0,nplot].set_xlabel('Area $\\mathregular{km^2}$', fontsize=fs)\n",
    "#         axa[0,nplot].yaxis.set_ticks_position('both')\n",
    "#         # pltlib.minorticks_on(axa[0])\n",
    "\n",
    "#         nplot += 1\n",
    "#         axa[0,nplot].axvline(0, lw=1.0, c='k')\n",
    "#         \"\"\"\n",
    "#         #Plot flux divergence values for each bin\n",
    "#         if gf.vm is not None and gf.H is not None:\n",
    "#             divQ_bin_mean = np.gradient(H_bin_mean * vm_bin_med * v_col_f)\n",
    "#             axa[1].plot(divQ_bin_mean, z_bin_centers, color='green')\n",
    "#         \"\"\"\n",
    "#         axa[0,nplot].plot(mb_bin_med, z_bin_centers, color='k')\n",
    "#         axa[0,nplot].axvline(gf.mb_mean, lw=0.5, ls=':', c='k', label='%0.2f m w.e./yr' % gf.mb_mean)\n",
    "#         axa[0,nplot].fill_betweenx(z_bin_centers, mb_bin_med-mb_bin_mad, mb_bin_med+mb_bin_mad, color='k', alpha=0.1)\n",
    "#         axa[0,nplot].fill_betweenx(z_bin_centers, 0, mb_bin_med, where=(mb_bin_med<0), color='r', alpha=0.2)\n",
    "#         axa[0,nplot].fill_betweenx(z_bin_centers, 0, mb_bin_med, where=(mb_bin_med>0), color='b', alpha=0.2)\n",
    "#         #axa[nplot].set_xlabel('dh/dt (m/yr)')\n",
    "#         axa[0,nplot].set_xlabel('Mass balance (m w.e./yr)', fontsize=fs)\n",
    "#         axa[0,nplot].legend(prop={'size':8}, loc='upper right')\n",
    "#         axa[0,nplot].yaxis.set_ticks_position('both')\n",
    "#         # pltlib.minorticks_on(axa[1])\n",
    "#         #Hide y-axis labels\n",
    "#         axa[0,nplot].axes.yaxis.set_ticklabels([])\n",
    "#         axa[0,nplot].set_xlim(*dz_clim)\n",
    "\n",
    "#     if gf.debris_thick is not None:\n",
    "#         nplot += 1\n",
    "#         axa[0,nplot].errorbar(debris_thick_med*100., z_bin_centers, xerr=debris_thick_mad*100, color='k', fmt='o', ms=3, label='Debris Thickness', alpha=0.6)\n",
    "#     if gf.debris_class is not None:\n",
    "#         axa[0,nplot].plot(perc_debris, z_bin_centers, color='sienna', label='Debris Coverage')\n",
    "#         axa[0,nplot].plot(perc_pond, z_bin_centers, color='turquoise', label='Pond Coverage')\n",
    "#     if gf.debris_thick is not None or gf.debris_class is not None:\n",
    "#         axa[0,nplot].set_xlim(0, 100)\n",
    "#         axa[0,nplot].yaxis.set_ticks_position('both')\n",
    "#         # pltlib.minorticks_on(axa[2])\n",
    "#         axa[0,nplot].axes.yaxis.set_ticklabels([])\n",
    "#         axa[0,nplot].legend(prop={'size':8}, loc='upper right')\n",
    "#         axa[0,nplot].set_xlabel('Debris thickness (cm), coverage (%)', fontsize=fs)\n",
    "\n",
    "#     if gf.H is not None:\n",
    "#         nplot += 1\n",
    "#         axa[0,nplot].plot(H_bin_mean, z_bin_centers, color='b', label='H (%0.2f m)' % gf.H_mean)\n",
    "#         axa[0,nplot].fill_betweenx(z_bin_centers, H_bin_mean-H_bin_std, H_bin_mean+H_bin_std, color='b', alpha=0.1)\n",
    "#         axa[0,nplot].set_xlabel('Ice Thickness (m)', fontsize=fs)\n",
    "#         axa[0,nplot].legend(prop={'size':8}, loc='lower right')\n",
    "#         # pltlib.minorticks_on(axa[3])\n",
    "#         #axa[nplot].set_xlim(0, 400)\n",
    "#         axa[0,nplot].yaxis.tick_left()\n",
    "#         axa[0,nplot].yaxis.set_ticks_position('both')\n",
    "#         axa[0,nplot].yaxis.set_label_position(\"right\")\n",
    "    \n",
    "#     if gf.vm is not None:\n",
    "#         nplot += 1\n",
    "# #         ax4 = axa[0,nplot].twinx()\n",
    "#         axa[0,nplot].set_xlabel('Velocity (m/yr)', fontsize=fs)\n",
    "#         axa[0,nplot].plot(vm_bin_med, z_bin_centers, color='g', label='Vm (%0.2f m/yr)' % gf.vm_mean)\n",
    "#         axa[0,nplot].fill_betweenx(z_bin_centers, vm_bin_med-vm_bin_mad, vm_bin_med+vm_bin_mad, color='g', alpha=0.1)\n",
    "#         #ax4.set_xlim(0, 50)\n",
    "#         axa[0,nplot].xaxis.tick_bottom()\n",
    "#         axa[0,nplot].xaxis.set_label_position(\"bottom\")\n",
    "#         axa[0,nplot].legend(prop={'size':8}, loc='upper right')\n",
    "        \n",
    "#         nplot += 1\n",
    "# #         axa[0,nplot].set_xlabel('divQ (??)', fontsize=fs)\n",
    "# #         axa[0,nplot].plot(vm_bin_med, z_bin_centers, color='g', label='Vm (%0.2f m/yr)' % gf.vm_mean)\n",
    "# #         axa[0,nplot].fill_betweenx(z_bin_centers, vm_bin_med-vm_bin_mad, vm_bin_med+vm_bin_mad, color='g', alpha=0.1)\n",
    "# #         #ax4.set_xlim(0, 50)\n",
    "# #         axa[0,nplot].xaxis.tick_bottom()\n",
    "# #         axa[0,nplot].xaxis.set_label_position(\"bottom\")\n",
    "# #         axa[0,nplot].legend(prop={'size':8}, loc='upper right')\n",
    "# #         gf.divQ\n",
    "    \n",
    "# #     if gf.vm is not None:\n",
    "# #         nplot += 1\n",
    "# # #         ax4 = axa[0,nplot].twinx()\n",
    "# #         axa[0,nplot].set_xlabel('Velocity (m/yr)', fontsize=fs)\n",
    "# #         axa[0,nplot].plot(vm_bin_med, z_bin_centers, color='g', label='Vm (%0.2f m/yr)' % gf.vm_mean)\n",
    "# #         axa[0,nplot].fill_betweenx(z_bin_centers, vm_bin_med-vm_bin_mad, vm_bin_med+vm_bin_mad, color='g', alpha=0.1)\n",
    "# #         #ax4.set_xlim(0, 50)\n",
    "# #         axa[0,nplot].xaxis.tick_bottom()\n",
    "# #         axa[0,nplot].xaxis.set_label_position(\"bottom\")\n",
    "# #         axa[0,nplot].legend(prop={'size':8}, loc='upper right')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     #Make room for suptitle\n",
    "#     plt.subplots_adjust(top=0.95, wspace=0.1)\n",
    "#     #print(\"Saving aed plot\")\n",
    "#     fig_fn = os.path.join(outdir_fig, gf.feat_fn+'_mb_aed.png')\n",
    "#     #plt.savefig(fig_fn, bbox_inches='tight', dpi=300)\n",
    "#     plt.savefig(fig_fn, dpi=300)\n",
    "#     plt.close(f)\n",
    "    \n",
    "    \n",
    "    outbins_df = pd.DataFrame(outbins, columns=outbins_header.split(','))\n",
    "    return outbins_df, z_bin_edges\n",
    "#     return z_bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\"\"\"\n",
    "Compute debris thickness through sub-debris and temperature inversion methods\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "# import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import xarray as xr\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "from pygeotools.lib import malib, warplib, geolib, iolib, timelib\n",
    "# from imview.lib import pltlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/Documents/Dave_Rounce/DebrisGlaciers_WG/Melt_Intercomparison/debris_global/../output/ts_tif/HMA_debris_tsinfo.nc\n"
     ]
    }
   ],
   "source": [
    "import globaldebris_input as input\n",
    "\n",
    "#INPUT\n",
    "# topdir='/Users/davidrounce/Documents/Dave_Rounce/HiMAT/DEMs/'\n",
    "# #Output directory\n",
    "# outdir = topdir + 'Shean_2019_0213/mb_combined_20190213_nmad_bins/'\n",
    "# outdir_fig = outdir + '/figures/'\n",
    "# outdir_csv = outdir + '/csv'\n",
    "\n",
    "verbose=False\n",
    "extra_layers=True\n",
    "min_glac_area_writeout=0\n",
    "min_valid_area_perc = 0\n",
    "buff_dist = 1000\n",
    "bin_width = 5\n",
    "\n",
    "ts_info_fullfn = input.ts_fp + input.roi + '_debris_tsinfo.nc'\n",
    "\n",
    "print(ts_info_fullfn)\n",
    "\n",
    "#INPUT\n",
    "glac_shp_fn_dict = {'13':input.main_directory + '/../../../HiMAT/RGI/rgi60/13_rgi60_CentralAsia/13_rgi60_CentralAsia.shp',\n",
    "                    '14':input.main_directory + '/../../../HiMAT/RGI/rgi60/14_rgi60_SouthAsiaWest/14_rgi60_SouthAsiaWest.shp',\n",
    "                    '15':input.main_directory + '/../../../HiMAT/RGI/rgi60/15_rgi60_SouthAsiaEast/15_rgi60_SouthAsiaEast.shp'}\n",
    "glac_shp_proj_fp = input.output_fp + 'glac_shp_proj/'\n",
    "if os.path.exists(glac_shp_proj_fp) == False:\n",
    "    os.makedirs(glac_shp_proj_fp)\n",
    "\n",
    "#DEM\n",
    "z1_dir_sample = ('/Users/davidrounce/Documents/Dave_Rounce/HiMAT/IceThickness_Farinotti/surface_DEMs_RGI60/' + \n",
    "          'surface_DEMs_RGI60-XXXX/')\n",
    "z1_fn_sample = 'surface_DEM_RGI60-XXXX.tif'\n",
    "# Ice thickness\n",
    "huss_dir_sample = ('/Users/davidrounce/Documents/Dave_Rounce/HiMAT/IceThickness_Farinotti/' + \n",
    "                   'composite_thickness_RGI60-all_regions/RGI60-XXXX/')\n",
    "huss_fn_sample = 'RGI60-XXXX_thickness.tif'\n",
    "\n",
    "if os.path.exists(input.ts_fp) == False:\n",
    "    os.makedirs(input.ts_fp)\n",
    "    \n",
    "outdir_csv = input.outdir_emvel_fp \n",
    "outdir_fig = input.outdir_emvel_fp  + '../figures/'\n",
    "\n",
    "if os.path.exists(glac_shp_proj_fp) == False:\n",
    "    os.makedirs(glac_shp_proj_fp)\n",
    "if os.path.exists(outdir_csv) == False:\n",
    "    os.makedirs(outdir_csv)\n",
    "if os.path.exists(outdir_fig) == False:\n",
    "    os.makedirs(outdir_fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7934\n",
      "4066 glaciers in region 13 are included in this model run: ['00093', '00130', '00135', '00137', '00140', '00147', '00175', '00181', '00183', '00203', '00210', '00277', '00358', '00382', '00386', '00391', '00394', '00400', '00401', '00403', '00439', '00440', '00441', '00465', '00561', '00585', '00594', '00604', '00606', '00611', '00628', '00643', '00693', '00713', '00750', '00751', '00757', '00761', '00763', '00777', '00788', '00809', '00830', '00834', '00838', '00880', '00884', '00885', '00891', '00905'] and more\n",
      "2465 glaciers in region 14 are included in this model run: ['00005', '00018', '00032', '00036', '00043', '00057', '00063', '00072', '00088', '00101', '00104', '00111', '00131', '00142', '00145', '00146', '00159', '00163', '00164', '00187', '00213', '00219', '00222', '00225', '00235', '00243', '00251', '00271', '00287', '00309', '00323', '00326', '00336', '00346', '00347', '00352', '00353', '00363', '00366', '00367', '00370', '00372', '00380', '00398', '00403', '00432', '00449', '00453', '00456', '00466'] and more\n",
      "1403 glaciers in region 15 are included in this model run: ['00024', '00026', '00055', '00057', '00107', '00186', '00194', '00232', '00233', '00234', '00288', '00355', '00356', '00358', '00368', '00372', '00379', '00399', '00406', '00410', '00423', '00475', '00503', '00612', '00617', '00621', '00639', '00642', '00643', '00647', '00648', '00655', '00679', '00726', '00835', '00850', '00855', '00868', '00869', '00872', '00880', '00881', '00885', '00894', '00898', '00899', '00909', '00910', '00911', '00920'] and more\n",
      "This study is focusing on 7934 glaciers in region [13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "rgiid_list = []\n",
    "rgiid_fn_list = []\n",
    "for i in os.listdir(input.mb_binned_fp_wdebris):\n",
    "    if i.endswith('mb_bins_wdebris.csv'):\n",
    "        rgiid_list.append(i[0:8])\n",
    "        rgiid_fn_list.append(i)\n",
    "        \n",
    "rgiid_list = sorted(rgiid_list)\n",
    "rgiid_fn_list = sorted(rgiid_fn_list)\n",
    "\n",
    "print(len(rgiid_list))\n",
    "\n",
    "main_glac_rgi = selectglaciersrgitable(rgiid_list)\n",
    "main_glac_rgi['bin_fn'] = rgiid_fn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group datasets by nearest lat/lon\n",
    "ds = xr.open_dataset(input.debris_elevstats_fullfn)\n",
    "#  argmin() finds the minimum distance between the glacier lat/lon and the GCM pixel\n",
    "lat_nearidx = (np.abs(main_glac_rgi['CenLat'].values[:,np.newaxis] - \n",
    "                      ds['latitude'][:].values).argmin(axis=1))\n",
    "lon_nearidx = (np.abs(main_glac_rgi['CenLon'].values[:,np.newaxis] - \n",
    "                      ds['longitude'][:].values).argmin(axis=1))\n",
    "\n",
    "latlon_nearidx = list(zip(lat_nearidx, lon_nearidx))\n",
    "latlon_nearidx_unique = sorted(list(set(latlon_nearidx)))\n",
    "\n",
    "main_glac_rgi['latlon_nearidx'] = latlon_nearidx\n",
    "latlon_unique_dict = dict(zip(latlon_nearidx_unique,np.arange(0,len(latlon_nearidx_unique))))\n",
    "latlon_unique_dict_reversed = dict(zip(np.arange(0,len(latlon_nearidx_unique)),latlon_nearidx_unique))\n",
    "main_glac_rgi['latlon_unique_no'] = main_glac_rgi['latlon_nearidx'].map(latlon_unique_dict)\n",
    "\n",
    "ds_latlon = ds.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0 45.0 79.25\n",
      "\n",
      "\n",
      "HACK TO BYPASS VALID AREA\n",
      "\n",
      "\n",
      "  year mean +/- std: 2015.0 0.0\n",
      "  doy mean +/- std: 216.2 1.4\n",
      "    doy median: 216.2\n",
      "  dayfrac mean +/- std: 5.441 0.004\n",
      "\n",
      " 1 45.0 79.5\n",
      "  year mean +/- std: 2015.0 0.1\n",
      "  doy mean +/- std: 216.0 2.4\n",
      "    doy median: 216.2\n",
      "  dayfrac mean +/- std: 5.441 0.0\n",
      "\n",
      " 2 45.0 79.75\n",
      "  year mean +/- std: 2015.0 0.0\n",
      "  doy mean +/- std: 216.2 0.8\n",
      "    doy median: 216.2\n",
      "  dayfrac mean +/- std: 5.441 0.0\n",
      "\n",
      " 3 45.0 80.0\n",
      "  year mean +/- std: 2015.0 0.0\n",
      "  doy mean +/- std: 197.2 8.3\n",
      "    doy median: 193.2\n",
      "  dayfrac mean +/- std: 5.354 0.037\n",
      "\n",
      " 4 45.0 80.25\n",
      "  year mean +/- std: 2015.0 0.1\n",
      "  doy mean +/- std: 193.9 5.2\n",
      "    doy median: 193.2\n",
      "  dayfrac mean +/- std: 5.338 0.01\n",
      "\n",
      " 5 45.0 80.5\n",
      "  year mean +/- std: 2015.0 0.0\n",
      "  doy mean +/- std: 193.2 0.0\n",
      "    doy median: 193.2\n",
      "  dayfrac mean +/- std: 5.336 0.0\n",
      "\n",
      " 6 44.75 79.75\n",
      "  year mean +/- std: 2014.8 0.4\n",
      "  doy mean +/- std: 215.5 19.1\n",
      "    doy median: 217.4\n",
      "  dayfrac mean +/- std: 5.443 0.002\n",
      "\n",
      " 7 44.75 80.0\n",
      "  year mean +/- std: 2015.0 0.1\n",
      "  doy mean +/- std: 204.9 12.1\n",
      "    doy median: 202.4\n",
      "  dayfrac mean +/- std: 5.385 0.05\n",
      "\n",
      " 8 44.75 80.5\n",
      "  year mean +/- std: 2015.0 0.0\n",
      "  doy mean +/- std: 193.3 1.3\n",
      "    doy median: 193.2\n",
      "  dayfrac mean +/- std: 5.336 0.0\n",
      "\n",
      " 9 44.75 80.75\n",
      "  year mean +/- std: 2015.0 0.1\n",
      "  doy mean +/- std: 194.2 6.6\n",
      "    doy median: 193.2\n",
      "  dayfrac mean +/- std: 5.337 0.006\n",
      "\n",
      " 10 44.5 80.25\n",
      "  year mean +/- std: 2015.0 0.1\n",
      "  doy mean +/- std: 194.5 6.5\n",
      "    doy median: 193.2\n",
      "  dayfrac mean +/- std: 5.338 0.012\n",
      "\n",
      " 11 44.5 80.5\n",
      "  year mean +/- std: 2015.0 0.0\n",
      "  doy mean +/- std: 193.2 0.0\n",
      "    doy median: 193.2\n",
      "  dayfrac mean +/- std: 5.336 0.0\n",
      "\n",
      " 12 44.25 83.25\n",
      "  year mean +/- std: 2015.0 0.0\n",
      "  doy mean +/- std: 196.3 2.3\n",
      "    doy median: 195.2\n",
      "  dayfrac mean +/- std: 5.147 0.035\n",
      "\n",
      " 13 44.25 83.5\n",
      "  year mean +/- std: 2015.0 0.1\n",
      "  doy mean +/- std: 196.1 3.0\n",
      "    doy median: 195.2\n",
      "  dayfrac mean +/- std: 5.139 0.025\n",
      "\n",
      " 14 44.0 83.25\n",
      "  year mean +/- std: 2015.0 0.1\n",
      "  doy mean +/- std: 195.3 1.6\n",
      "    doy median: 195.2\n",
      "  dayfrac mean +/- std: 5.134 0.005\n",
      "\n",
      " 15 44.0 83.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-90de2dae7328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglac_shp_lyr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mgf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlacFeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglacname_fieldname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglacnum_fieldname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%i of %i: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-601112a8896f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, feat, glacname_fieldname, glacnum_fieldname)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglacnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglac_geom_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeolib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeom_dup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetGeometryRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglac_geom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeolib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeom_dup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglac_geom_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;31m#Hack to deal with fact that this is not preserved in geom when loaded from pickle on disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/pygeotools/lib/geolib.py\u001b[0m in \u001b[0;36mgeom_dup\u001b[0;34m(geom)\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0mNeeded\u001b[0m \u001b[0mto\u001b[0m \u001b[0mavoid\u001b[0m \u001b[0msegfault\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mpassing\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0maround\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mSee\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mtrac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mosgeo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mgdal\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mwiki\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mPythonGotchas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m     \"\"\"\n\u001b[0;32m--> 637\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mogr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateGeometryFromWkt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExportToWkt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m     \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAssignSpatialReference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetSpatialReference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/debris_thickness_global/lib/python3.6/site-packages/osgeo/ogr.py\u001b[0m in \u001b[0;36mExportToWkt\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   6089\u001b[0m         \u001b[0mCurrently\u001b[0m \u001b[0mOGRERR_NONE\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0malways\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6090\u001b[0m         \"\"\"\n\u001b[0;32m-> 6091\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ogr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeometry_ExportToWkt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process each group and derive elevation statistics for the debris cover\n",
    "# ts_year_med = np.zeros((len(ds.latitude.values), len(ds.longitude.values)))\n",
    "# ts_doy_med = np.zeros((len(ds.latitude.values), len(ds.longitude.values)))\n",
    "# ts_hr_med = np.zeros((len(ds.latitude.values), len(ds.longitude.values)))\n",
    "\n",
    "# for nlatlon, latlon_idx in enumerate([1095]):\n",
    "for nlatlon, latlon_idx in enumerate(list(np.arange(0,len(latlon_nearidx_unique)))):\n",
    "        \n",
    "    main_glac_rgi_subset = main_glac_rgi[main_glac_rgi['latlon_unique_no'] == latlon_idx]\n",
    "    \n",
    "    lat_idx, lon_idx = latlon_unique_dict_reversed[latlon_idx]\n",
    "    \n",
    "    print('\\n', latlon_idx, ds_latlon['latitude'][lat_idx].values, ds_latlon['longitude'][lon_idx].values)\n",
    "\n",
    "    df_all = None\n",
    "    \n",
    "    doy_list = []\n",
    "    year_list = []\n",
    "    dayfrac_list = []\n",
    "        \n",
    "        # =====\n",
    "    for nglac, glac_idx in enumerate(main_glac_rgi_subset.index.values):\n",
    "#     for nglac, glac_idx in enumerate([main_glac_rgi_subset.index.values[1]]):\n",
    "        glac_str = main_glac_rgi_subset.loc[glac_idx,'rgino_str']\n",
    "        rgiid = main_glac_rgi_subset.loc[glac_idx,'RGIId']\n",
    "        region = glac_str.split('.')[0]\n",
    "\n",
    "        if verbose:\n",
    "            print(nglac, glac_idx, rgiid,'\\n')\n",
    "        \n",
    "        df = pd.read_csv(input.mb_binned_fp_wdebris + main_glac_rgi_subset.loc[glac_idx,'bin_fn'])\n",
    "\n",
    "\n",
    "        # Process only glaciers with debris\n",
    "        debris_switch=False\n",
    "        if ' perc_debris' in df.columns:\n",
    "            # Process dataframe\n",
    "            output_cns = ['# bin_center_elev_m', ' z1_bin_area_valid_km2', ' perc_debris']\n",
    "            df = df[output_cns]\n",
    "            df['# bin_center_elev_m'] = df['# bin_center_elev_m'].astype(np.float) \n",
    "            df[' z1_bin_area_valid_km2'] = df[' z1_bin_area_valid_km2'].astype(np.float)\n",
    "            \n",
    "            # Remove nan values\n",
    "            df[' perc_debris'] = df[' perc_debris'].astype(np.float)\n",
    "            df.fillna(0, inplace=True)\n",
    "            \n",
    "            if verbose:\n",
    "                print(glac_str, df[' perc_debris'].max())\n",
    "            \n",
    "            df['area_debris_km2'] = df[' z1_bin_area_valid_km2'] * df[' perc_debris'] / 100\n",
    "            \n",
    "            if df[' perc_debris'].max() > 10:\n",
    "                debris_switch = True\n",
    "                debris_idx = np.where(df[' perc_debris'] > 50)[0]\n",
    "                \n",
    "        if debris_switch:\n",
    "            if verbose:\n",
    "                print('processing', glac_str)\n",
    "\n",
    "            # ===== Project shapefile =====\n",
    "            huss_dir = huss_dir_sample.replace('XXXX',str(region.zfill(2)))\n",
    "            huss_fn = huss_fn_sample.replace('XXXX',glac_str)\n",
    "\n",
    "            proj_fn = os.path.join(huss_dir, huss_fn) # THIS PROJECTION IS KEY!\n",
    "            ds = gdal.Open(proj_fn)\n",
    "            prj = ds.GetProjection()\n",
    "            srs = osr.SpatialReference(wkt=prj)\n",
    "            aea_srs = srs\n",
    "\n",
    "            # If projected shapefile already exists, then skip projection\n",
    "            glac_shp_proj_fn = glac_shp_proj_fp + glac_str + '_crs' + str(aea_srs.GetAttrValue(\"AUTHORITY\", 1)) + '.shp'\n",
    "\n",
    "            if os.path.exists(glac_shp_proj_fn) == False:\n",
    "                glac_shp_proj = glac_shp_single.to_crs({'init': 'epsg:' + str(aea_srs.GetAttrValue(\"AUTHORITY\", 1))})\n",
    "                glac_shp_proj.to_file(glac_shp_proj_fn)\n",
    "\n",
    "                # Shape layer processing\n",
    "                glac_shp_init = gpd.read_file(glac_shp_fn_dict[region])\n",
    "                if verbose:\n",
    "                    print('Shp init crs:', glac_shp_init.crs)\n",
    "\n",
    "                glac_shp_single = glac_shp_init[glac_shp_init['RGIId'] == rgiid]\n",
    "                glac_shp_single = glac_shp_single.reset_index()\n",
    "\n",
    "\n",
    "            glac_shp_ds = ogr.Open(glac_shp_proj_fn, 0)\n",
    "            glac_shp_lyr = glac_shp_ds.GetLayer()\n",
    "            #This should be contained in features\n",
    "            glac_shp_srs = glac_shp_lyr.GetSpatialRef()\n",
    "            feat_count = glac_shp_lyr.GetFeatureCount()\n",
    "            if verbose:\n",
    "                print(\"Input glacier polygon count: %i\" % feat_count)\n",
    "\n",
    "            # Load DEM\n",
    "            z1_dir = z1_dir_sample.replace('XXXX',str(region.zfill(2)))\n",
    "            z1_fn = z1_fn_sample.replace('XXXX',glac_str)\n",
    "            z1_ds = gdal.Open(z1_dir + z1_fn)\n",
    "            z1_int_geom = geolib.ds_geom_intersection([z1_ds, z1_ds], t_srs=glac_shp_srs)\n",
    "\n",
    "            glacfeat_list = []\n",
    "            glacname_fieldname = \"Name\"\n",
    "            glacnum_fieldname = \"RGIId\"\n",
    "            glacnum_fmt = '%08.5f'\n",
    "\n",
    "            for n, feat in enumerate(glac_shp_lyr):\n",
    "                gf = GlacFeat(feat, glacname_fieldname, glacnum_fieldname)\n",
    "                if verbose:\n",
    "                    print(\"%i of %i: %s\" % (n+1, feat_count, gf.feat_fn))\n",
    "                #NOTE: Input must be in projected coordinate system, ideally equal area\n",
    "                #Should check this and reproject\n",
    "                gf.geom_attributes(srs=aea_srs)\n",
    "                glacfeat_list.append(gf)\n",
    "\n",
    "            if verbose:\n",
    "                print(gf.feat_fn)\n",
    "\n",
    "            fn_dict = OrderedDict()\n",
    "            #We at least want to warp the two input DEMs\n",
    "            fn_dict['z1'] = os.path.join(z1_dir, z1_fn)\n",
    "\n",
    "            if extra_layers and (gf.glac_area_km2 > min_glac_area_writeout):\n",
    "                if verbose:\n",
    "                    print(gf.glacnum)\n",
    "\n",
    "                # Ice thickness data\n",
    "                ice_thick_fn = os.path.join(huss_dir, huss_fn)\n",
    "                if os.path.exists(ice_thick_fn):\n",
    "                    fn_dict['ice_thick'] = ice_thick_fn\n",
    "\n",
    "\n",
    "                if os.path.exists(input.ts_fp + input.ts_fn_dict[input.roi]):\n",
    "                    fn_dict['ts'] = input.ts_fp + input.ts_fn_dict[input.roi]\n",
    "                    \n",
    "                if os.path.exists(input.ts_fp + input.ts_dayfrac_fn_dict[input.roi]):\n",
    "                    fn_dict['ts_dayfrac'] = input.ts_fp + input.ts_dayfrac_fn_dict[input.roi]\n",
    "                if os.path.exists(input.ts_fp + input.ts_year_fn_dict[input.roi]):\n",
    "                    fn_dict['ts_year'] = input.ts_fp + input.ts_year_fn_dict[input.roi]\n",
    "                if os.path.exists(input.ts_fp + input.ts_doy_fn_dict[input.roi]):\n",
    "                    fn_dict['ts_doy'] = input.ts_fp + input.ts_doy_fn_dict[input.roi]\n",
    "\n",
    "\n",
    "            #Expand extent to include buffered region around glacier polygon\n",
    "            warp_extent = geolib.pad_extent(gf.glac_geom_extent, width=buff_dist)\n",
    "            if verbose:\n",
    "                print(\"Expanding extent\")\n",
    "                print(gf.glac_geom_extent)\n",
    "                print(warp_extent)\n",
    "                print(aea_srs)\n",
    "\n",
    "            #Warp everything to common res/extent/proj\n",
    "            ds_list = warplib.memwarp_multi_fn(fn_dict.values(), res=input.ts_stats_res, \\\n",
    "                    extent=warp_extent, t_srs=aea_srs, verbose=verbose, \\\n",
    "                    r='cubic')\n",
    "\n",
    "            ds_dict = dict(zip(fn_dict.keys(), ds_list))\n",
    "\n",
    "            if verbose:\n",
    "                print(ds_list)\n",
    "                print(fn_dict.keys())\n",
    "\n",
    "            #Prepare mask for all glaciers within buffered area, not just the current glacier polygon\n",
    "            glac_shp_ds = ogr.Open(glac_shp_proj_fn, 0)\n",
    "            glac_shp_lyr = glac_shp_ds.GetLayer()\n",
    "\n",
    "            #Get global glacier mask\n",
    "            #Want this to be True over ALL glacier surfaces, not just the current polygon\n",
    "            glac_shp_lyr_mask = geolib.lyr2mask(glac_shp_lyr, ds_dict['ice_thick'])\n",
    "\n",
    "            #Create buffer around glacier polygon\n",
    "            glac_geom_buff = gf.glac_geom.Buffer(buff_dist)\n",
    "            #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "            glac_geom_buff_mask = geolib.geom2mask(glac_geom_buff, ds_dict['ice_thick'])\n",
    "\n",
    "            # ds masks\n",
    "            ds_list_masked = [iolib.ds_getma(i) for i in ds_list]\n",
    "            dem1 = np.ma.masked_less_equal(ds_list_masked[0], 0)\n",
    "            dems_mask = dem1.mask\n",
    "            if verbose:\n",
    "                print('list of datasets:', len(ds_list_masked), fn_dict.values())\n",
    "\n",
    "            #Combine to identify ~1 km buffer around glacier polygon over static rock\n",
    "            static_buffer_mask = np.ma.mask_or(~glac_shp_lyr_mask, glac_geom_buff_mask)\n",
    "            static_shp_lyr_mask = np.ma.mask_or(static_buffer_mask, dems_mask)\n",
    "\n",
    "            if 'z1' in ds_dict:\n",
    "                #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "                glac_geom_mask = geolib.geom2mask(gf.glac_geom, ds_dict['z1'])\n",
    "                gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1']))\n",
    "\n",
    "                #Now apply glacier mask AND mask NaN values\n",
    "                glac_geom_mask = np.ma.mask_or(glac_geom_mask, dems_mask)\n",
    "                gf.z1 = np.ma.array(gf.z1, mask=glac_geom_mask)\n",
    "\n",
    "                if verbose:\n",
    "                    print('\\n\\n# z1 pixels:', gf.z1.count(), '\\n')\n",
    "                if gf.z1.count() == 0:\n",
    "                    if verbose:\n",
    "                        print(\"No z1 pixels\")\n",
    "            else:\n",
    "                print(\"Unable to load z1 ds\")\n",
    "\n",
    "            # ===== ADD VARIOUS LAYERS TO gf =====\n",
    "            if nlatlon + nglac == 0:\n",
    "                print('\\n\\nHACK TO BYPASS VALID AREA\\n\\n')\n",
    "            gf.valid_area_perc = 100\n",
    "\n",
    "            if gf.valid_area_perc < (100. * min_valid_area_perc):\n",
    "                if verbose:\n",
    "                    print(\"Not enough valid pixels. %0.1f%% percent of glacier polygon area\" % (gf.valid_area_perc))\n",
    "            #     return None\n",
    "\n",
    "            else:\n",
    "                #Filter dz - throw out abs differences >150 m\n",
    "\n",
    "                #Compute dz, volume change, mass balance and stats\n",
    "                gf.z1_stats = malib.get_stats(gf.z1)\n",
    "                z1_elev_med = gf.z1_stats[5]\n",
    "                z1_elev_min, z1_elev_max = malib.calcperc(gf.z1, (0.1, 99.9))\n",
    "\n",
    "                #Caluclate stats for aspect and slope using z2\n",
    "                #Requires GDAL 2.1+\n",
    "                gf.z1_aspect = np.ma.array(geolib.gdaldem_mem_ds(ds_dict['z1'], processing='aspect', returnma=True), mask=glac_geom_mask)\n",
    "                gf.z1_aspect_stats = malib.get_stats(gf.z1_aspect)\n",
    "                z1_aspect_med = gf.z1_aspect_stats[5]\n",
    "                gf.z1_slope = np.ma.array(geolib.gdaldem_mem_ds(ds_dict['z1'], processing='slope', returnma=True), mask=glac_geom_mask)\n",
    "                gf.z1_slope_stats = malib.get_stats(gf.z1_slope)\n",
    "                z1_slope_med = gf.z1_slope_stats[5]\n",
    "\n",
    "                #Can estimate ELA values computed from hypsometry and typical AAR\n",
    "                #For now, assume ELA is mean\n",
    "                gf.z1_ela = None\n",
    "                gf.z1_ela = gf.z1_stats[3]\n",
    "                #Note: in theory, the ELA should get higher with mass loss\n",
    "                #In practice, using mean and same polygon, ELA gets lower as glacier surface thins\n",
    "\n",
    "\n",
    "                if extra_layers and (gf.glac_area_km2 > min_glac_area_writeout):\n",
    "                    if 'ice_thick' in ds_dict:\n",
    "                        #Load ice thickness\n",
    "                        gf.H = np.ma.array(iolib.ds_getma(ds_dict['ice_thick']), mask=glac_geom_mask)\n",
    "                        gf.H_mean = gf.H.mean()\n",
    "                        if verbose:\n",
    "                            print('mean ice thickness [m]:', gf.H_mean)\n",
    "\n",
    "                    if 'ts' in ds_dict:\n",
    "                        #Load surface temperature maps\n",
    "                        gf.ts = np.ma.array(iolib.ds_getma(ds_dict['ts']), mask=glac_geom_mask)\n",
    "                        gf.ts.mask = np.ma.mask_or(glac_geom_mask, np.ma.getmask(np.ma.masked_array(gf.ts.data, np.isnan(gf.ts.data))))\n",
    "                    else:\n",
    "                        gf.ts = None\n",
    "                        \n",
    "                    if 'ts_dayfrac' in ds_dict:\n",
    "                        #Load surface temperature maps\n",
    "                        gf.ts_dayfrac = np.ma.array(iolib.ds_getma(ds_dict['ts_dayfrac']), mask=glac_geom_mask)\n",
    "                        gf.ts_dayfrac.mask = np.ma.mask_or(glac_geom_mask, \n",
    "                                                           np.ma.getmask(np.ma.masked_array(gf.ts_dayfrac.data, np.isnan(gf.ts_dayfrac.data))))\n",
    "                    else:\n",
    "                        gf.ts_dayfrac = None\n",
    "                        \n",
    "                    if 'ts_year' in ds_dict:\n",
    "                        #Load surface temperature maps\n",
    "                        gf.ts_year = np.ma.array(iolib.ds_getma(ds_dict['ts_year']), mask=glac_geom_mask)\n",
    "                        gf.ts_year.mask = np.ma.mask_or(glac_geom_mask, \n",
    "                                                        np.ma.getmask(np.ma.masked_array(gf.ts_year.data, np.isnan(gf.ts_year.data))))\n",
    "                    else:\n",
    "                        gf.ts_year = None\n",
    "                        \n",
    "                    if 'ts_doy' in ds_dict:\n",
    "                        #Load surface temperature maps\n",
    "                        gf.ts_doy = np.ma.array(iolib.ds_getma(ds_dict['ts_doy']), mask=glac_geom_mask)\n",
    "                        gf.ts_doy.mask = np.ma.mask_or(glac_geom_mask, \n",
    "                                                       np.ma.getmask(np.ma.masked_array(gf.ts_doy.data, np.isnan(gf.ts_doy.data))))\n",
    "                    else:\n",
    "                        gf.ts_doy = None\n",
    "\n",
    "                gf.res = geolib.get_res(ds_dict['z1'])\n",
    "\n",
    "                if verbose:\n",
    "                    print('Area [km2]:', gf.glac_area / 1e6)\n",
    "                    print('-------------------------------')\n",
    "                    \n",
    "                # Isolate values with positive surface temperatures below mean elevation\n",
    "                zmean_mask = np.ma.mask_or(glac_geom_mask,  \n",
    "                                           np.ma.getmask(np.ma.masked_greater(gf.z1, gf.z1.compressed().mean())))\n",
    "                ts_zmean_mask = np.ma.mask_or(zmean_mask,\n",
    "                                              np.ma.getmask(np.ma.masked_less(gf.ts, 0)))\n",
    "\n",
    "                gf.ts_doy.mask = zmean_mask\n",
    "                gf.ts_year.mask = zmean_mask\n",
    "                gf.ts_dayfrac.mask = zmean_mask\n",
    "                doy_list.extend(list(gf.ts_doy.compressed()))\n",
    "                year_list.extend(list(gf.ts_year.compressed()))\n",
    "                dayfrac_list.extend(list(gf.ts_dayfrac.compressed()))\n",
    "                \n",
    "    # Statistics\n",
    "    print('  year mean +/- std:', np.round(np.mean(year_list),1), np.round(np.std(year_list),1)) \n",
    "    print('  doy mean +/- std:', np.round(np.mean(doy_list),1), np.round(np.std(doy_list),1)) \n",
    "    print('    doy median:', np.round(np.median(doy_list),1)) \n",
    "    print('  dayfrac mean +/- std:', np.round(np.mean(dayfrac_list),3), np.round(np.std(dayfrac_list),3)) \n",
    "    \n",
    "    # Compute statistics\n",
    "    year_mean = np.mean(year_list)\n",
    "    year_std = np.std(year_list)\n",
    "    doy_mean = np.mean(doy_list)\n",
    "    doy_std = np.std(doy_list)\n",
    "    dayfrac_mean = np.mean(dayfrac_list)\n",
    "    dayfrac_std = np.std(dayfrac_list)\n",
    "\n",
    "\n",
    "    # Update array\n",
    "    elevstats_mean[lat_idx,lon_idx] = elev_mean\n",
    "    elevstats_std[lat_idx,lon_idx] = elev_std\n",
    "    elevstats_med[lat_idx,lon_idx] = elev_med\n",
    "    elevstats_mad[lat_idx,lon_idx] = elev_mad\n",
    "    elevstats_min[lat_idx,lon_idx] = zmin_debris\n",
    "    elevstats_max[lat_idx,lon_idx] = zmax_debris\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                \n",
    "        \n",
    "\n",
    "#             # Merge datasets together to compute elevation stats for each lat/lon\n",
    "#             if df_all is None:\n",
    "#                 df_all = df\n",
    "#                 if len(debris_idx) > 1:\n",
    "#                     zmin_debris = df['# bin_center_elev_m'].values[debris_idx[0]]\n",
    "#                     zmax_debris = df['# bin_center_elev_m'].values[debris_idx[-1]]\n",
    "#             else:\n",
    "#                 # If new min elevation lower than min old elevation\n",
    "#                 if elev_min < df_all['# bin_center_elev_m'].values[0]:\n",
    "#                     elev2add = np.arange(elev_min,df_all['# bin_center_elev_m'].values[0], binsize)\n",
    "#                     df_all_add = pd.DataFrame(np.zeros((len(elev2add), len(df.columns))), columns=df.columns)\n",
    "#                     df_all_add['# bin_center_elev_m'] = elev2add\n",
    "#                     df_all = pd.concat([df_all,df_all_add], axis=0)\n",
    "#                     df_all.sort_values('# bin_center_elev_m', inplace=True)\n",
    "\n",
    "#                 # If new max elevation higher than max old elevation\n",
    "#                 if elev_max > df_all['# bin_center_elev_m'].values[-1]:\n",
    "#                     elev2add = np.arange(df_all['# bin_center_elev_m'].values[-1] + binsize, elev_max + 1, binsize)\n",
    "#                     df_all_add = pd.DataFrame(np.zeros((len(elev2add), len(df.columns))), columns=df.columns)\n",
    "#                     df_all_add['# bin_center_elev_m'] = elev2add\n",
    "#                     df_all = pd.concat([df_all,df_all_add], axis=0)\n",
    "#                     df_all.sort_values('# bin_center_elev_m', inplace=True)\n",
    "\n",
    "#                 # If new min elevation higher than min old elevation\n",
    "#                 if df_all['# bin_center_elev_m'].values[0] < elev_min:\n",
    "#                     elev2add = np.arange(df_all['# bin_center_elev_m'].values[0], elev_min, binsize)\n",
    "#                     df_add = pd.DataFrame(np.zeros((len(elev2add), len(df.columns))), columns=df.columns)\n",
    "#                     df_add['# bin_center_elev_m'] = elev2add\n",
    "#                     df = pd.concat([df,df_add], axis=0)\n",
    "#                     df.sort_values('# bin_center_elev_m', inplace=True)\n",
    "\n",
    "#                 # If new max elevation lower than max old elevation \n",
    "#                 if df_all['# bin_center_elev_m'].values[-1] > elev_max:\n",
    "#                     elev2add = np.arange(elev_max + binsize, df_all['# bin_center_elev_m'].values[-1] + 1, binsize)\n",
    "#                     df_add = pd.DataFrame(np.zeros((len(elev2add), len(df.columns))), columns=df.columns)\n",
    "#                     df_add['# bin_center_elev_m'] = elev2add\n",
    "#                     df = pd.concat([df,df_add], axis=0)\n",
    "#                     df.sort_values('# bin_center_elev_m', inplace=True)\n",
    "\n",
    "#                 df_all.reset_index(inplace=True, drop=True)\n",
    "#                 df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#                 # Merge bins (area-weighted)\n",
    "#                 df_all[' z1_bin_area_valid_km2'] = df_all[' z1_bin_area_valid_km2'] + df[' z1_bin_area_valid_km2']\n",
    "#                 df_all['area_debris_km2'] = df_all['area_debris_km2'] + df['area_debris_km2']\n",
    "#                 df_all[' perc_debris'] = df_all['area_debris_km2'] / df_all[' z1_bin_area_valid_km2'] * 100\n",
    "                \n",
    "#                 if len(debris_idx) > 1:\n",
    "#                     if df['# bin_center_elev_m'].values[debris_idx[0]] < zmin_debris:\n",
    "#                         zmin_debris = df['# bin_center_elev_m'].values[debris_idx[0]]\n",
    "#                     if df['# bin_center_elev_m'].values[debris_idx[-1]] > zmax_debris:\n",
    "#                         zmax_debris = df['# bin_center_elev_m'].values[debris_idx[-1]]\n",
    "\n",
    "#     # Area-weighted statistics\n",
    "#     if df_all is not None:\n",
    "#         # Assume 10 m horizontal resolution for computing the area-weighted statistics of the debris elevation\n",
    "#         pixel_res = 10\n",
    "\n",
    "#         # Estimate pixels in each bin\n",
    "#         df_all['pixels_debris'] = np.round(df_all['area_debris_km2'] / (pixel_res / 1000)**2, 0)\n",
    "\n",
    "#         elev_list_all = []\n",
    "#         for nelev, elev in enumerate(df_all['# bin_center_elev_m']):\n",
    "#             elev_list_single = list(np.repeat(elev, df_all.loc[nelev,'pixels_debris']))\n",
    "#             elev_list_all.extend(elev_list_single)\n",
    "\n",
    "        \n",
    "        \n",
    "#         print('mean +/- std:', np.round(elev_mean,0), '+/-', np.round(elev_std,0), \n",
    "#               ';  med +/- mad:', np.round(elev_med,0), '+/-', np.round(elev_mad,0), \n",
    "#               ';  zmin:', zmin_debris, 'zmax:', zmax_debris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    # bin_center_elev_m   z1_bin_area_valid_km2   perc_debris  area_debris_km2\n",
      "0                4375.0                   0.000          0.00         0.000000\n",
      "1                4425.0                   0.000          0.00         0.000000\n",
      "2                4475.0                   0.010        100.00         0.010000\n",
      "3                4525.0                   0.225         98.54         0.221715\n",
      "4                4575.0                   0.291         95.70         0.278487\n",
      "5                4625.0                   0.255         99.23         0.253036\n",
      "6                4675.0                   0.243        100.00         0.243000\n",
      "7                4725.0                   0.186         99.65         0.185349\n",
      "8                4775.0                   0.100         70.78         0.070780\n",
      "9                4825.0                   0.052         25.97         0.013504\n",
      "10               4875.0                   0.070         25.47         0.017829\n",
      "11               4925.0                   0.072         19.82         0.014270\n",
      "12               4975.0                   0.057         20.45         0.011657\n",
      "13               5025.0                   0.063         10.31         0.006495\n",
      "14               5075.0                   0.081          0.00         0.000000\n",
      "15               5125.0                   0.090          0.00         0.000000\n",
      "16               5175.0                   0.087          0.00         0.000000\n",
      "17               5225.0                   0.104          1.86         0.001934\n",
      "18               5275.0                   0.113          0.58         0.000655\n",
      "19               5325.0                   0.098          0.00         0.000000\n",
      "20               5375.0                   0.074          0.00         0.000000\n",
      "21               5425.0                   0.046          0.00         0.000000\n",
      "22               5475.0                   0.020          0.00         0.000000\n",
      "23               5525.0                   0.017          0.00         0.000000\n",
      "24               5575.0                   0.023          0.00         0.000000\n",
      "25               5625.0                   0.034          0.00         0.000000\n",
      "26               5675.0                   0.032          0.00         0.000000\n",
      "27               5725.0                   0.040          0.00         0.000000\n",
      "28               5775.0                   0.057          0.00         0.000000\n",
      "29               5825.0                   0.071          0.00         0.000000\n",
      "30               5875.0                   0.077          0.00         0.000000\n",
      "31               5925.0                   0.081          0.00         0.000000\n",
      "32               5975.0                   0.053          0.00         0.000000\n",
      "33               6025.0                   0.036          0.00         0.000000\n",
      "34               6075.0                   0.019          0.00         0.000000\n",
      "35               6125.0                   0.012          0.00         0.000000\n",
      "36               6175.0                   0.008          0.00         0.000000\n",
      "37               6225.0                   0.000          0.00         0.000000\n",
      "38               6275.0                   0.000          0.00         0.000000\n",
      "39               6325.0                   0.000          0.00         0.000000\n",
      "40               6375.0                   0.000          0.00         0.000000\n",
      "41               6425.0                   0.000          0.00         0.000000\n",
      "42               6475.0                   0.000          0.00         0.000000\n",
      "43               6525.0                   0.000          0.00         0.000000\n",
      "44               6575.0                   0.000          0.00         0.000000\n",
      "45               6625.0                   0.000          0.00         0.000000\n",
      "46               6675.0                   0.000          0.00         0.000000\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (latitude: 81, longitude: 161)\n",
      "Coordinates:\n",
      "  * latitude   (latitude) float32 45.0 44.75 44.5 44.25 ... 25.5 25.25 25.0\n",
      "  * longitude  (longitude) float32 65.0 65.25 65.5 65.75 ... 104.5 104.75 105.0\n",
      "Data variables:\n",
      "    zmean      (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
      "    zstd       (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
      "    zmed       (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
      "    zmad       (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
      "    zmin       (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
      "    zmax       (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# Export to dataset\n",
    "ds_elevstats = xr.Dataset({'zmean': (['latitude', 'longitude'], elevstats_mean),\n",
    "                           'zstd': (['latitude', 'longitude'], elevstats_std),\n",
    "                           'zmed': (['latitude', 'longitude'], elevstats_med),\n",
    "                           'zmad': (['latitude', 'longitude'], elevstats_mad),\n",
    "                           'zmin': (['latitude', 'longitude'], elevstats_min),\n",
    "                           'zmax': (['latitude', 'longitude'], elevstats_max),},\n",
    "                          coords={'latitude': ds.latitude.values,\n",
    "                                  'longitude': ds.longitude.values})\n",
    "attrs_dict={\n",
    "     'zmean':{'units':'m a.s.l.',\n",
    "         'long_name':'mean elevation',\n",
    "         'comment': 'mean elevation associated with the debris for the given lat/lon'},\n",
    "     'zstd':{'units':'m a.s.l.',\n",
    "         'long_name':'standard deviation of the debris elevation',\n",
    "         'comment': 'standard deviation of the debris elevation associated with the debris for the given lat/lon'},\n",
    "     'zmed':{'units':'m a.s.l.',\n",
    "         'long_name':'median elevation',\n",
    "         'comment': 'median elevation associated with the debris for the given lat/lon'},\n",
    "     'zmad':{'units':'m a.s.l.',\n",
    "         'long_name':'median absolute deviation of the debris elevation',\n",
    "         'comment': 'median absolute deviation of the debris elevation associated with the debris for the given lat/lon'},\n",
    "     'zmin':{'units':'m a.s.l.',\n",
    "         'long_name':'minimum elevation',\n",
    "         'comment': 'minimum elevation with >50% debris cover for the given lat/lon'},\n",
    "     'zmax':{'units':'m a.s.l.',\n",
    "         'long_name':'maximum elevation',\n",
    "         'comment': 'maximum elevation with >50% debris cover for the given lat/lon'}}\n",
    "\n",
    "for vn in ['zmean', 'zstd', 'zmed', 'zmad']:\n",
    "    ds_elevstats[vn].attrs = attrs_dict[vn]\n",
    "    \n",
    "ds_elevstats.to_netcdf(debris_elevstats_fullfn.replace('.nc','v2.nc'))\n",
    "                \n",
    "print(ds_elevstats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.75 76.5 \n",
      " 4344.953214642455 413.2688619987504 4325.0 370.65\n"
     ]
    }
   ],
   "source": [
    "# lat_idx = 68\n",
    "# lon_idx = 88\n",
    "lat_idx = 37\n",
    "lon_idx = 46\n",
    "print(ds['latitude'][lat_idx].values, ds['longitude'][lon_idx].values,\n",
    "      '\\n', ds_elevstats['zmean'][lat_idx,lon_idx].values, ds_elevstats['zstd'][lat_idx,lon_idx].values, \n",
    "      ds_elevstats['zmed'][lat_idx,lon_idx].values, ds_elevstats['zmad'][lat_idx,lon_idx].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1137"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latlon_unique_dict[(68,88)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
